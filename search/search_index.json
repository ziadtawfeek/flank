{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Flank Flank is a massively parallel Android and iOS test runner for Firebase Test Lab . Flank is YAML compatible with the gcloud CLI . Flank provides extra features to accelerate velocity and increase quality. Download https://github.com/Flank/flank/releases/latest/download/flank.jar Sponsors See error monitoring docs to disable Sentry error monitoring. Contributing Install JDK 15 (it works also correctly on the previous version, a newer version is not guaranteed to work properly): Oracle OpenJDK AdoptJDK Use JetBrains Toolbox to install IntelliJ IDEA Community Clone the repo git clone --recursive https://github.com/Flank/flank.git git submodule update --init --recursive updates the submodules Open build.gradle.kts in the main Flank base directory with IntelliJ IDEA Community , this will open the entire Flank mono repo test runner contributions can be made in the test_runner\\ subdirectory Features Test sharding Cost reporting Stability testing HTML report JUnit XML report Smart Flank Exit Codes Exit code Description 0 All tests passed 1 A general failure occurred. Possible causes include: a filename that does not exist or an HTTP/network error. 2 Usually indicates missing or wrong usage of flags, incorrect parameters, errors in config files. 10 At least one matrix not finished (usually a FTL internal error) or unexpected error occurred. 15 Firebase Test Lab could not determine if the test matrix passed or failed, because of an unexpected error. 18 The test environment for this test execution is not supported because of incompatible test dimensions. This error might occur if the selected Android API level is not supported by the selected device type. 19 The test matrix was canceled by the user. 20 A test infrastructure error occurred. CLI Flank supports CLI flags for each YAML parameter. The CLI flags are useful to selectively override YAML file values. Pass the --help flag to see the full documentation. For example: flank android run --help CLI flags work well with environment variables. You can override a value like this: flank android run --local-result-dir=$APP_NAME Flank configuration app, test, and xctestrun-file support ~ , environment variables, and globs ( , *) when resolving paths iOS example Run test_runner/flank.ios.yml with flank to verify iOS execution is working. ./gradlew clean test_runner:build test_runner:shadowJar java -jar ./test_runner/build/libs/flank-*.jar firebase test ios run # gcloud args match the official gcloud cli # https://cloud.google.com/sdk/gcloud/reference/alpha/firebase/test/ios/run gcloud : # -- GcloudYml -- ### Results Bucket ## The name of a Google Cloud Storage bucket where raw test results will be stored # results-bucket: tmp_flank ### Results Directory ## The name of a unique Google Cloud Storage object within the results bucket where raw test results will be stored ## (default: a timestamp with a random suffix). # results-dir: tmp ### Record Video flag ## Enable video recording during the test. Disabled by default. Use --record-video to enable. # record-video: true ### Timeout ## The max time this test execution can run before it is cancelled (default: 15m). ## It does not include any time necessary to prepare and clean up the target device. ## The maximum possible testing time is 45m on physical devices and 60m on virtual devices. ## The TIMEOUT units can be h, m, or s. If no unit is given, seconds are assumed. # timeout: 30m ### Asynchronous flag ## Invoke a test asynchronously without waiting for test results. # async: false ### Client Details ## A key-value map of additional details to attach to the test matrix. ## Arbitrary key-value pairs may be attached to a test matrix to provide additional context about the tests being run. ## When consuming the test results, such as in Cloud Functions or a CI system, ## these details can add additional context such as a link to the corresponding pull request. # client-details # key1: value1 # key2: value2 ### Network Profile ## The name of the network traffic profile, for example LTE, HSPA, etc, ## which consists of a set of parameters to emulate network conditions when running the test ## (default: no network shaping; see available profiles listed by the `flank test network-profiles list` command). ## This feature only works on physical devices. # network-profile: LTE ### Result History Name ## The history name for your test results (an arbitrary string label; default: the application's label from the APK manifest). ## All tests which use the same history name will have their results grouped together in the Firebase console in a time-ordered test history list. # results-history-name: android-history ### Number of Flaky Test Attempts ## The number of times a TestExecution should be re-attempted if one or more\\nof its test cases fail for any reason. ## The maximum number of reruns allowed is 10. Default is 0, which implies no reruns. # num-flaky-test-attempts: 0 ### Fail Fast ## If true, only a single attempt at most will be made to run each execution/shard in the matrix. ## Flaky test attempts are not affected. Normally, 2 or more attempts are made if a potential ## infrastructure issue is detected. This feature is for latency sensitive workloads. The ## incidence of execution failures may be significantly greater for fail-fast matrices and support ## is more limited because of that expectation. # fail-fast: false # -- IosGcloudYml -- ### IOS Test Package Path ## The path to the test package (a zip file containing the iOS app and XCTest files). ## The given path may be in the local filesystem or in Google Cloud Storage using a URL beginning with gs://. ## Note: any .xctestrun file in this zip file will be ignored if --xctestrun-file is specified. test : ./src/test/kotlin/ftl/fixtures/tmp/earlgrey_example.zip ### IOS XCTestrun File Path ## The path to an .xctestrun file that will override any .xctestrun file contained in the --test package. ## Because the .xctestrun file contains environment variables along with test methods to run and/or ignore, ## this can be useful for customizing or sharding test suites. The given path should be in the local filesystem. ## Note: this path should usually be pointing to the xctestrun file within the derived data folder ## For example ./derivedDataPath/Build/Products/EarlGreyExampleSwiftTests_iphoneos13.4-arm64e.xctestrun xctestrun-file : ./src/test/kotlin/ftl/fixtures/tmp/EarlGreyExampleSwiftTests_iphoneos13.4-arm64e.xctestrun ### Xcode Version ## The version of Xcode that should be used to run an XCTest. ## Defaults to the latest Xcode version supported in Firebase Test Lab. ## This Xcode version must be supported by all iOS versions selected in the test matrix. # xcode-version: 10.1 ### IOS Device Parameters ## A list of DIMENSION=VALUE pairs which specify a target device to test against. ## This flag may be repeated to specify multiple devices. ## The four device dimensions are: model, version, locale, and orientation. # device: # - model: iphone8 # version: 12.0 # locale: en # orientation: portrait # - model: iphonex # version: 12.0 # locale: es_ES # orientation: landscape ### Directories to Pull ## A list of paths that will be copied from the device's storage to the designated results bucket after the test ## is complete. These must be absolute paths under /private/var/mobile/Media or /Documents ## of the app under test. If the path is under an app's /Documents, it must be prefixed with the app's bundle id and a colon # directories-to-pull: # - /private/var/mobile/Media ### Other File paths ## A list of device-path=file-path pairs that specify the paths of the test device and the files you want pushed to the device prior to testing. ## Device paths should either be under the Media shared folder (e.g. prefixed with /private/var/mobile/Media) or ## within the documents directory of the filesystem of an app under test (e.g. /Documents). Device paths to app ## filesystems should be prefixed by the bundle ID and a colon. Source file paths may be in the local filesystem or in Google Cloud Storage (gs://\u2026). # other-files # com.my.app:/Documents/file.txt: local/file.txt # /private/var/mobile/Media/file.jpg: gs://bucket/file.jpg ### Additional IPA's ## List of up to 100 additional IPAs to install, in addition to the one being directly tested. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. # additional-ipas: # - gs://bucket/additional.ipa # - path/to/local/ipa/file.ipa ### Scenario Numbers ## A list of game-loop scenario numbers which will be run as part of the test (default: all scenarios). ## A maximum of 1024 scenarios may be specified in one test matrix, but the maximum number may also be limited by the overall test --timeout setting. # scenario-numbers: # - 1 # - 2 # - 3 ### Test type ## The type of iOS test to run. TYPE must be one of: xctest, game-loop. Default: xctest # type: xctest ### Application Path ## The path to the application archive (.ipa file) for game-loop testing. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## This flag is only valid when --type=game-loop is also set # app: # - gs://bucket/additional.ipa OR path/to/local/ipa/file.ipa ### Testing with Special Entitlements ## Enables testing special app entitlements. Re-signs an app having special entitlements with a new application-identifier. ## This currently supports testing Push Notifications (aps-environment) entitlement for up to one app in a project. ## Note: Because this changes the app's identifier, make sure none of the resources in your zip file contain direct references to the test app's bundle id. # test-special-entitlements: false flank : # -- FlankYml -- ### Max Test Shards ## test shards - the amount of groups to split the test suite into ## set to -1 to use one shard per test. default: 1 # max-test-shards: 1 ## Shard Time ## shard time - the amount of time tests within a shard should take ## when set to > 0, the shard count is dynamically set based on time up to the maximum limit defined by max-test-shards ## 2 minutes (120) is recommended. ## default: -1 (unlimited) # shard-time: -1 ### Number of Test Runs ## test runs - the amount of times to run the tests. ## 1 runs the tests once. 10 runs all the tests 10x # num-test-runs: 1 ### Smart Flank GCS Paths ## Google cloud storage path to store the JUnit XML results from the last run. ## NOTE: Empty results will not be uploaded # smart-flank-gcs-path: gs://tmp_flank/flank/test_app_ios.xml ### Smart Flank Disable Upload flag ## Disables smart flank JUnit XML uploading. Useful for preventing timing data from being updated. ## Default: false # smart-flank-disable-upload: false ### Use Average Test Time For New Tests flag ## Enable using average time from previous tests duration when using SmartShard and tests did not run before. ## Default: false # use-average-test-time-for-new-tests: true ### Default Test Time ## Set default test time used for calculating shards. ## Default: 120.0 # default-test-time: 15 ### Default Class Test Time ## Set default test time (in seconds) used for calculating shards of parametrized classes when previous tests results are not available. ## Default test time for classes should be different from the default time for test ## Default: 240.0 # default-class-test-time: 30 ### Disable Sharding flag ## Disables sharding. Useful for parameterized tests. # disable-sharding: false ### Test targets always Run ## always run - these tests are inserted at the beginning of every shard ## Execution order is not guaranteed by Flank. Users are responsible for configuring their own device test runner logic. # test-targets-always-run: # - className/testName ### Files to Download ## regex is matched against bucket paths, for example: 2019-01-09_00:18:07.314000_hCMY/shard_0/EarlGreyExampleSwiftTests_iphoneos12.1-arm64e.xctestrun # files-to-download: # - .*\\.mp4$ # -- IosFlankYml -- ### Test Targets ## test targets - a list of tests to run. omit to run all tests. # test-targets: # - className/testName ### Billing Project ID ## The billing enabled Google Cloud Platform project id to use # project: flank-open-source ### Local Result Directory Storage ## Local folder to store the test result. Folder is DELETED before each run to ensure only artifacts from the new run are saved. # local-result-dir: flank ### Run Timeout ## The max time this test run can execute before it is cancelled (default: unlimited). # run-timeout: 60m ### Keep File Path flag ## Keeps the full path of downloaded files. Required when file names are not unique. ## Default: false # keep-file-path: false ### Ignore Failed Tests flag ## Terminate with exit code 0 when there are failed tests. ## Useful for Fladle and other gradle plugins that don't expect the process to have a non-zero exit code. ## The JUnit XML is used to determine failure. (default: false) # ignore-failed-tests: true ### Output Style flag ## Output style of execution status. May be one of [verbose, multi, single, compact]. ## For runs with only one test execution the default value is 'verbose', in other cases ## 'multi' is used as the default. The output style 'multi' is not displayed correctly on consoles ## which don't support ansi codes, to avoid corrupted output use single or verbose. ## The output style `compact` is used to produce less detailed output, it prints just Args, test and matrix count, weblinks, cost, and result reports. # output-style: single ### Full Junit Result flag ## Enable create additional local junit result on local storage with failure nodes on passed flaky tests. # full-junit-result: false ### Disable Result Upload flag ## Disables flank results upload on gcloud storage. ## Default: false # disable-results-upload: false ### Disable usage statistics flag ## Disable sending usage statistics (without sensitive data) to the analytic tool. ## Default: false # disable-usage-statistics: false ### Only Test Configuration ## Constrains a test action to only test a specified test configuration within a test plan and exclude all other test configurations. ## Flank can combine multiple constraint options, but -only-test-configuration has precedence over -skip-test-configuration. ## Each test configuration name must match the name of a configuration specified in a test plan and is case-sensitive. ## Default: null (run all test configurations) # only-test-configuration: en ### Skip Test Configuration ## Constrains a test action to skip a specified test configuration and include all other test configurations. ## Flank can combine multiple constraint options, but -only-test-configuration has precedence over -skip-test-configuration. ## Each test configuration name must match the name of a configuration specified in a test plan and is case-sensitive. ## Default: null (run all test configurations) # skip-test-configuration: en ### Enable output report with set type ## Saves output results as parsable file and optionally upload it to Gcloud.. ## Default: none # output-report: none ### Disable config validation (for both, yml and command line) ## If true, Flank won't validate options provided by the user. In general, it's not a good idea but, ## there are cases when this could be useful for a user ## (example: project can use devices that are not commonly available, the project has higher sharding limits, etc). ## Default: false # skip-config-validation: false ### Path to the custom sharding JSON file ## Flank will apply provided sharding to the configuration. ## For detailed explanation please check https://github.com/Flank/flank/blob/master/docs/feature/1665-custom-sharding.md # custom-sharding-json: ./custom_sharding.json Android example Run test_runner/flank.yml with flank to verify Android execution is working. ./gradlew clean test_runner:build test_runner:shadowJar java -jar ./test_runner/build/libs/flank-*.jar firebase test android run # gcloud args match the official gcloud cli # See the docs for full gcloud details https://cloud.google.com/sdk/gcloud/reference/firebase/test/android/run gcloud : # -- GcloudYml -- ### Result Bucket ## The name of a Google Cloud Storage bucket where raw test results will be stored # results-bucket: tmp_flank ### Result Directory ## The name of a unique Google Cloud Storage object within the results bucket where raw test results will be stored ## (default: a timestamp with a random suffix). # results-dir: tmp ### Record Video flag ## Enable video recording during the test. Disabled by default. Use --record-video to enable. # record-video: true ### Timeout ## The max time this test execution can run before it is cancelled (default: 15m). ## It does not include any time necessary to prepare and clean up the target device. ## The maximum possible testing time is 45m on physical devices and 60m on virtual devices. ## The TIMEOUT units can be h, m, or s. If no unit is given, seconds are assumed. # timeout: 30m ### Asynchronous flag ## Invoke a test asynchronously without waiting for test results. # async: false ### Client Details ## A key-value map of additional details to attach to the test matrix. ## Arbitrary key-value pairs may be attached to a test matrix to provide additional context about the tests being run. ## When consuming the test results, such as in Cloud Functions or a CI system, ## these details can add additional context such as a link to the corresponding pull request. # client-details # key1: value1 # key2: value2 ### Network Profile ## The name of the network traffic profile, for example LTE, HSPA, etc, ## which consists of a set of parameters to emulate network conditions when running the test ## (default: no network shaping; see available profiles listed by the `flank test network-profiles list` command). ## This feature only works on physical devices. # network-profile: LTE ### Result History Name ## The history name for your test results (an arbitrary string label; default: the application's label from the APK manifest). ## All tests which use the same history name will have their results grouped together in the Firebase console in a time-ordered test history list. # results-history-name: android-history ### Number of Flaky Test Attempts ## The number of times a TestExecution should be re-attempted if one or more\\nof its test cases fail for any reason. ## The maximum number of reruns allowed is 10. Default is 0, which implies no reruns. # num-flaky-test-attempts: 0 ### Fail Fast ## If true, only a single attempt at most will be made to run each execution/shard in the matrix. ## Flaky test attempts are not affected. Normally, 2 or more attempts are made if a potential ## infrastructure issue is detected. This feature is for latency sensitive workloads. The ## incidence of execution failures may be significantly greater for fail-fast matrices and support ## is more limited because of that expectation. # fail-fast: false # -- AndroidGcloudYml -- ## Android Application Path ## The path to the application binary file. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## Android App Bundles are specified as .aab, all other files are assumed to be APKs. app : ../test_projects/android/apks/app-debug.apk ### Android Binary File Path ## The path to the binary file containing instrumentation tests. ## The given path may be in the local filesystem or in Google Cloud Storage using a URL beginning with gs://. test : ../test_projects/android/apks/app-debug-androidTest.apk ### Additional APK's ## A list of up to 100 additional APKs to install, in addition to those being directly tested. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. # additional-apks: additional-apk1.apk,additional-apk2.apk,additional-apk3.apk ### Auto Google Login flag ## Automatically log into the test device using a preconfigured Google account before beginning the test. ## Disabled by default. Use --auto-google-login to enable. # auto-google-login: true ### Use Orchestrator Flag ## Whether each test runs in its own Instrumentation instance with the Android Test Orchestrator ## (default: Orchestrator is used). Disable with --no-use-orchestrator. ## See https://developer.android.com/training/testing/junit-runner.html#using-android-test-orchestrator # use-orchestrator: true ### Environment Variables ## A comma-separated, key=value map of environment variables and their desired values. This flag is repeatable. ## The environment variables are mirrored as extra options to the am instrument -e KEY1 VALUE1 \u2026 command and ## passed to your test runner (typically AndroidJUnitRunner) # environment-variables: # coverage: true # coverageFilePath: /sdcard/ # clearPackageData: true ### Directories to Pull ## A list of paths that will be copied from the device's storage to the designated results bucket after the test ## is complete. These must be absolute paths under /sdcard or /data/local/tmp # directories-to-pull: # - /sdcard/ ### Grant Permissions flag ## Whether to grant runtime permissions on the device before the test begins. ## By default, all permissions are granted. PERMISSIONS must be one of: all, none # grant-permissions: all ### Test Type ## The type of test to run. TYPE must be one of: instrumentation, robo, game-loop. # type: instrumentation ### Other Files ## A list of device-path: file-path pairs that indicate the device paths to push files to the device before starting tests, and the paths of files to push. ## Device paths must be under absolute, whitelisted paths (${EXTERNAL_STORAGE}, or ${ANDROID_DATA}/local/tmp). ## Source file paths may be in the local filesystem or in Google Cloud Storage (gs://\u2026). # other-files # - /sdcard/dir1/file1.txt: local/file.txt # - /sdcard/dir2/file2.jpg: gs://bucket/file.jpg ### OBB Files ## A list of one or two Android OBB file names which will be copied to each test device before the tests will run (default: None). ## Each OBB file name must conform to the format as specified by Android (e.g. [main|patch].0300110.com.example.android.obb) and will be installed into <shared-storage>/Android/obb/<package-name>/ on the test device. # obb-files: # - local/file/path/test1.obb # - local/file/path/test2.obb ### Scenario Numbers ## A list of game-loop scenario numbers which will be run as part of the test (default: all scenarios). ## A maximum of 1024 scenarios may be specified in one test matrix, but the maximum number may also be limited by the overall test --timeout setting. # scenario-numbers: # - 1 # - 2 # - 3 ### Scenario Labels ## A list of game-loop scenario labels (default: None). Each game-loop scenario may be labeled in the APK manifest file with one or more arbitrary strings, creating logical groupings (e.g. GPU_COMPATIBILITY_TESTS). ## If --scenario-numbers and --scenario-labels are specified together, Firebase Test Lab will first execute each scenario from --scenario-numbers. ## It will then expand each given scenario label into a list of scenario numbers marked with that label, and execute those scenarios. # scenario-labels: # - label1 # - label2 ### OBB filenames ## A list of OBB required filenames. OBB file name must conform to the format as specified by Android e.g. ## [main|patch].0300110.com.example.android.obb which will be installed into <shared-storage>/Android/obb/<package-name>/ on the device. # obb-names: # - [main|patch].<VERSION>.com.example.android.obb ### Performance Metric flag ## Monitor and record performance metrics: CPU, memory, network usage, and FPS (game-loop only). ## Disabled by default. Use --performance-metrics to enable. # performance-metrics: true ### Number of Uniform Shards ## Specifies the number of shards into which you want to evenly distribute test cases. ## The shards are run in parallel on separate devices. For example, ## if your test execution contains 20 test cases and you specify four shards, each shard executes five test cases. ## The number of shards should be less than the total number of test cases. ## The number of shards specified must be >= 1 and <= 50. ## This option cannot be used along max-test-shards and is not compatible with smart sharding. ## If you want to take benefits of smart sharding use max-test-shards instead. ## default: null # num-uniform-shards: 50 ### Instrumentation Test Runner Class ## The fully-qualified Java class name of the instrumentation test runner ## (default: the last name extracted from the APK manifest). # test-runner-class: com.foo.TestRunner ### Test Targets ## A list of one or more test target filters to apply (default: run all test targets). ## Each target filter must be fully qualified with the package name, class name, or test annotation desired. ## Supported test filters by am instrument -e \u2026 include: ## class, notClass, size, annotation, notAnnotation, package, notPackage, testFile, notTestFile ## See https://developer.android.com/reference/android/support/test/runner/AndroidJUnitRunner for more information. # test-targets: # - class com.example.app.ExampleUiTest#testPasses ### Robo Directives ## A map of robo_directives that you can use to customize the behavior of Robo test. ## The type specifies the action type of the directive, which may take on values click, text or ignore. ## If no type is provided, text will be used by default. ## Each key should be the Android resource name of a target UI element and each value should be the text input for that element. ## Values are only permitted for text type elements, so no value should be specified for click and ignore type elements. # robo-directives: # \"text:input_resource_name\": message # \"click:button_resource_name\": \"\" ### Robo Scripts ## The path to a Robo Script JSON file. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## You can guide the Robo test to perform specific actions by recording a Robo Script in Android Studio and then specifying this argument. ## Learn more at https://firebase.google.com/docs/test-lab/robo-ux-test#scripting. # robo-script: path_to_robo_script ### Android Device Parameters ## A list of DIMENSION=VALUE pairs which specify a target device to test against. ## This flag may be repeated to specify multiple devices. ## The four device dimensions are: model, version, locale, and orientation. # device: # - model: NexusLowRes # version: 28 # locale: en # orientation: portrait # - model: NexusLowRes # version: 27 ### test-targets-for-shard ## Specifies a group of packages, classes, and/or test cases to run in each shard (a group of test cases). ## The shards are run in parallel on separate devices. You can repeat this flag up to 50 times to specify multiple shards when one or more physical devices are selected, ## or up to 500 times when no physical devices are selected. ## Note: If you include the flags environment-variable or test-targets when running test-targets-for-shard, the flags are applied to all the shards you create. # test-target-for-shard: # - package com.package1.for.shard1 # - class com.package2.for.shard2.Class ### parameterized-tests ## Specifies how to handle tests which contain the parameterization annotation. ## 4 options are available ## default: treat Parameterized tests as normal and shard accordingly ## ignore-all: Parameterized tests are ignored and not sharded ## shard-into-single: Parameterized tests are collected and put into a single shard ## shard-into-multiple: Parameterized tests are collected and sharded into different shards based upon matching names. (Experimental) ## Note: If left blank default is used. Default usage may result in significant increase/difference of shard times observed ## Note: If shard-into-single is used, a single additional shard is created that will run the Parameterized tests separately. ## Note: If shard-into-multiple is used, each parameterized test will be matched by its corresponding name and sharded into a separate shard. ## This may dramatically increase the amount of expected shards depending upon how many parameterized tests are discovered. # parameterized-tests: default flank : # -- FlankYml -- ### Max Test Shards ## test shards - the amount of groups to split the test suite into ## set to -1 to use one shard per test. default: 1 # max-test-shards: 1 ### Shard Time ## shard time - the amount of time tests within a shard should take ## when set to > 0, the shard count is dynamically set based on time up to the maximum limit defined by max-test-shards ## 2 minutes (120) is recommended. ## default: -1 (unlimited) # shard-time: -1 ### Number of Test Runs ## test runs - the amount of times to run the tests. ## 1 runs the tests once. 10 runs all the tests 10x # num-test-runs: 1 ### Smart Flank GCS Path ## Google cloud storage path where the JUnit XML results from the last run is stored. ## NOTE: Empty results will not be uploaded # smart-flank-gcs-path: gs://tmp_flank/tmp/JUnitReport.xml ### Smart Flank Upload Disable flag ## Disables smart flank JUnit XML uploading. Useful for preventing timing data from being updated. ## Default: false # smart-flank-disable-upload: false ### Use Average Test Time for New Tests flag ## Enable using average time from previous tests duration when using SmartShard and tests did not run before. ## Default: false # use-average-test-time-for-new-tests: true ### Default Test Time ## Set default test time used for calculating shards. ## Default: 120.0 # default-test-time: 15 ### Default Class Test Time ## Set default test time (in seconds) used for calculating shards of parametrized classes when previous tests results are not available. ## Default test time for classes should be different from the default time for test ## Default: 240.0 # default-class-test-time: 30 ### Disable Sharding flag ## Disables sharding. Useful for parameterized tests. # disable-sharding: false ### Test Targets Always Run ## always run - these tests are inserted at the beginning of every shard ## Execution order is not guaranteed by Flank. Users are responsible for configuring their own device test runner logic. # test-targets-always-run: # - class com.example.app.ExampleUiTest#testPasses ### Files to Download ## regex is matched against bucket paths, for example: 2019-01-09_00:13:06.106000_YCKl/shard_0/NexusLowRes-28-en-portrait/bugreport.txt # files-to-download: # - .*\\.mp4$ ### Billing Project ID ## The billing enabled Google Cloud Platform id name to use # project: flank-open-source ### Local Results Directory ## Local folder to store the test result. Folder is DELETED before each run to ensure only artifacts from the new run are saved. # local-result-dir: flank ### Keep File Path flag ## Keeps the full path of downloaded files. Required when file names are not unique. ## Default: false # keep-file-path: false ### Additional App/Test APKS ## Include additional app/test apk pairs in the run. Apks are unique by just filename and not by path! ## If app is omitted, then the top level app is used for that pair. ## You can overwrite global config per each test pair. ## Currently supported options are: max-test-shards, test-targets, client-details, environment-variables, device # additional-app-test-apks: # - app: ../test_projects/android/apks/app-debug.apk # test: ../test_projects/android/apks/app1-debug-androidTest.apk # device: # - model: Nexus6P # version: 27 # - test: ../test_projects/android/apks/app2-debug-androidTest.apk # max-test-shards: 5 ### Run Timeout ## The max time this test run can execute before it is cancelled (default: unlimited). # run-timeout: 60m ### Ignore Failed Test flag ## Terminate with exit code 0 when there are failed tests. ## Useful for Fladle and other gradle plugins that don't expect the process to have a non-zero exit code. ## The JUnit XML is used to determine failure. (default: false) # ignore-failed-tests: true ### Legacy Junit Results flag ## Flank provides two ways for parsing junit xml results. ## New way uses google api instead of merging xml files, but can generate slightly different output format. ## This flag allows fallback for legacy xml junit results parsing ## Currently available for android, iOS still uses only legacy way. # legacy-junit-result: false ### Output Style flag ## Output style of execution status. May be one of [verbose, multi, single, compact]. ## For runs with only one test execution the default value is 'verbose', in other cases ## 'multi' is used as the default. The output style 'multi' is not displayed correctly on consoles ## which don't support ansi codes, to avoid corrupted output use single or verbose. ## The output style `compact` is used to produce less detailed output, it prints just Args, test and matrix count, weblinks, cost, and result reports. # output-style: single ### Full Junit Result flag ## Enable create additional local junit result on local storage with failure nodes on passed flaky tests. # full-junit-result: false ### Disable Results Upload flag ## Disables flank results upload on gcloud storage. ## Default: false # disable-results-upload: false ### Disable usage statistics flag ## Disable sending usage statistics (without sensitive data) to the analytic tool. ## Default: false # disable-usage-statistics: false ### Enable output report with set type ## Saves output results as parsable file and optionally upload it to Gcloud. Possible values are [none, json]. ## Default: none # output-report: none ### Disable config validation (for both, yml and command line) ## If true, Flank won't validate options provided by the user. In general, it's not a good idea but, ## there are cases when this could be useful for a user ## (example: project can use devices that are not commonly available, the project has higher sharding limits, etc). ## Default: false # skip-config-validation: false ### Path to the custom sharding JSON file ## Flank will apply provided sharding to the configuration. ## For detailed explanation please check https://github.com/Flank/flank/blob/master/docs/feature/1665-custom-sharding.md # custom-sharding-json: ./custom_sharding.json Flank Wrapper Flank wrapper is a solution to always run the latest version of Flank. It will download the latest version of Flank itself always when it changed. Using Flank wrapper is similar to using Flank, all options provided to Flank wrapper will be passed to Flank itself. To download the latest version of Flank wrapper, please visit GitHub releases and search for tag flank_wrapper-XXX . There are also shell and a batch wrapper over .jar file included. Android code coverage Update your app's build.gradle to build with coverage and use orchestrator. A custom gradle task is defined to generate the coverage report. def coverageEnabled = project.hasProperty('coverage') android { defaultConfig { testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\" // runs pm clear after each test invocation testInstrumentationRunnerArguments clearPackageData: 'true' } buildTypes { debug { testCoverageEnabled true } } // https://google.github.io/android-gradle-dsl/current/com.android.build.gradle.internal.dsl.TestOptions.html#com.android.build.gradle.internal.dsl.TestOptions:animationsDisabled testOptions { execution 'ANDROIDX_TEST_ORCHESTRATOR' animationsDisabled = true } } dependencies { androidTestUtil 'androidx.test:orchestrator:1.1.1' androidTestImplementation(\"androidx.test:runner:1.1.1\") androidTestImplementation(\"androidx.test.ext:junit:1.1.0\") androidTestImplementation(\"androidx.test.ext:junit-ktx:1.1.0\") androidTestImplementation(\"androidx.test.ext:truth:1.1.0\") androidTestImplementation(\"androidx.test.espresso.idling:idling-concurrent:3.1.1\") androidTestImplementation(\"androidx.test.espresso.idling:idling-net:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-accessibility:3.1.1\") androidTestImplementation(\"androidx.test:rules:1.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-core:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-contrib:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-idling-resource:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-intents:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-web:3.1.1\") } if (coverageEnabled) { // gradle -Pcoverage firebaseJacoco task firebaseJacoco(type: JacocoReport) { group = \"Reporting\" description = \"Generate Jacoco coverage reports for Firebase test lab.\" def excludes = [ '**/R.class', '**/R$*.class', '**/BuildConfig.*', \"**/androidx\"] def javaClasses = fileTree(dir: \"${project.buildDir}/intermediates/javac/debug/classes\", excludes: excludes) def kotlinClasses = fileTree(dir: \"${project.buildDir}/tmp/kotlin-classes/debug\", excludes: excludes) getClassDirectories().setFrom(files([javaClasses, kotlinClasses])) getSourceDirectories().setFrom(files([ 'src/main/java', 'src/main/kotlin', 'src/androidTest/java', 'src/androidTest/kotlin'])) def ecFiles = project.fileTree(dir: '..', include: 'results/coverage_ec/**/sdcard/*.ec') ecFiles.forEach { println(\"Reading in $it\") } getExecutionData().setFrom(ecFiles) reports { html { enabled true } xml { enabled false } } } } Starting from Android Marshmallow we must grant runtime permissions to write to external storage. Following snippet in test class solves that issue. If you want to get coverage files when using orchestrator, you must set this Rule for each test class. import androidx.test.rule.GrantPermissionRule ; import static android.Manifest.permission.READ_EXTERNAL_STORAGE ; import static android.Manifest.permission.WRITE_EXTERNAL_STORAGE ; class MyEspressoTest { @Rule GrantPermissionRule grantPermissionRule = GrantPermissionRule . grant ( READ_EXTERNAL_STORAGE , WRITE_EXTERNAL_STORAGE ); // other configuration and tests } Here's an example flank.yml. Note that `coverage` and `coverageFilePath` must be set when using orchestrator with coverage. `coverageFile` is not used. Orchestrator will generate one coverage file per test. `coverageFilePath` must be a directory, not a file. gcloud : app : ./app/build/outputs/apk/debug/app-debug.apk test : ./app/build/outputs/apk/androidTest/debug/app-debug-androidTest.apk environment-variables : coverage : true coverageFilePath : /sdcard/ clearPackageData : true directories-to-pull : - /sdcard/ # use a named results dir that's used by the gradle task results-dir : coverage_ec flank : disableSharding : true files-to-download : - .*/sdcard/[^/]+\\.ec$ - Build the app with coverage: `./gradlew -Pcoverage build` - Run flank `flank android run` - Generate the report `./gradlew -Pcoverage firebaseJacoco` - Open the report in `./build/reports/jacoco/firebaseJacoco/html/index.html` CI integration Download Flank from GitHub releases. Stable. Get the latest stable version number and replace the XXX with the version number. wget --quiet https://github.com/Flank/flank/releases/download/vXXX/flank.jar -O ./flank.jar java -jar ./flank.jar android run Snapshot (published after every commit) wget --quiet https://github.com/Flank/flank/releases/download/flank_snapshot/flank.jar -O ./flank.jar java -jar ./flank.jar android run In CI, it may be useful to generate the file via a shell script: cat << 'EOF' > ./flank.yml gcloud: app: ../../test_projects/android/apks/app-debug.apk test: ../../test_projects/android/apks/app-debug-androidTest.apk EOF Circle CI Circle CI has a firebase testlab orb that supports Flank. Bitrise Bitrise has an official flank step . Gradle Plugin Fladle is a Gradle plugin for Flank that provides DSL configuration and task based execution. Flank on Windows In order to build or run Flank using Windows please follow guide of building/running it using Windows WSL. Native support is not currently supported. Authenticate with a Google account Run flank auth login . Flank will save the credential to ~/.flank . Google account authentication allows each person to have a unique non-shared credential. A service account is still recommended for CI. Authenticate with a service account Follow the test lab docs to create a service account. - Save the credential to $HOME/.config/gcloud/application_default_credentials.json or set GOOGLE_APPLICATION_CREDENTIALS when using a custom path. - Set the project id in flank.yml or set the GOOGLE_CLOUD_PROJECT environment variable. - (Since 21.01) if projectId is not set in a config yml file, flank uses the first available project ID among the following sources: 1. The project ID specified in the JSON credentials file pointed by the GOOGLE_APPLICATION_CREDENTIALS environment variable fladle 1. The project ID specified by the GOOGLE_CLOUD_PROJECT environment variable 1. The project ID specified in the JSON credentials file $HOME/.config/gcloud/application_default_credentials.json For continuous integration, base64 encode the credential as GCLOUD_KEY . Then write the file using a shell script. Note that gcloud CLI does not need to be installed. Flank works without any dependency on gcloud CLI. Encode JSON locally. base64 -i \" $HOME /.config/gcloud/application_default_credentials.json\" | pbcopy Then in CI decode the JSON. GCLOUD_DIR = \" $HOME /.config/gcloud/\" mkdir -p \" $GCLOUD_DIR \" echo \" $GCLOUD_KEY \" | base64 --decode > \" $GCLOUD_DIR /application_default_credentials.json\" Running with gcloud directly flank.yml is compatible with the gcloud CLI. gcloud firebase test android run flank.yml:gcloud gcloud alpha firebase test ios run flank.ios.yml:gcloud NOTE: You will need to activate gcloud's service account for the above commands to work. Doctor Use the doctor command to check for errors in the YAML. flank firebase test android doctor flank firebase test ios doctor Check version Flank supports printing the current version. $ flank -v v3.0-SNAPSHOT Maven You can consume Flank via maven. See the maven repo for all supported versions. repositories { maven(url = \"https://dl.bintray.com/flank/maven\") } dependencies { compile(\"flank:flank:master-SNAPSHOT\") } or GitHub packages Groovy dependencies { implementation \"com.github.flank:flank:<latest version>\" } Kotlin dependencies { implementation ( \"com.github.flank:flank:<latest version>\" ) } Gradle Enterprise Export API It is possible to fetch metrics from Gradle builds. For detailed info please visit Gradle Export API and flank's example gradle-export-api . FAQ 1) > Access Not Configured. Cloud Tool Results API has not been used in project 764086051850 before or it is disabled. This error means authentication hasn't been setup properly. See `Authenticate with a service account` in this readme. 2) > How do I use Flank without typing long commands? Add Flank ' s [ bash helper folder ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / ) to your $PATH environment variable . This will allow you to call the shell scripts in that helper folder from anywhere . With the [ flank ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / flank ) shell script , you can use `flank` instead of `java -jar flank.jar` . Examples: - `flank android run` - `flank ios run` With the [ update_flank . sh ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / update_flank . sh ) shell script , you can rebuild `flank.jar` . 3) > Test run failed to complete. Expected 786 tests, received 660 Try setting `use-orchestrator: false`. Parameterized tests [are not compatible with orchestrator](https://stackoverflow.com/questions/48735268/unable-to-run-parameterized-tests-with-android-test-orchestrator). Flank uses [orchestrator by default on Android.](https://developer.android.com/training/testing/junit-runner) 4) > I have an issue when attempting to sync the Flank Gradle project Task 'prepareKotlinBuildScriptModel' not found in project ':test_runner'. or similar - Make sure you do not change any module specific settings for Gradle - Clear IDE cache using ` File > Invalidate Caches / Restart ` - Re - import project using root ` build . gradle . kts ` - Sync project again 5) > Does Flank support Cucumber? Please check document for more info 6) > How can I find project id? Please check the firebase documentation about finding the project id 7) > How do I run Flank with a proxy? java -Dhttp.proxyHost=localhost -Dhttp.proxyPort=8080 -Dhttp.proxyUser=user -Dhttp.proxyPassword=pass -jar ./test_runner/build/libs/flank.jar firebase test android run See google-auth-library-java for details. Resources Instrumenting Firebase Test Lab","title":"Home"},{"location":"#flank","text":"Flank is a massively parallel Android and iOS test runner for Firebase Test Lab . Flank is YAML compatible with the gcloud CLI . Flank provides extra features to accelerate velocity and increase quality.","title":"Flank"},{"location":"#download","text":"https://github.com/Flank/flank/releases/latest/download/flank.jar","title":"Download"},{"location":"#sponsors","text":"See error monitoring docs to disable Sentry error monitoring.","title":"Sponsors"},{"location":"#contributing","text":"Install JDK 15 (it works also correctly on the previous version, a newer version is not guaranteed to work properly): Oracle OpenJDK AdoptJDK Use JetBrains Toolbox to install IntelliJ IDEA Community Clone the repo git clone --recursive https://github.com/Flank/flank.git git submodule update --init --recursive updates the submodules Open build.gradle.kts in the main Flank base directory with IntelliJ IDEA Community , this will open the entire Flank mono repo test runner contributions can be made in the test_runner\\ subdirectory","title":"Contributing"},{"location":"#features","text":"Test sharding Cost reporting Stability testing HTML report JUnit XML report Smart Flank","title":"Features"},{"location":"#exit-codes","text":"Exit code Description 0 All tests passed 1 A general failure occurred. Possible causes include: a filename that does not exist or an HTTP/network error. 2 Usually indicates missing or wrong usage of flags, incorrect parameters, errors in config files. 10 At least one matrix not finished (usually a FTL internal error) or unexpected error occurred. 15 Firebase Test Lab could not determine if the test matrix passed or failed, because of an unexpected error. 18 The test environment for this test execution is not supported because of incompatible test dimensions. This error might occur if the selected Android API level is not supported by the selected device type. 19 The test matrix was canceled by the user. 20 A test infrastructure error occurred.","title":"Exit Codes"},{"location":"#cli","text":"Flank supports CLI flags for each YAML parameter. The CLI flags are useful to selectively override YAML file values. Pass the --help flag to see the full documentation. For example: flank android run --help CLI flags work well with environment variables. You can override a value like this: flank android run --local-result-dir=$APP_NAME","title":"CLI"},{"location":"#flank-configuration","text":"app, test, and xctestrun-file support ~ , environment variables, and globs ( , *) when resolving paths","title":"Flank configuration"},{"location":"#ios-example","text":"Run test_runner/flank.ios.yml with flank to verify iOS execution is working. ./gradlew clean test_runner:build test_runner:shadowJar java -jar ./test_runner/build/libs/flank-*.jar firebase test ios run # gcloud args match the official gcloud cli # https://cloud.google.com/sdk/gcloud/reference/alpha/firebase/test/ios/run gcloud : # -- GcloudYml -- ### Results Bucket ## The name of a Google Cloud Storage bucket where raw test results will be stored # results-bucket: tmp_flank ### Results Directory ## The name of a unique Google Cloud Storage object within the results bucket where raw test results will be stored ## (default: a timestamp with a random suffix). # results-dir: tmp ### Record Video flag ## Enable video recording during the test. Disabled by default. Use --record-video to enable. # record-video: true ### Timeout ## The max time this test execution can run before it is cancelled (default: 15m). ## It does not include any time necessary to prepare and clean up the target device. ## The maximum possible testing time is 45m on physical devices and 60m on virtual devices. ## The TIMEOUT units can be h, m, or s. If no unit is given, seconds are assumed. # timeout: 30m ### Asynchronous flag ## Invoke a test asynchronously without waiting for test results. # async: false ### Client Details ## A key-value map of additional details to attach to the test matrix. ## Arbitrary key-value pairs may be attached to a test matrix to provide additional context about the tests being run. ## When consuming the test results, such as in Cloud Functions or a CI system, ## these details can add additional context such as a link to the corresponding pull request. # client-details # key1: value1 # key2: value2 ### Network Profile ## The name of the network traffic profile, for example LTE, HSPA, etc, ## which consists of a set of parameters to emulate network conditions when running the test ## (default: no network shaping; see available profiles listed by the `flank test network-profiles list` command). ## This feature only works on physical devices. # network-profile: LTE ### Result History Name ## The history name for your test results (an arbitrary string label; default: the application's label from the APK manifest). ## All tests which use the same history name will have their results grouped together in the Firebase console in a time-ordered test history list. # results-history-name: android-history ### Number of Flaky Test Attempts ## The number of times a TestExecution should be re-attempted if one or more\\nof its test cases fail for any reason. ## The maximum number of reruns allowed is 10. Default is 0, which implies no reruns. # num-flaky-test-attempts: 0 ### Fail Fast ## If true, only a single attempt at most will be made to run each execution/shard in the matrix. ## Flaky test attempts are not affected. Normally, 2 or more attempts are made if a potential ## infrastructure issue is detected. This feature is for latency sensitive workloads. The ## incidence of execution failures may be significantly greater for fail-fast matrices and support ## is more limited because of that expectation. # fail-fast: false # -- IosGcloudYml -- ### IOS Test Package Path ## The path to the test package (a zip file containing the iOS app and XCTest files). ## The given path may be in the local filesystem or in Google Cloud Storage using a URL beginning with gs://. ## Note: any .xctestrun file in this zip file will be ignored if --xctestrun-file is specified. test : ./src/test/kotlin/ftl/fixtures/tmp/earlgrey_example.zip ### IOS XCTestrun File Path ## The path to an .xctestrun file that will override any .xctestrun file contained in the --test package. ## Because the .xctestrun file contains environment variables along with test methods to run and/or ignore, ## this can be useful for customizing or sharding test suites. The given path should be in the local filesystem. ## Note: this path should usually be pointing to the xctestrun file within the derived data folder ## For example ./derivedDataPath/Build/Products/EarlGreyExampleSwiftTests_iphoneos13.4-arm64e.xctestrun xctestrun-file : ./src/test/kotlin/ftl/fixtures/tmp/EarlGreyExampleSwiftTests_iphoneos13.4-arm64e.xctestrun ### Xcode Version ## The version of Xcode that should be used to run an XCTest. ## Defaults to the latest Xcode version supported in Firebase Test Lab. ## This Xcode version must be supported by all iOS versions selected in the test matrix. # xcode-version: 10.1 ### IOS Device Parameters ## A list of DIMENSION=VALUE pairs which specify a target device to test against. ## This flag may be repeated to specify multiple devices. ## The four device dimensions are: model, version, locale, and orientation. # device: # - model: iphone8 # version: 12.0 # locale: en # orientation: portrait # - model: iphonex # version: 12.0 # locale: es_ES # orientation: landscape ### Directories to Pull ## A list of paths that will be copied from the device's storage to the designated results bucket after the test ## is complete. These must be absolute paths under /private/var/mobile/Media or /Documents ## of the app under test. If the path is under an app's /Documents, it must be prefixed with the app's bundle id and a colon # directories-to-pull: # - /private/var/mobile/Media ### Other File paths ## A list of device-path=file-path pairs that specify the paths of the test device and the files you want pushed to the device prior to testing. ## Device paths should either be under the Media shared folder (e.g. prefixed with /private/var/mobile/Media) or ## within the documents directory of the filesystem of an app under test (e.g. /Documents). Device paths to app ## filesystems should be prefixed by the bundle ID and a colon. Source file paths may be in the local filesystem or in Google Cloud Storage (gs://\u2026). # other-files # com.my.app:/Documents/file.txt: local/file.txt # /private/var/mobile/Media/file.jpg: gs://bucket/file.jpg ### Additional IPA's ## List of up to 100 additional IPAs to install, in addition to the one being directly tested. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. # additional-ipas: # - gs://bucket/additional.ipa # - path/to/local/ipa/file.ipa ### Scenario Numbers ## A list of game-loop scenario numbers which will be run as part of the test (default: all scenarios). ## A maximum of 1024 scenarios may be specified in one test matrix, but the maximum number may also be limited by the overall test --timeout setting. # scenario-numbers: # - 1 # - 2 # - 3 ### Test type ## The type of iOS test to run. TYPE must be one of: xctest, game-loop. Default: xctest # type: xctest ### Application Path ## The path to the application archive (.ipa file) for game-loop testing. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## This flag is only valid when --type=game-loop is also set # app: # - gs://bucket/additional.ipa OR path/to/local/ipa/file.ipa ### Testing with Special Entitlements ## Enables testing special app entitlements. Re-signs an app having special entitlements with a new application-identifier. ## This currently supports testing Push Notifications (aps-environment) entitlement for up to one app in a project. ## Note: Because this changes the app's identifier, make sure none of the resources in your zip file contain direct references to the test app's bundle id. # test-special-entitlements: false flank : # -- FlankYml -- ### Max Test Shards ## test shards - the amount of groups to split the test suite into ## set to -1 to use one shard per test. default: 1 # max-test-shards: 1 ## Shard Time ## shard time - the amount of time tests within a shard should take ## when set to > 0, the shard count is dynamically set based on time up to the maximum limit defined by max-test-shards ## 2 minutes (120) is recommended. ## default: -1 (unlimited) # shard-time: -1 ### Number of Test Runs ## test runs - the amount of times to run the tests. ## 1 runs the tests once. 10 runs all the tests 10x # num-test-runs: 1 ### Smart Flank GCS Paths ## Google cloud storage path to store the JUnit XML results from the last run. ## NOTE: Empty results will not be uploaded # smart-flank-gcs-path: gs://tmp_flank/flank/test_app_ios.xml ### Smart Flank Disable Upload flag ## Disables smart flank JUnit XML uploading. Useful for preventing timing data from being updated. ## Default: false # smart-flank-disable-upload: false ### Use Average Test Time For New Tests flag ## Enable using average time from previous tests duration when using SmartShard and tests did not run before. ## Default: false # use-average-test-time-for-new-tests: true ### Default Test Time ## Set default test time used for calculating shards. ## Default: 120.0 # default-test-time: 15 ### Default Class Test Time ## Set default test time (in seconds) used for calculating shards of parametrized classes when previous tests results are not available. ## Default test time for classes should be different from the default time for test ## Default: 240.0 # default-class-test-time: 30 ### Disable Sharding flag ## Disables sharding. Useful for parameterized tests. # disable-sharding: false ### Test targets always Run ## always run - these tests are inserted at the beginning of every shard ## Execution order is not guaranteed by Flank. Users are responsible for configuring their own device test runner logic. # test-targets-always-run: # - className/testName ### Files to Download ## regex is matched against bucket paths, for example: 2019-01-09_00:18:07.314000_hCMY/shard_0/EarlGreyExampleSwiftTests_iphoneos12.1-arm64e.xctestrun # files-to-download: # - .*\\.mp4$ # -- IosFlankYml -- ### Test Targets ## test targets - a list of tests to run. omit to run all tests. # test-targets: # - className/testName ### Billing Project ID ## The billing enabled Google Cloud Platform project id to use # project: flank-open-source ### Local Result Directory Storage ## Local folder to store the test result. Folder is DELETED before each run to ensure only artifacts from the new run are saved. # local-result-dir: flank ### Run Timeout ## The max time this test run can execute before it is cancelled (default: unlimited). # run-timeout: 60m ### Keep File Path flag ## Keeps the full path of downloaded files. Required when file names are not unique. ## Default: false # keep-file-path: false ### Ignore Failed Tests flag ## Terminate with exit code 0 when there are failed tests. ## Useful for Fladle and other gradle plugins that don't expect the process to have a non-zero exit code. ## The JUnit XML is used to determine failure. (default: false) # ignore-failed-tests: true ### Output Style flag ## Output style of execution status. May be one of [verbose, multi, single, compact]. ## For runs with only one test execution the default value is 'verbose', in other cases ## 'multi' is used as the default. The output style 'multi' is not displayed correctly on consoles ## which don't support ansi codes, to avoid corrupted output use single or verbose. ## The output style `compact` is used to produce less detailed output, it prints just Args, test and matrix count, weblinks, cost, and result reports. # output-style: single ### Full Junit Result flag ## Enable create additional local junit result on local storage with failure nodes on passed flaky tests. # full-junit-result: false ### Disable Result Upload flag ## Disables flank results upload on gcloud storage. ## Default: false # disable-results-upload: false ### Disable usage statistics flag ## Disable sending usage statistics (without sensitive data) to the analytic tool. ## Default: false # disable-usage-statistics: false ### Only Test Configuration ## Constrains a test action to only test a specified test configuration within a test plan and exclude all other test configurations. ## Flank can combine multiple constraint options, but -only-test-configuration has precedence over -skip-test-configuration. ## Each test configuration name must match the name of a configuration specified in a test plan and is case-sensitive. ## Default: null (run all test configurations) # only-test-configuration: en ### Skip Test Configuration ## Constrains a test action to skip a specified test configuration and include all other test configurations. ## Flank can combine multiple constraint options, but -only-test-configuration has precedence over -skip-test-configuration. ## Each test configuration name must match the name of a configuration specified in a test plan and is case-sensitive. ## Default: null (run all test configurations) # skip-test-configuration: en ### Enable output report with set type ## Saves output results as parsable file and optionally upload it to Gcloud.. ## Default: none # output-report: none ### Disable config validation (for both, yml and command line) ## If true, Flank won't validate options provided by the user. In general, it's not a good idea but, ## there are cases when this could be useful for a user ## (example: project can use devices that are not commonly available, the project has higher sharding limits, etc). ## Default: false # skip-config-validation: false ### Path to the custom sharding JSON file ## Flank will apply provided sharding to the configuration. ## For detailed explanation please check https://github.com/Flank/flank/blob/master/docs/feature/1665-custom-sharding.md # custom-sharding-json: ./custom_sharding.json","title":"iOS example"},{"location":"#android-example","text":"Run test_runner/flank.yml with flank to verify Android execution is working. ./gradlew clean test_runner:build test_runner:shadowJar java -jar ./test_runner/build/libs/flank-*.jar firebase test android run # gcloud args match the official gcloud cli # See the docs for full gcloud details https://cloud.google.com/sdk/gcloud/reference/firebase/test/android/run gcloud : # -- GcloudYml -- ### Result Bucket ## The name of a Google Cloud Storage bucket where raw test results will be stored # results-bucket: tmp_flank ### Result Directory ## The name of a unique Google Cloud Storage object within the results bucket where raw test results will be stored ## (default: a timestamp with a random suffix). # results-dir: tmp ### Record Video flag ## Enable video recording during the test. Disabled by default. Use --record-video to enable. # record-video: true ### Timeout ## The max time this test execution can run before it is cancelled (default: 15m). ## It does not include any time necessary to prepare and clean up the target device. ## The maximum possible testing time is 45m on physical devices and 60m on virtual devices. ## The TIMEOUT units can be h, m, or s. If no unit is given, seconds are assumed. # timeout: 30m ### Asynchronous flag ## Invoke a test asynchronously without waiting for test results. # async: false ### Client Details ## A key-value map of additional details to attach to the test matrix. ## Arbitrary key-value pairs may be attached to a test matrix to provide additional context about the tests being run. ## When consuming the test results, such as in Cloud Functions or a CI system, ## these details can add additional context such as a link to the corresponding pull request. # client-details # key1: value1 # key2: value2 ### Network Profile ## The name of the network traffic profile, for example LTE, HSPA, etc, ## which consists of a set of parameters to emulate network conditions when running the test ## (default: no network shaping; see available profiles listed by the `flank test network-profiles list` command). ## This feature only works on physical devices. # network-profile: LTE ### Result History Name ## The history name for your test results (an arbitrary string label; default: the application's label from the APK manifest). ## All tests which use the same history name will have their results grouped together in the Firebase console in a time-ordered test history list. # results-history-name: android-history ### Number of Flaky Test Attempts ## The number of times a TestExecution should be re-attempted if one or more\\nof its test cases fail for any reason. ## The maximum number of reruns allowed is 10. Default is 0, which implies no reruns. # num-flaky-test-attempts: 0 ### Fail Fast ## If true, only a single attempt at most will be made to run each execution/shard in the matrix. ## Flaky test attempts are not affected. Normally, 2 or more attempts are made if a potential ## infrastructure issue is detected. This feature is for latency sensitive workloads. The ## incidence of execution failures may be significantly greater for fail-fast matrices and support ## is more limited because of that expectation. # fail-fast: false # -- AndroidGcloudYml -- ## Android Application Path ## The path to the application binary file. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## Android App Bundles are specified as .aab, all other files are assumed to be APKs. app : ../test_projects/android/apks/app-debug.apk ### Android Binary File Path ## The path to the binary file containing instrumentation tests. ## The given path may be in the local filesystem or in Google Cloud Storage using a URL beginning with gs://. test : ../test_projects/android/apks/app-debug-androidTest.apk ### Additional APK's ## A list of up to 100 additional APKs to install, in addition to those being directly tested. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. # additional-apks: additional-apk1.apk,additional-apk2.apk,additional-apk3.apk ### Auto Google Login flag ## Automatically log into the test device using a preconfigured Google account before beginning the test. ## Disabled by default. Use --auto-google-login to enable. # auto-google-login: true ### Use Orchestrator Flag ## Whether each test runs in its own Instrumentation instance with the Android Test Orchestrator ## (default: Orchestrator is used). Disable with --no-use-orchestrator. ## See https://developer.android.com/training/testing/junit-runner.html#using-android-test-orchestrator # use-orchestrator: true ### Environment Variables ## A comma-separated, key=value map of environment variables and their desired values. This flag is repeatable. ## The environment variables are mirrored as extra options to the am instrument -e KEY1 VALUE1 \u2026 command and ## passed to your test runner (typically AndroidJUnitRunner) # environment-variables: # coverage: true # coverageFilePath: /sdcard/ # clearPackageData: true ### Directories to Pull ## A list of paths that will be copied from the device's storage to the designated results bucket after the test ## is complete. These must be absolute paths under /sdcard or /data/local/tmp # directories-to-pull: # - /sdcard/ ### Grant Permissions flag ## Whether to grant runtime permissions on the device before the test begins. ## By default, all permissions are granted. PERMISSIONS must be one of: all, none # grant-permissions: all ### Test Type ## The type of test to run. TYPE must be one of: instrumentation, robo, game-loop. # type: instrumentation ### Other Files ## A list of device-path: file-path pairs that indicate the device paths to push files to the device before starting tests, and the paths of files to push. ## Device paths must be under absolute, whitelisted paths (${EXTERNAL_STORAGE}, or ${ANDROID_DATA}/local/tmp). ## Source file paths may be in the local filesystem or in Google Cloud Storage (gs://\u2026). # other-files # - /sdcard/dir1/file1.txt: local/file.txt # - /sdcard/dir2/file2.jpg: gs://bucket/file.jpg ### OBB Files ## A list of one or two Android OBB file names which will be copied to each test device before the tests will run (default: None). ## Each OBB file name must conform to the format as specified by Android (e.g. [main|patch].0300110.com.example.android.obb) and will be installed into <shared-storage>/Android/obb/<package-name>/ on the test device. # obb-files: # - local/file/path/test1.obb # - local/file/path/test2.obb ### Scenario Numbers ## A list of game-loop scenario numbers which will be run as part of the test (default: all scenarios). ## A maximum of 1024 scenarios may be specified in one test matrix, but the maximum number may also be limited by the overall test --timeout setting. # scenario-numbers: # - 1 # - 2 # - 3 ### Scenario Labels ## A list of game-loop scenario labels (default: None). Each game-loop scenario may be labeled in the APK manifest file with one or more arbitrary strings, creating logical groupings (e.g. GPU_COMPATIBILITY_TESTS). ## If --scenario-numbers and --scenario-labels are specified together, Firebase Test Lab will first execute each scenario from --scenario-numbers. ## It will then expand each given scenario label into a list of scenario numbers marked with that label, and execute those scenarios. # scenario-labels: # - label1 # - label2 ### OBB filenames ## A list of OBB required filenames. OBB file name must conform to the format as specified by Android e.g. ## [main|patch].0300110.com.example.android.obb which will be installed into <shared-storage>/Android/obb/<package-name>/ on the device. # obb-names: # - [main|patch].<VERSION>.com.example.android.obb ### Performance Metric flag ## Monitor and record performance metrics: CPU, memory, network usage, and FPS (game-loop only). ## Disabled by default. Use --performance-metrics to enable. # performance-metrics: true ### Number of Uniform Shards ## Specifies the number of shards into which you want to evenly distribute test cases. ## The shards are run in parallel on separate devices. For example, ## if your test execution contains 20 test cases and you specify four shards, each shard executes five test cases. ## The number of shards should be less than the total number of test cases. ## The number of shards specified must be >= 1 and <= 50. ## This option cannot be used along max-test-shards and is not compatible with smart sharding. ## If you want to take benefits of smart sharding use max-test-shards instead. ## default: null # num-uniform-shards: 50 ### Instrumentation Test Runner Class ## The fully-qualified Java class name of the instrumentation test runner ## (default: the last name extracted from the APK manifest). # test-runner-class: com.foo.TestRunner ### Test Targets ## A list of one or more test target filters to apply (default: run all test targets). ## Each target filter must be fully qualified with the package name, class name, or test annotation desired. ## Supported test filters by am instrument -e \u2026 include: ## class, notClass, size, annotation, notAnnotation, package, notPackage, testFile, notTestFile ## See https://developer.android.com/reference/android/support/test/runner/AndroidJUnitRunner for more information. # test-targets: # - class com.example.app.ExampleUiTest#testPasses ### Robo Directives ## A map of robo_directives that you can use to customize the behavior of Robo test. ## The type specifies the action type of the directive, which may take on values click, text or ignore. ## If no type is provided, text will be used by default. ## Each key should be the Android resource name of a target UI element and each value should be the text input for that element. ## Values are only permitted for text type elements, so no value should be specified for click and ignore type elements. # robo-directives: # \"text:input_resource_name\": message # \"click:button_resource_name\": \"\" ### Robo Scripts ## The path to a Robo Script JSON file. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## You can guide the Robo test to perform specific actions by recording a Robo Script in Android Studio and then specifying this argument. ## Learn more at https://firebase.google.com/docs/test-lab/robo-ux-test#scripting. # robo-script: path_to_robo_script ### Android Device Parameters ## A list of DIMENSION=VALUE pairs which specify a target device to test against. ## This flag may be repeated to specify multiple devices. ## The four device dimensions are: model, version, locale, and orientation. # device: # - model: NexusLowRes # version: 28 # locale: en # orientation: portrait # - model: NexusLowRes # version: 27 ### test-targets-for-shard ## Specifies a group of packages, classes, and/or test cases to run in each shard (a group of test cases). ## The shards are run in parallel on separate devices. You can repeat this flag up to 50 times to specify multiple shards when one or more physical devices are selected, ## or up to 500 times when no physical devices are selected. ## Note: If you include the flags environment-variable or test-targets when running test-targets-for-shard, the flags are applied to all the shards you create. # test-target-for-shard: # - package com.package1.for.shard1 # - class com.package2.for.shard2.Class ### parameterized-tests ## Specifies how to handle tests which contain the parameterization annotation. ## 4 options are available ## default: treat Parameterized tests as normal and shard accordingly ## ignore-all: Parameterized tests are ignored and not sharded ## shard-into-single: Parameterized tests are collected and put into a single shard ## shard-into-multiple: Parameterized tests are collected and sharded into different shards based upon matching names. (Experimental) ## Note: If left blank default is used. Default usage may result in significant increase/difference of shard times observed ## Note: If shard-into-single is used, a single additional shard is created that will run the Parameterized tests separately. ## Note: If shard-into-multiple is used, each parameterized test will be matched by its corresponding name and sharded into a separate shard. ## This may dramatically increase the amount of expected shards depending upon how many parameterized tests are discovered. # parameterized-tests: default flank : # -- FlankYml -- ### Max Test Shards ## test shards - the amount of groups to split the test suite into ## set to -1 to use one shard per test. default: 1 # max-test-shards: 1 ### Shard Time ## shard time - the amount of time tests within a shard should take ## when set to > 0, the shard count is dynamically set based on time up to the maximum limit defined by max-test-shards ## 2 minutes (120) is recommended. ## default: -1 (unlimited) # shard-time: -1 ### Number of Test Runs ## test runs - the amount of times to run the tests. ## 1 runs the tests once. 10 runs all the tests 10x # num-test-runs: 1 ### Smart Flank GCS Path ## Google cloud storage path where the JUnit XML results from the last run is stored. ## NOTE: Empty results will not be uploaded # smart-flank-gcs-path: gs://tmp_flank/tmp/JUnitReport.xml ### Smart Flank Upload Disable flag ## Disables smart flank JUnit XML uploading. Useful for preventing timing data from being updated. ## Default: false # smart-flank-disable-upload: false ### Use Average Test Time for New Tests flag ## Enable using average time from previous tests duration when using SmartShard and tests did not run before. ## Default: false # use-average-test-time-for-new-tests: true ### Default Test Time ## Set default test time used for calculating shards. ## Default: 120.0 # default-test-time: 15 ### Default Class Test Time ## Set default test time (in seconds) used for calculating shards of parametrized classes when previous tests results are not available. ## Default test time for classes should be different from the default time for test ## Default: 240.0 # default-class-test-time: 30 ### Disable Sharding flag ## Disables sharding. Useful for parameterized tests. # disable-sharding: false ### Test Targets Always Run ## always run - these tests are inserted at the beginning of every shard ## Execution order is not guaranteed by Flank. Users are responsible for configuring their own device test runner logic. # test-targets-always-run: # - class com.example.app.ExampleUiTest#testPasses ### Files to Download ## regex is matched against bucket paths, for example: 2019-01-09_00:13:06.106000_YCKl/shard_0/NexusLowRes-28-en-portrait/bugreport.txt # files-to-download: # - .*\\.mp4$ ### Billing Project ID ## The billing enabled Google Cloud Platform id name to use # project: flank-open-source ### Local Results Directory ## Local folder to store the test result. Folder is DELETED before each run to ensure only artifacts from the new run are saved. # local-result-dir: flank ### Keep File Path flag ## Keeps the full path of downloaded files. Required when file names are not unique. ## Default: false # keep-file-path: false ### Additional App/Test APKS ## Include additional app/test apk pairs in the run. Apks are unique by just filename and not by path! ## If app is omitted, then the top level app is used for that pair. ## You can overwrite global config per each test pair. ## Currently supported options are: max-test-shards, test-targets, client-details, environment-variables, device # additional-app-test-apks: # - app: ../test_projects/android/apks/app-debug.apk # test: ../test_projects/android/apks/app1-debug-androidTest.apk # device: # - model: Nexus6P # version: 27 # - test: ../test_projects/android/apks/app2-debug-androidTest.apk # max-test-shards: 5 ### Run Timeout ## The max time this test run can execute before it is cancelled (default: unlimited). # run-timeout: 60m ### Ignore Failed Test flag ## Terminate with exit code 0 when there are failed tests. ## Useful for Fladle and other gradle plugins that don't expect the process to have a non-zero exit code. ## The JUnit XML is used to determine failure. (default: false) # ignore-failed-tests: true ### Legacy Junit Results flag ## Flank provides two ways for parsing junit xml results. ## New way uses google api instead of merging xml files, but can generate slightly different output format. ## This flag allows fallback for legacy xml junit results parsing ## Currently available for android, iOS still uses only legacy way. # legacy-junit-result: false ### Output Style flag ## Output style of execution status. May be one of [verbose, multi, single, compact]. ## For runs with only one test execution the default value is 'verbose', in other cases ## 'multi' is used as the default. The output style 'multi' is not displayed correctly on consoles ## which don't support ansi codes, to avoid corrupted output use single or verbose. ## The output style `compact` is used to produce less detailed output, it prints just Args, test and matrix count, weblinks, cost, and result reports. # output-style: single ### Full Junit Result flag ## Enable create additional local junit result on local storage with failure nodes on passed flaky tests. # full-junit-result: false ### Disable Results Upload flag ## Disables flank results upload on gcloud storage. ## Default: false # disable-results-upload: false ### Disable usage statistics flag ## Disable sending usage statistics (without sensitive data) to the analytic tool. ## Default: false # disable-usage-statistics: false ### Enable output report with set type ## Saves output results as parsable file and optionally upload it to Gcloud. Possible values are [none, json]. ## Default: none # output-report: none ### Disable config validation (for both, yml and command line) ## If true, Flank won't validate options provided by the user. In general, it's not a good idea but, ## there are cases when this could be useful for a user ## (example: project can use devices that are not commonly available, the project has higher sharding limits, etc). ## Default: false # skip-config-validation: false ### Path to the custom sharding JSON file ## Flank will apply provided sharding to the configuration. ## For detailed explanation please check https://github.com/Flank/flank/blob/master/docs/feature/1665-custom-sharding.md # custom-sharding-json: ./custom_sharding.json","title":"Android example"},{"location":"#flank-wrapper","text":"Flank wrapper is a solution to always run the latest version of Flank. It will download the latest version of Flank itself always when it changed. Using Flank wrapper is similar to using Flank, all options provided to Flank wrapper will be passed to Flank itself. To download the latest version of Flank wrapper, please visit GitHub releases and search for tag flank_wrapper-XXX . There are also shell and a batch wrapper over .jar file included.","title":"Flank Wrapper"},{"location":"#android-code-coverage","text":"Update your app's build.gradle to build with coverage and use orchestrator. A custom gradle task is defined to generate the coverage report. def coverageEnabled = project.hasProperty('coverage') android { defaultConfig { testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\" // runs pm clear after each test invocation testInstrumentationRunnerArguments clearPackageData: 'true' } buildTypes { debug { testCoverageEnabled true } } // https://google.github.io/android-gradle-dsl/current/com.android.build.gradle.internal.dsl.TestOptions.html#com.android.build.gradle.internal.dsl.TestOptions:animationsDisabled testOptions { execution 'ANDROIDX_TEST_ORCHESTRATOR' animationsDisabled = true } } dependencies { androidTestUtil 'androidx.test:orchestrator:1.1.1' androidTestImplementation(\"androidx.test:runner:1.1.1\") androidTestImplementation(\"androidx.test.ext:junit:1.1.0\") androidTestImplementation(\"androidx.test.ext:junit-ktx:1.1.0\") androidTestImplementation(\"androidx.test.ext:truth:1.1.0\") androidTestImplementation(\"androidx.test.espresso.idling:idling-concurrent:3.1.1\") androidTestImplementation(\"androidx.test.espresso.idling:idling-net:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-accessibility:3.1.1\") androidTestImplementation(\"androidx.test:rules:1.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-core:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-contrib:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-idling-resource:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-intents:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-web:3.1.1\") } if (coverageEnabled) { // gradle -Pcoverage firebaseJacoco task firebaseJacoco(type: JacocoReport) { group = \"Reporting\" description = \"Generate Jacoco coverage reports for Firebase test lab.\" def excludes = [ '**/R.class', '**/R$*.class', '**/BuildConfig.*', \"**/androidx\"] def javaClasses = fileTree(dir: \"${project.buildDir}/intermediates/javac/debug/classes\", excludes: excludes) def kotlinClasses = fileTree(dir: \"${project.buildDir}/tmp/kotlin-classes/debug\", excludes: excludes) getClassDirectories().setFrom(files([javaClasses, kotlinClasses])) getSourceDirectories().setFrom(files([ 'src/main/java', 'src/main/kotlin', 'src/androidTest/java', 'src/androidTest/kotlin'])) def ecFiles = project.fileTree(dir: '..', include: 'results/coverage_ec/**/sdcard/*.ec') ecFiles.forEach { println(\"Reading in $it\") } getExecutionData().setFrom(ecFiles) reports { html { enabled true } xml { enabled false } } } } Starting from Android Marshmallow we must grant runtime permissions to write to external storage. Following snippet in test class solves that issue. If you want to get coverage files when using orchestrator, you must set this Rule for each test class. import androidx.test.rule.GrantPermissionRule ; import static android.Manifest.permission.READ_EXTERNAL_STORAGE ; import static android.Manifest.permission.WRITE_EXTERNAL_STORAGE ; class MyEspressoTest { @Rule GrantPermissionRule grantPermissionRule = GrantPermissionRule . grant ( READ_EXTERNAL_STORAGE , WRITE_EXTERNAL_STORAGE ); // other configuration and tests } Here's an example flank.yml. Note that `coverage` and `coverageFilePath` must be set when using orchestrator with coverage. `coverageFile` is not used. Orchestrator will generate one coverage file per test. `coverageFilePath` must be a directory, not a file. gcloud : app : ./app/build/outputs/apk/debug/app-debug.apk test : ./app/build/outputs/apk/androidTest/debug/app-debug-androidTest.apk environment-variables : coverage : true coverageFilePath : /sdcard/ clearPackageData : true directories-to-pull : - /sdcard/ # use a named results dir that's used by the gradle task results-dir : coverage_ec flank : disableSharding : true files-to-download : - .*/sdcard/[^/]+\\.ec$ - Build the app with coverage: `./gradlew -Pcoverage build` - Run flank `flank android run` - Generate the report `./gradlew -Pcoverage firebaseJacoco` - Open the report in `./build/reports/jacoco/firebaseJacoco/html/index.html`","title":"Android code coverage"},{"location":"#ci-integration","text":"Download Flank from GitHub releases. Stable. Get the latest stable version number and replace the XXX with the version number. wget --quiet https://github.com/Flank/flank/releases/download/vXXX/flank.jar -O ./flank.jar java -jar ./flank.jar android run Snapshot (published after every commit) wget --quiet https://github.com/Flank/flank/releases/download/flank_snapshot/flank.jar -O ./flank.jar java -jar ./flank.jar android run In CI, it may be useful to generate the file via a shell script: cat << 'EOF' > ./flank.yml gcloud: app: ../../test_projects/android/apks/app-debug.apk test: ../../test_projects/android/apks/app-debug-androidTest.apk EOF","title":"CI integration"},{"location":"#circle-ci","text":"Circle CI has a firebase testlab orb that supports Flank.","title":"Circle CI"},{"location":"#bitrise","text":"Bitrise has an official flank step .","title":"Bitrise"},{"location":"#gradle-plugin","text":"Fladle is a Gradle plugin for Flank that provides DSL configuration and task based execution.","title":"Gradle Plugin"},{"location":"#flank-on-windows","text":"In order to build or run Flank using Windows please follow guide of building/running it using Windows WSL. Native support is not currently supported.","title":"Flank on Windows"},{"location":"#authenticate-with-a-google-account","text":"Run flank auth login . Flank will save the credential to ~/.flank . Google account authentication allows each person to have a unique non-shared credential. A service account is still recommended for CI.","title":"Authenticate with a Google account"},{"location":"#authenticate-with-a-service-account","text":"Follow the test lab docs to create a service account. - Save the credential to $HOME/.config/gcloud/application_default_credentials.json or set GOOGLE_APPLICATION_CREDENTIALS when using a custom path. - Set the project id in flank.yml or set the GOOGLE_CLOUD_PROJECT environment variable. - (Since 21.01) if projectId is not set in a config yml file, flank uses the first available project ID among the following sources: 1. The project ID specified in the JSON credentials file pointed by the GOOGLE_APPLICATION_CREDENTIALS environment variable fladle 1. The project ID specified by the GOOGLE_CLOUD_PROJECT environment variable 1. The project ID specified in the JSON credentials file $HOME/.config/gcloud/application_default_credentials.json For continuous integration, base64 encode the credential as GCLOUD_KEY . Then write the file using a shell script. Note that gcloud CLI does not need to be installed. Flank works without any dependency on gcloud CLI. Encode JSON locally. base64 -i \" $HOME /.config/gcloud/application_default_credentials.json\" | pbcopy Then in CI decode the JSON. GCLOUD_DIR = \" $HOME /.config/gcloud/\" mkdir -p \" $GCLOUD_DIR \" echo \" $GCLOUD_KEY \" | base64 --decode > \" $GCLOUD_DIR /application_default_credentials.json\"","title":"Authenticate with a service account"},{"location":"#running-with-gcloud-directly","text":"flank.yml is compatible with the gcloud CLI. gcloud firebase test android run flank.yml:gcloud gcloud alpha firebase test ios run flank.ios.yml:gcloud NOTE: You will need to activate gcloud's service account for the above commands to work.","title":"Running with gcloud directly"},{"location":"#doctor","text":"Use the doctor command to check for errors in the YAML. flank firebase test android doctor flank firebase test ios doctor","title":"Doctor"},{"location":"#check-version","text":"Flank supports printing the current version. $ flank -v v3.0-SNAPSHOT","title":"Check version"},{"location":"#maven","text":"You can consume Flank via maven. See the maven repo for all supported versions. repositories { maven(url = \"https://dl.bintray.com/flank/maven\") } dependencies { compile(\"flank:flank:master-SNAPSHOT\") } or GitHub packages Groovy dependencies { implementation \"com.github.flank:flank:<latest version>\" } Kotlin dependencies { implementation ( \"com.github.flank:flank:<latest version>\" ) }","title":"Maven"},{"location":"#gradle-enterprise-export-api","text":"It is possible to fetch metrics from Gradle builds. For detailed info please visit Gradle Export API and flank's example gradle-export-api .","title":"Gradle Enterprise Export API"},{"location":"#faq","text":"1) > Access Not Configured. Cloud Tool Results API has not been used in project 764086051850 before or it is disabled. This error means authentication hasn't been setup properly. See `Authenticate with a service account` in this readme. 2) > How do I use Flank without typing long commands? Add Flank ' s [ bash helper folder ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / ) to your $PATH environment variable . This will allow you to call the shell scripts in that helper folder from anywhere . With the [ flank ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / flank ) shell script , you can use `flank` instead of `java -jar flank.jar` . Examples: - `flank android run` - `flank ios run` With the [ update_flank . sh ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / update_flank . sh ) shell script , you can rebuild `flank.jar` . 3) > Test run failed to complete. Expected 786 tests, received 660 Try setting `use-orchestrator: false`. Parameterized tests [are not compatible with orchestrator](https://stackoverflow.com/questions/48735268/unable-to-run-parameterized-tests-with-android-test-orchestrator). Flank uses [orchestrator by default on Android.](https://developer.android.com/training/testing/junit-runner) 4) > I have an issue when attempting to sync the Flank Gradle project Task 'prepareKotlinBuildScriptModel' not found in project ':test_runner'. or similar - Make sure you do not change any module specific settings for Gradle - Clear IDE cache using ` File > Invalidate Caches / Restart ` - Re - import project using root ` build . gradle . kts ` - Sync project again 5) > Does Flank support Cucumber? Please check document for more info 6) > How can I find project id? Please check the firebase documentation about finding the project id 7) > How do I run Flank with a proxy? java -Dhttp.proxyHost=localhost -Dhttp.proxyPort=8080 -Dhttp.proxyUser=user -Dhttp.proxyPassword=pass -jar ./test_runner/build/libs/flank.jar firebase test android run See google-auth-library-java for details.","title":"FAQ"},{"location":"#resources","text":"Instrumenting Firebase Test Lab","title":"Resources"},{"location":"analytics_comparsion/","text":"Analytics To optimize the user experience and make a proper decision about features supported by Flank there is a need to collect data about usage of Flank. Tools choosing It was suggested to use Firebase/Google analytics because Flank is ultimately a Firebase integration (for test lab). Due to the lack of API and no easy method to use for server/desktop/native apps, it was decided to research for alternatives. After some research 2 tools were find as interesting solution for Flank use case:\\ - MixPanel Analytics with features: - Java SDK , - dashboard - many tools to analyze the usage of Flank - free plan could be used by 100k users/month - Segment by Twilio with features: - dashboard - Java SDK - dashboard - free Plan Includes 1,000 visitors/mo Other evaluated tools do not have good API, free plan, and/or dashboard, so as a team we decided to make a proof of concept for MixPanel and Segment POC Mixpanel Scenario Send configuration provided by a user to MixPanel Sending events to Mixpanel is easy from developer perspective Add dependencu implementation ( \"com.mixpanel:mixpanel-java:1.5.0\" ) Register user val messageBuilder = MessageBuilder ( PROJECT_TOKEN ); val props = JSONObject () props . put ( \"name\" , args . project ) //set name as project id to easiest identification on users list val update = messageBuilder . set ( project , props ) // Send the update to mixpanel val mixpanel = MixpanelAPI () mixpanel . sendMessage ( update ) 1. Send event val messageBuilder = MessageBuilder ( PROJECT_TOKEN ) // You can send properties along with events val props = JSONObject () props . put ( \"type\" , \"gameloop\" ) props . put ( \"outputStyle\" , \"single\" ) val configurationEvent = messageBuilder . event ( distinctId , \"configuration\" , props ) val api = MixpanelAPI () api . sendMessage ( configurationEvent ) You could find working code here Results Configuration and sending events is easy. Mixpanel dashboard is advanced and probably meets our requirements. We could send all changed parameters by one event or send changed parameters one by one. Pros easy to use standalone easy to configure POC Segment Scenario Send configuration provided by a user to Segment. Description During making POC Segment appears to be not a standalone solution. It is just a tunnel to map and send data to other destination(s). A destination for this proof of concept was Google Analytics. Usage in Flank Sending events to Segment is super easy from developer perspective 1. Add dependency implementation ( \"com.segment.analytics.java:analytics:2.1.1\" ) 1. Create instance of Analytics private val analytics = Analytics . builder ( \"WRITE KEY\" ) // could be found on Segment source configuration . build () 1. Enqueue event with map properties analytics . enqueue ( TrackMessage . builder ( \"EVENT NAME\" ) // event name . userId ( \"USER ID\" ) // user id for indentification purpose . properties ( configurationProperties ) // properties Map<String, Any> ) 1. Flush events to make sure that they will be sent analytics . flush () Results The destination configuration was very complicated. It was not well described on Segment site. Segment needs Google Tracking ID for sending events to them, however to have it, you must use an old version of Google Analytics(Universal) which is not so easy to find, new Google Analytics does not use this property and Segment does not work with the latest version of Google Analytics. After configuring Segment and Google Analytics (Universal old version) events are received by the Google tool. However, data are not readable and could not be analyzed. This tool is mostly used for websites and does not allow querying properties or custom events. Maybe this is the fault of Segment integration or Flank specific use case, but this tool does not work properly with Google Analytics. Other destinations are not worth to be a consideration, because it is better to use standalone versions of them. Pros easy to use Cons it is not standalone (needs additional tools) configuration of destinations is not easy, there is not enough information for it implementation force user to use an older version of the end solution (Google Analytics) Decision MixPanel vs Segment Based on Flank needs it was decided that MixPanel will deliver the best experience to Team. Segment is not a standalone tool and its configuration is painful, so it is better to stick to MixPanel which provides easy out of the box integration and met all criteria created by Flank team. The work on analytics will be continue in nearest future. Flank team should consider which use cases are needed for analytics as entry point for further development of it in Flank.","title":"Analytics"},{"location":"analytics_comparsion/#analytics","text":"To optimize the user experience and make a proper decision about features supported by Flank there is a need to collect data about usage of Flank.","title":"Analytics"},{"location":"analytics_comparsion/#tools-choosing","text":"It was suggested to use Firebase/Google analytics because Flank is ultimately a Firebase integration (for test lab). Due to the lack of API and no easy method to use for server/desktop/native apps, it was decided to research for alternatives. After some research 2 tools were find as interesting solution for Flank use case:\\ - MixPanel Analytics with features: - Java SDK , - dashboard - many tools to analyze the usage of Flank - free plan could be used by 100k users/month - Segment by Twilio with features: - dashboard - Java SDK - dashboard - free Plan Includes 1,000 visitors/mo Other evaluated tools do not have good API, free plan, and/or dashboard, so as a team we decided to make a proof of concept for MixPanel and Segment","title":"Tools choosing"},{"location":"analytics_comparsion/#poc-mixpanel","text":"","title":"POC Mixpanel"},{"location":"analytics_comparsion/#scenario","text":"Send configuration provided by a user to MixPanel Sending events to Mixpanel is easy from developer perspective Add dependencu implementation ( \"com.mixpanel:mixpanel-java:1.5.0\" ) Register user val messageBuilder = MessageBuilder ( PROJECT_TOKEN ); val props = JSONObject () props . put ( \"name\" , args . project ) //set name as project id to easiest identification on users list val update = messageBuilder . set ( project , props ) // Send the update to mixpanel val mixpanel = MixpanelAPI () mixpanel . sendMessage ( update ) 1. Send event val messageBuilder = MessageBuilder ( PROJECT_TOKEN ) // You can send properties along with events val props = JSONObject () props . put ( \"type\" , \"gameloop\" ) props . put ( \"outputStyle\" , \"single\" ) val configurationEvent = messageBuilder . event ( distinctId , \"configuration\" , props ) val api = MixpanelAPI () api . sendMessage ( configurationEvent ) You could find working code here","title":"Scenario"},{"location":"analytics_comparsion/#results","text":"Configuration and sending events is easy. Mixpanel dashboard is advanced and probably meets our requirements. We could send all changed parameters by one event or send changed parameters one by one.","title":"Results"},{"location":"analytics_comparsion/#pros","text":"easy to use standalone easy to configure","title":"Pros"},{"location":"analytics_comparsion/#poc-segment","text":"","title":"POC Segment"},{"location":"analytics_comparsion/#scenario_1","text":"Send configuration provided by a user to Segment.","title":"Scenario"},{"location":"analytics_comparsion/#description","text":"During making POC Segment appears to be not a standalone solution. It is just a tunnel to map and send data to other destination(s). A destination for this proof of concept was Google Analytics.","title":"Description"},{"location":"analytics_comparsion/#usage-in-flank","text":"Sending events to Segment is super easy from developer perspective 1. Add dependency implementation ( \"com.segment.analytics.java:analytics:2.1.1\" ) 1. Create instance of Analytics private val analytics = Analytics . builder ( \"WRITE KEY\" ) // could be found on Segment source configuration . build () 1. Enqueue event with map properties analytics . enqueue ( TrackMessage . builder ( \"EVENT NAME\" ) // event name . userId ( \"USER ID\" ) // user id for indentification purpose . properties ( configurationProperties ) // properties Map<String, Any> ) 1. Flush events to make sure that they will be sent analytics . flush ()","title":"Usage in Flank"},{"location":"analytics_comparsion/#results_1","text":"The destination configuration was very complicated. It was not well described on Segment site. Segment needs Google Tracking ID for sending events to them, however to have it, you must use an old version of Google Analytics(Universal) which is not so easy to find, new Google Analytics does not use this property and Segment does not work with the latest version of Google Analytics. After configuring Segment and Google Analytics (Universal old version) events are received by the Google tool. However, data are not readable and could not be analyzed. This tool is mostly used for websites and does not allow querying properties or custom events. Maybe this is the fault of Segment integration or Flank specific use case, but this tool does not work properly with Google Analytics. Other destinations are not worth to be a consideration, because it is better to use standalone versions of them.","title":"Results"},{"location":"analytics_comparsion/#pros_1","text":"easy to use","title":"Pros"},{"location":"analytics_comparsion/#cons","text":"it is not standalone (needs additional tools) configuration of destinations is not easy, there is not enough information for it implementation force user to use an older version of the end solution (Google Analytics)","title":"Cons"},{"location":"analytics_comparsion/#decision-mixpanel-vs-segment","text":"Based on Flank needs it was decided that MixPanel will deliver the best experience to Team. Segment is not a standalone tool and its configuration is painful, so it is better to stick to MixPanel which provides easy out of the box integration and met all criteria created by Flank team. The work on analytics will be continue in nearest future. Flank team should consider which use cases are needed for analytics as entry point for further development of it in Flank.","title":"Decision MixPanel vs Segment"},{"location":"architecture/","text":"Architecture This document describes abstract architecture design which should be able to apply to any use-case in flank scope, starting from user interface and ending on remote API calls. Use it as a reference for explaining common abstract problems around implementation. Table of contents Motivation Goals Scalability Layers Presentation Responsibilities Constrains How to scale Dependencies Domain Execution context Top-level function Low-level function Responsibilities Constrains How to scale Dependencies Static Dynamic Both Tool How to scale Dependencies API How to scale Dependencies Client How to scale Dependencies Adapter How to scale Dependencies Implementation Public API Components composition Code composition Vertical Horizontal Horizontal Layered Motivation Without well-defined architecture, the application can grow in an uncontrolled way. This typically increases the amount of unwanted redundancy, unneeded calls, and unnecessary logical operations which as result makes code harder to understand, more error-prone, and sometimes even impossible to scale. Goals The architecture should help achieve the following goals: Organize implementation into restricted logical layers. Divide implementation into small and easy-to-understand parts. Identify scalability vectors for each part of the architecture. Make implementation easy to navigate through. Optimize the amount of code for implementation. Make implementation less error-prone. Scalability The design is specifying two types of scaling: Horizontal - by adding atomic components that are not related to each other, but must meet common requirements. Vertical - by expanding one component for new features. Typically, horizontal scaling is preferred when vertical scaling become to break the single responsibility principle. Layers The example diagram that is exposing relations between layers: Presentation The front-end layer of the flank application. From the higher level of view, the presentation layer is a bridge between end-user and business logic. From the implementation perspective it is just adapter for domain top-level function call. Responsibilities Implements user interface and adapt it to domain API. Converts input from user into domain top-level functions calls. Converts structural output result from the domain into the output specific for the presentation . Passes dynamic dependencies to domain if needed. Constrains SHOULD avoid logical operations CAN base on third-part framework or library invoke domain public functions CAN'T access API layer directly access tools deserved for domain How to scale Horizontal - by adding different UI implementations Along with domain top-level functions . Dependencies Domain layer Adapter layer (required meet domain interface) Domain Is the implementation of the application business logic. Exposes its own API as a one, or many public extension functions, called top-level functions . Each top-level function have its own execution context , can produce a structured output during the execution and can be composed of one or more low-level functions . This layer can be considered as a standalone library, that is providing access to business logic through pure kotlin functions. Execution context The context can provide arguments and dynamic functions required by the execution. Its name should reflect the related use case. Top-level function Is a public function placed in root of the domain package. Is responsible to implement domain logic directly or compose it using low-level functions , tools or API . For simplification, consider a simple common type for all top-level functions . typealias UseCase < A > = A .() -> Unit Where A is a generic type that is representing the execution context . Low-level function The low-level function is useful when it comes to dividing complex top-level function into the composition of smaller chunks or the reuse of some logic in many top-level functions. It is crucial in keeping the composition of low-level functions flat. More nesting in-depth can make a code much harder to understand and maintain. Responsibilities Contains the business logic of the application Provide access to the use cases through formalized API Constrains MUST define a dedicated execution context for each top-level function . keep the top-level functions directly inside root package. keep the low-level functions inside nested packages of the root. SHOULD access third-party clients through API layer abstraction. keep the execution context with related top-level function in the same file. CAN use tools directly. use API directly. CAN'T specify anything else than top-level functions & related contexts directly inside root package How to scale Horizontal - by adding new top-level functions . Vertically - by adding new low-level functions for a single top-level function . Dependencies The domain layer shouldn't implement complicated or specialized operations by itself or use third-party libraries directly. Instead of this, it can depend on dedicated internal tools and APIs, that are designed to exactly meet domain requirements. There are 2 types of domain dependencies: Static The dependencies that are providing its API through static imports. It's dedicated to the tools that don't need to be mocked for unit testing. In details the static dependency: CAN: Implement algorithms. Generate files. Parse files (only if can generate it also) Format data. Parse data CANNOT: Make network calls. Use external applications (shell, SQL server, etc..). Operate on binary files that need to be provided. Dynamic The dependencies that are provided to the domain through the execution context , as a reference. It's dedicated to the tools that need to be mocked for unit testing. CAN: Make network calls. Use external applications (shell, SQL server, etc..). Operate on binary files that need to be provided. Generate data structures. CANNOT: Generate files. Implement algorithms. SHOULD NOT: Parse formatted data (except simple strings like date). Format data (except simple string formatting). Both Additionally, both static and dynamic : CAN: Specify data structures. Map data structures. Tool The layer that groups various atomic tools required by the domain. Mainly static dependencies or not client specific dynamic dependencies. Typically, tools are specialized to solve one, or a group of related problems like: parsing and formatting calculating mapping Notice that, tools are solving specialized problems that are meeting domain requirements, but should be designed as standalone libraries that do know nothing about the whole domain problem. Instead, just solving well the small highly isolated part. Designing tools as standalone libraries makes the code more decoupled and easier to reuse if needed. How to scale Horizontal - as a group of libs just by adding more standalone tools if needed. Dependencies third-party library API The light-weight layer that is specifying structures and functional interfaces for client operations. Like tool this layer must exactly meet the domain requirements and specify public API designed for it. Unlike the tool , it cannot define any implementation, so it can be scaled horizontally by adding new not unrelated scopes. How to scale Horizontal - by adding more namespaces for structures and functional interfaces. Dependencies Only standard libraries Client The client-side specific operations not related directly to the domain. Typically, there are two purposes for this layer implementation: Is necessary to create a library wrapper for remote protocol, driven on WS, REST, TCP, etc... The third-party library is not convenient and requires some adjustments. How to scale Along with third-party API changes. Dependencies Network libraries third-party client library Adapter This layer is adapting client or third-party libraries to structures and interfaces, specified in the API layer. How to scale Along with API changes. Dependencies one of or many: internal client library third-party client library Implementation For convenience and clarity, the code should be written in a functional programming style. It's mandatory to avoid the OOP style which almost always makes things much more complicated than should be. Public API Any application or library must always have a public API and an internal/private part. For convenience keep public functions and structures in the root package, so the API will be easy to find. Additionally, if the component : is providing accessibility to public API's with additional structures. -It is mandatory to keep the public structures and functions distinct from internal implementation which should be kept in nested package(s). is just a simple tool with a compact implementation that is not specifying many structures. - Private implementations can be kept in the same file, just behind the public API or even the whole tool can be delivered as one public function if the implementation is simple enough. DO NOT keep multiple public functions along with internal implementations in the same file or package, because it is messes up the public API, which makes code harder to analyze and navigate. Components composition Business logic shouldn't implement complicated tools on its own because it is can mess up crucial high-level implementations making it harder to understand. Instead, it should be decomposed into high-level use-case implementations that operate on tools provided by specialized components. Code composition Typically, when huge features are divided into smaller functions and one of those functions is a (public) root, the functions can be composed in two different ways. Vertical The preceding function is calling the following, so the composition of functions is similar to a linked list. Try to AVOID this pattern where possible, especially in business logic. In some situations it can be even worse than one huge monolithic function with comments, for example when internal functions are not ordered correctly. Understanding the feature composed in vertical style, almost always require analyzing the whole chain of functions which typically is not efficient. Horizontal Root function is controlling independent internal and specialized functions. This approach gives a fast overview of high-level implementation but is hiding the details not important from the high-level perspective. Comparing to vertical composition where the cost of manual access to internal functions (jumping on references in IDE) in the worst-case scenario is n , the horizontal composition almost always gives 1 on the same layer (or 2 taking private functions into account if exist). Horizontal-Layered An example of horizontal composition in layered architecture can look as following:","title":"Architecture abstraction"},{"location":"architecture/#architecture","text":"This document describes abstract architecture design which should be able to apply to any use-case in flank scope, starting from user interface and ending on remote API calls. Use it as a reference for explaining common abstract problems around implementation.","title":"Architecture"},{"location":"architecture/#table-of-contents","text":"Motivation Goals Scalability Layers Presentation Responsibilities Constrains How to scale Dependencies Domain Execution context Top-level function Low-level function Responsibilities Constrains How to scale Dependencies Static Dynamic Both Tool How to scale Dependencies API How to scale Dependencies Client How to scale Dependencies Adapter How to scale Dependencies Implementation Public API Components composition Code composition Vertical Horizontal Horizontal Layered","title":"Table of contents"},{"location":"architecture/#motivation","text":"Without well-defined architecture, the application can grow in an uncontrolled way. This typically increases the amount of unwanted redundancy, unneeded calls, and unnecessary logical operations which as result makes code harder to understand, more error-prone, and sometimes even impossible to scale.","title":"Motivation "},{"location":"architecture/#goals","text":"The architecture should help achieve the following goals: Organize implementation into restricted logical layers. Divide implementation into small and easy-to-understand parts. Identify scalability vectors for each part of the architecture. Make implementation easy to navigate through. Optimize the amount of code for implementation. Make implementation less error-prone.","title":"Goals "},{"location":"architecture/#scalability","text":"The design is specifying two types of scaling: Horizontal - by adding atomic components that are not related to each other, but must meet common requirements. Vertical - by expanding one component for new features. Typically, horizontal scaling is preferred when vertical scaling become to break the single responsibility principle.","title":"Scalability "},{"location":"architecture/#layers","text":"The example diagram that is exposing relations between layers:","title":"Layers "},{"location":"architecture/#presentation","text":"The front-end layer of the flank application. From the higher level of view, the presentation layer is a bridge between end-user and business logic. From the implementation perspective it is just adapter for domain top-level function call.","title":"Presentation "},{"location":"architecture/#responsibilities","text":"Implements user interface and adapt it to domain API. Converts input from user into domain top-level functions calls. Converts structural output result from the domain into the output specific for the presentation . Passes dynamic dependencies to domain if needed.","title":"Responsibilities "},{"location":"architecture/#constrains","text":"SHOULD avoid logical operations CAN base on third-part framework or library invoke domain public functions CAN'T access API layer directly access tools deserved for domain","title":"Constrains "},{"location":"architecture/#how-to-scale","text":"Horizontal - by adding different UI implementations Along with domain top-level functions .","title":"How to scale "},{"location":"architecture/#dependencies","text":"Domain layer Adapter layer (required meet domain interface)","title":"Dependencies "},{"location":"architecture/#domain","text":"Is the implementation of the application business logic. Exposes its own API as a one, or many public extension functions, called top-level functions . Each top-level function have its own execution context , can produce a structured output during the execution and can be composed of one or more low-level functions . This layer can be considered as a standalone library, that is providing access to business logic through pure kotlin functions.","title":"Domain "},{"location":"architecture/#execution-context","text":"The context can provide arguments and dynamic functions required by the execution. Its name should reflect the related use case.","title":"Execution context "},{"location":"architecture/#top-level-function","text":"Is a public function placed in root of the domain package. Is responsible to implement domain logic directly or compose it using low-level functions , tools or API . For simplification, consider a simple common type for all top-level functions . typealias UseCase < A > = A .() -> Unit Where A is a generic type that is representing the execution context .","title":"Top-level function "},{"location":"architecture/#low-level-function","text":"The low-level function is useful when it comes to dividing complex top-level function into the composition of smaller chunks or the reuse of some logic in many top-level functions. It is crucial in keeping the composition of low-level functions flat. More nesting in-depth can make a code much harder to understand and maintain.","title":"Low-level function "},{"location":"architecture/#responsibilities_1","text":"Contains the business logic of the application Provide access to the use cases through formalized API","title":"Responsibilities "},{"location":"architecture/#constrains_1","text":"MUST define a dedicated execution context for each top-level function . keep the top-level functions directly inside root package. keep the low-level functions inside nested packages of the root. SHOULD access third-party clients through API layer abstraction. keep the execution context with related top-level function in the same file. CAN use tools directly. use API directly. CAN'T specify anything else than top-level functions & related contexts directly inside root package","title":"Constrains "},{"location":"architecture/#how-to-scale_1","text":"Horizontal - by adding new top-level functions . Vertically - by adding new low-level functions for a single top-level function .","title":"How to scale "},{"location":"architecture/#dependencies_1","text":"The domain layer shouldn't implement complicated or specialized operations by itself or use third-party libraries directly. Instead of this, it can depend on dedicated internal tools and APIs, that are designed to exactly meet domain requirements. There are 2 types of domain dependencies:","title":"Dependencies "},{"location":"architecture/#static","text":"The dependencies that are providing its API through static imports. It's dedicated to the tools that don't need to be mocked for unit testing. In details the static dependency: CAN: Implement algorithms. Generate files. Parse files (only if can generate it also) Format data. Parse data CANNOT: Make network calls. Use external applications (shell, SQL server, etc..). Operate on binary files that need to be provided.","title":"Static "},{"location":"architecture/#dynamic","text":"The dependencies that are provided to the domain through the execution context , as a reference. It's dedicated to the tools that need to be mocked for unit testing. CAN: Make network calls. Use external applications (shell, SQL server, etc..). Operate on binary files that need to be provided. Generate data structures. CANNOT: Generate files. Implement algorithms. SHOULD NOT: Parse formatted data (except simple strings like date). Format data (except simple string formatting).","title":"Dynamic "},{"location":"architecture/#both","text":"Additionally, both static and dynamic : CAN: Specify data structures. Map data structures.","title":"Both "},{"location":"architecture/#tool","text":"The layer that groups various atomic tools required by the domain. Mainly static dependencies or not client specific dynamic dependencies. Typically, tools are specialized to solve one, or a group of related problems like: parsing and formatting calculating mapping Notice that, tools are solving specialized problems that are meeting domain requirements, but should be designed as standalone libraries that do know nothing about the whole domain problem. Instead, just solving well the small highly isolated part. Designing tools as standalone libraries makes the code more decoupled and easier to reuse if needed.","title":"Tool "},{"location":"architecture/#how-to-scale_2","text":"Horizontal - as a group of libs just by adding more standalone tools if needed.","title":"How to scale "},{"location":"architecture/#dependencies_2","text":"third-party library","title":"Dependencies "},{"location":"architecture/#api","text":"The light-weight layer that is specifying structures and functional interfaces for client operations. Like tool this layer must exactly meet the domain requirements and specify public API designed for it. Unlike the tool , it cannot define any implementation, so it can be scaled horizontally by adding new not unrelated scopes.","title":"API "},{"location":"architecture/#how-to-scale_3","text":"Horizontal - by adding more namespaces for structures and functional interfaces.","title":"How to scale "},{"location":"architecture/#dependencies_3","text":"Only standard libraries","title":"Dependencies "},{"location":"architecture/#client","text":"The client-side specific operations not related directly to the domain. Typically, there are two purposes for this layer implementation: Is necessary to create a library wrapper for remote protocol, driven on WS, REST, TCP, etc... The third-party library is not convenient and requires some adjustments.","title":"Client "},{"location":"architecture/#how-to-scale_4","text":"Along with third-party API changes.","title":"How to scale "},{"location":"architecture/#dependencies_4","text":"Network libraries third-party client library","title":"Dependencies "},{"location":"architecture/#adapter","text":"This layer is adapting client or third-party libraries to structures and interfaces, specified in the API layer.","title":"Adapter "},{"location":"architecture/#how-to-scale_5","text":"Along with API changes.","title":"How to scale "},{"location":"architecture/#dependencies_5","text":"one of or many: internal client library third-party client library","title":"Dependencies "},{"location":"architecture/#implementation","text":"For convenience and clarity, the code should be written in a functional programming style. It's mandatory to avoid the OOP style which almost always makes things much more complicated than should be.","title":"Implementation "},{"location":"architecture/#public-api","text":"Any application or library must always have a public API and an internal/private part. For convenience keep public functions and structures in the root package, so the API will be easy to find. Additionally, if the component : is providing accessibility to public API's with additional structures. -It is mandatory to keep the public structures and functions distinct from internal implementation which should be kept in nested package(s). is just a simple tool with a compact implementation that is not specifying many structures. - Private implementations can be kept in the same file, just behind the public API or even the whole tool can be delivered as one public function if the implementation is simple enough. DO NOT keep multiple public functions along with internal implementations in the same file or package, because it is messes up the public API, which makes code harder to analyze and navigate.","title":"Public API "},{"location":"architecture/#components-composition","text":"Business logic shouldn't implement complicated tools on its own because it is can mess up crucial high-level implementations making it harder to understand. Instead, it should be decomposed into high-level use-case implementations that operate on tools provided by specialized components.","title":"Components composition "},{"location":"architecture/#code-composition","text":"Typically, when huge features are divided into smaller functions and one of those functions is a (public) root, the functions can be composed in two different ways.","title":"Code composition "},{"location":"architecture/#vertical","text":"The preceding function is calling the following, so the composition of functions is similar to a linked list. Try to AVOID this pattern where possible, especially in business logic. In some situations it can be even worse than one huge monolithic function with comments, for example when internal functions are not ordered correctly. Understanding the feature composed in vertical style, almost always require analyzing the whole chain of functions which typically is not efficient.","title":"Vertical "},{"location":"architecture/#horizontal","text":"Root function is controlling independent internal and specialized functions. This approach gives a fast overview of high-level implementation but is hiding the details not important from the high-level perspective. Comparing to vertical composition where the cost of manual access to internal functions (jumping on references in IDE) in the worst-case scenario is n , the horizontal composition almost always gives 1 on the same layer (or 2 taking private functions into account if exist).","title":"Horizontal "},{"location":"architecture/#horizontal-layered","text":"An example of horizontal composition in layered architecture can look as following:","title":"Horizontal-Layered "},{"location":"binaries_used_in_flank_ios_testing/","text":"Binaries used in Flank's iOS testing Location Binaries are placed in Flank binaries repository Usage The binaries are downloaded at runtime when they needed for Linux and Windows from Flank binaries repository . They are unpacked to <user directory>/.flank . If they already exist on this path, they are not downloaded again. Updating In order to update binaries just follow below steps: 1. checkout binaries repository 1. update them using: - updateBinariesWithFlankBash will update binaries for Linux and Windows using flank-scripts - update.sh (old method). It will update binaries for Linux OS 1. commit and push files (create PR with changes) 1. once they will be on master branch. CI job will update artifacts with proper files based on OS","title":"Binaries used in Flank's iOS testing"},{"location":"binaries_used_in_flank_ios_testing/#binaries-used-in-flanks-ios-testing","text":"","title":"Binaries used in Flank's iOS testing"},{"location":"binaries_used_in_flank_ios_testing/#location","text":"Binaries are placed in Flank binaries repository","title":"Location"},{"location":"binaries_used_in_flank_ios_testing/#usage","text":"The binaries are downloaded at runtime when they needed for Linux and Windows from Flank binaries repository . They are unpacked to <user directory>/.flank . If they already exist on this path, they are not downloaded again.","title":"Usage"},{"location":"binaries_used_in_flank_ios_testing/#updating","text":"In order to update binaries just follow below steps: 1. checkout binaries repository 1. update them using: - updateBinariesWithFlankBash will update binaries for Linux and Windows using flank-scripts - update.sh (old method). It will update binaries for Linux OS 1. commit and push files (create PR with changes) 1. once they will be on master branch. CI job will update artifacts with proper files based on OS","title":"Updating"},{"location":"building_updating_flank/","text":"Building and Updating Flank Ensure that all steps taken for contributing and building Flank have been followed, which are found here Building an updated flank To build an updated version of flank from source simply run (This assumes you are in the root Flank directory) ./gradlew flankFullRun The flank full run task, builds a clean Flan, runs all tests and runs the updateFlank gradle task. This will create the Flank.jar file and place it in /test_runner/bash Building a minimized and optimized version of Flank (Proguard) To build a proguarded version of Flank ./gradlew applyProguard This will generate a second Flank.jar , named Flank-proguard.jar found at /test_runner/bash To make use of this jar copy and rename it to Flank.jar","title":"Building & updating Flank"},{"location":"building_updating_flank/#building-and-updating-flank","text":"Ensure that all steps taken for contributing and building Flank have been followed, which are found here","title":"Building and Updating Flank"},{"location":"building_updating_flank/#building-an-updated-flank","text":"To build an updated version of flank from source simply run (This assumes you are in the root Flank directory) ./gradlew flankFullRun The flank full run task, builds a clean Flan, runs all tests and runs the updateFlank gradle task. This will create the Flank.jar file and place it in /test_runner/bash","title":"Building an updated flank"},{"location":"building_updating_flank/#building-a-minimized-and-optimized-version-of-flank-proguard","text":"To build a proguarded version of Flank ./gradlew applyProguard This will generate a second Flank.jar , named Flank-proguard.jar found at /test_runner/bash To make use of this jar copy and rename it to Flank.jar","title":"Building a minimized and optimized version of Flank (Proguard)"},{"location":"client_generation/","text":"Overview The google-api-*-client projects are officially marked as maintenance mode . They don't have a consistent code generation approach. Each binding appears to do something different. apis-client-generator is used to generate REST bindings. The new clients are google-cloud-* . They are in the process of adopting the latest code generation tech googleapis/toolkit which is gRPC based. These handwritten packages are going to migrate to live on top of clients generated from the Google API Code Generator https://github.com/GoogleCloudPlatform/google-cloud-go/issues/266#issuecomment-221083266 API client summary Google API clients ( google-api-*-client ) - Low level - Auto generated - Made for JSON REST APIs - Older project - maintence mode Google APIs toolkit ( googleapis/toolkit ) - Low level - Auto generated - Made for gRPC APIs - New project - actively developed Google Cloud libraries ( google-cloud-* ) - High level - Hand written - Built on top of the low level clients google-api-client - maintenance mode google-api-nodejs-client google-api-php-client google-api-python-client google-api-ruby-client google-api-go-client google-api-go-generator google-api-java-client google-api-javascript-client - not offically marked as maintence, however no new updates since May 2017. google-api-dotnet-client google-api-objectivec-client Deprecated. Replaced entirely by google-api-objectivec-client-for-rest google-api-cpp-client apis-client-generator - active apis-client-generator Java, C++, C#, GWT, PHP, Dart Used to generate google-api-java-client Recommended way to generate bindings for REST APIs apitools - maintenance mode apitools Generates Python only. Used by gcloud CLI Writes protobuf files from API discovery google-cloud - active Actively developed. Google Cloud APIs only. google-cloud-ruby google-cloud-node google-cloud-python google-cloud-go google-cloud-java google-cloud-php google-cloud-dotnet googleapis/toolkit - active The latest code generation tech from Google. googleapis/toolkit googleapis/api-compiler Generating Java APIs with apis-client-generator Install Google API client generator from source. git clone https://github.com/google/apis-client-generator.git pip install . Generate the library manually: generate_library \\ --input=./testing_v1.json \\ --language=java \\ --output_dir=./testing Alternatively, generate the library by running generate.sh","title":"Client generation"},{"location":"client_generation/#overview","text":"The google-api-*-client projects are officially marked as maintenance mode . They don't have a consistent code generation approach. Each binding appears to do something different. apis-client-generator is used to generate REST bindings. The new clients are google-cloud-* . They are in the process of adopting the latest code generation tech googleapis/toolkit which is gRPC based. These handwritten packages are going to migrate to live on top of clients generated from the Google API Code Generator https://github.com/GoogleCloudPlatform/google-cloud-go/issues/266#issuecomment-221083266","title":"Overview"},{"location":"client_generation/#api-client-summary","text":"Google API clients ( google-api-*-client ) - Low level - Auto generated - Made for JSON REST APIs - Older project - maintence mode Google APIs toolkit ( googleapis/toolkit ) - Low level - Auto generated - Made for gRPC APIs - New project - actively developed Google Cloud libraries ( google-cloud-* ) - High level - Hand written - Built on top of the low level clients","title":"API client summary"},{"location":"client_generation/#google-api-client-maintenance-mode","text":"google-api-nodejs-client google-api-php-client google-api-python-client google-api-ruby-client google-api-go-client google-api-go-generator google-api-java-client google-api-javascript-client - not offically marked as maintence, however no new updates since May 2017. google-api-dotnet-client google-api-objectivec-client Deprecated. Replaced entirely by google-api-objectivec-client-for-rest google-api-cpp-client","title":"google-api-client - maintenance mode"},{"location":"client_generation/#apis-client-generator-active","text":"apis-client-generator Java, C++, C#, GWT, PHP, Dart Used to generate google-api-java-client Recommended way to generate bindings for REST APIs","title":"apis-client-generator - active"},{"location":"client_generation/#apitools-maintenance-mode","text":"apitools Generates Python only. Used by gcloud CLI Writes protobuf files from API discovery","title":"apitools - maintenance mode"},{"location":"client_generation/#google-cloud-active","text":"Actively developed. Google Cloud APIs only. google-cloud-ruby google-cloud-node google-cloud-python google-cloud-go google-cloud-java google-cloud-php google-cloud-dotnet","title":"google-cloud - active"},{"location":"client_generation/#googleapistoolkit-active","text":"The latest code generation tech from Google. googleapis/toolkit googleapis/api-compiler","title":"googleapis/toolkit - active"},{"location":"client_generation/#generating-java-apis-with-apis-client-generator","text":"Install Google API client generator from source. git clone https://github.com/google/apis-client-generator.git pip install . Generate the library manually: generate_library \\ --input=./testing_v1.json \\ --language=java \\ --output_dir=./testing Alternatively, generate the library by running generate.sh","title":"Generating Java APIs with apis-client-generator"},{"location":"cucumber_support/","text":"Cucumber support Firebase test lab and Flank do not support Cucumber. However, you could run these tests. - To make them work properly please disable sharding using .yml options: flank : disable-sharding : true or by using command-line option shell script --disable-sharding If you would like to use orchestrator please make sure that you are using at least version 1.3.0 of it. Currently , Flank will run Cucumber tests only if there are other Instrumented tests to run in your test apk. In other cases Flank will fast fail with There are no tests to run message.","title":"Cucumber"},{"location":"cucumber_support/#cucumber-support","text":"Firebase test lab and Flank do not support Cucumber. However, you could run these tests. - To make them work properly please disable sharding using .yml options: flank : disable-sharding : true or by using command-line option shell script --disable-sharding If you would like to use orchestrator please make sure that you are using at least version 1.3.0 of it. Currently , Flank will run Cucumber tests only if there are other Instrumented tests to run in your test apk. In other cases Flank will fast fail with There are no tests to run message.","title":"Cucumber support"},{"location":"dependencies_update_process/","text":"Dependencies update process Description Process run commands and update files with defined versions in the provided file and create PR with changes. Modules Gradle Versions Plugin which check dependencies version and generate report Command in flank-scripts which update dependencies versions Github action job which runs dependencies check every Monday at 5 AM UTC or on-demand Usage Manually (root directory) Generate report using command ./gradlew dependencyUpdates -DoutputFormatter=json -DoutputDir=. Build flank scripts using script ./flank-scripts/bash/buildFlankScripts.sh Run ./flank-scripts/bash/flankScripts dependencies update Github action Run Update dependencies job using Github action menu by clicking Run workflow button Merging to master Success path If all PR jobs will succeed it means that dependencies update will not break the current code base and pull requests could be successfully merged. Failure path If any of PR job will fail, it means that dependencies update will break our codebase and code should be aligned before merging","title":"Dependencies update"},{"location":"dependencies_update_process/#dependencies-update-process","text":"","title":"Dependencies update process"},{"location":"dependencies_update_process/#description","text":"Process run commands and update files with defined versions in the provided file and create PR with changes.","title":"Description"},{"location":"dependencies_update_process/#modules","text":"Gradle Versions Plugin which check dependencies version and generate report Command in flank-scripts which update dependencies versions Github action job which runs dependencies check every Monday at 5 AM UTC or on-demand","title":"Modules"},{"location":"dependencies_update_process/#usage","text":"","title":"Usage"},{"location":"dependencies_update_process/#manually-root-directory","text":"Generate report using command ./gradlew dependencyUpdates -DoutputFormatter=json -DoutputDir=. Build flank scripts using script ./flank-scripts/bash/buildFlankScripts.sh Run ./flank-scripts/bash/flankScripts dependencies update","title":"Manually (root directory)"},{"location":"dependencies_update_process/#github-action","text":"Run Update dependencies job using Github action menu by clicking Run workflow button","title":"Github action"},{"location":"dependencies_update_process/#merging-to-master","text":"","title":"Merging to master"},{"location":"dependencies_update_process/#success-path","text":"If all PR jobs will succeed it means that dependencies update will not break the current code base and pull requests could be successfully merged.","title":"Success path"},{"location":"dependencies_update_process/#failure-path","text":"If any of PR job will fail, it means that dependencies update will break our codebase and code should be aligned before merging","title":"Failure path"},{"location":"error_monitoring/","text":"Flank Error Monitoring Flank uses Sentry to monitor test runner stability. Sentry enables data driven decisions when prioritizing bug fixes. https://sentry.io/ https://docs.sentry.io/ Data Captured Sentry captures the following error data: Flank - Stacktrace - releaseStage of Flank (production or snapshot) - version of Flank (git commit Flank was built from) Device - hostname - locale - osArch - osName - osVersion - runtimeVersions of Java Disable Sentry Flank respects the same analytics opt out as gcloud CLI. echo \"DISABLED\" > ~/.gsutil/analytics-uuid More information To see how Sentry is integrated within the Flank project please see the Flank Sentry testcase and the actual Sentry implementation Sentry Data Security, Privacy, and Compliance Overview","title":"Flank Error Monitoring"},{"location":"error_monitoring/#flank-error-monitoring","text":"Flank uses Sentry to monitor test runner stability. Sentry enables data driven decisions when prioritizing bug fixes. https://sentry.io/ https://docs.sentry.io/","title":"Flank Error Monitoring"},{"location":"error_monitoring/#data-captured","text":"Sentry captures the following error data: Flank - Stacktrace - releaseStage of Flank (production or snapshot) - version of Flank (git commit Flank was built from) Device - hostname - locale - osArch - osName - osVersion - runtimeVersions of Java","title":"Data Captured"},{"location":"error_monitoring/#disable-sentry","text":"Flank respects the same analytics opt out as gcloud CLI. echo \"DISABLED\" > ~/.gsutil/analytics-uuid","title":"Disable Sentry"},{"location":"error_monitoring/#more-information","text":"To see how Sentry is integrated within the Flank project please see the Flank Sentry testcase and the actual Sentry implementation Sentry Data Security, Privacy, and Compliance Overview","title":"More information"},{"location":"exit_codes_and_exceptions/","text":"Flank exit codes and exceptions Exit code | Returned by exception | Description -- | -- | -- | 0 | | All tests passed 1 | FlankGeneralError, FlankTimeoutError | A general failure occurred. Possible causes include: a filename that does not exist or an HTTP/network error. 2 | FlankConfigurationError, YmlValidationError | Usually indicates missing or wrong usage of flags, incorrect parameters, errors in config files. 3 | FlankNoTestsError | No tests detected in the test apk, or they have been filtered out. 10 | FailedMatrixError | At least one matrix not finished (usually a FTL internal error) or unexpected error occurred. 15 | FTLError | Firebase Test Lab could not determine if the test matrix passed or failed, because of an unexpected error. 18 | IncompatibleTestDimensionError | The test environment for this test execution is not supported because of incompatible test dimensions. This error might occur if the selected Android API level is not supported by the selected device type. 19 | MatrixCanceledError | The test matrix was canceled by the user. 20 | InfrastructureError | A test infrastructure error occurred.","title":"Flank exit codes and exceptions"},{"location":"exit_codes_and_exceptions/#flank-exit-codes-and-exceptions","text":"Exit code | Returned by exception | Description -- | -- | -- | 0 | | All tests passed 1 | FlankGeneralError, FlankTimeoutError | A general failure occurred. Possible causes include: a filename that does not exist or an HTTP/network error. 2 | FlankConfigurationError, YmlValidationError | Usually indicates missing or wrong usage of flags, incorrect parameters, errors in config files. 3 | FlankNoTestsError | No tests detected in the test apk, or they have been filtered out. 10 | FailedMatrixError | At least one matrix not finished (usually a FTL internal error) or unexpected error occurred. 15 | FTLError | Firebase Test Lab could not determine if the test matrix passed or failed, because of an unexpected error. 18 | IncompatibleTestDimensionError | The test environment for this test execution is not supported because of incompatible test dimensions. This error might occur if the selected Android API level is not supported by the selected device type. 19 | MatrixCanceledError | The test matrix was canceled by the user. 20 | InfrastructureError | A test infrastructure error occurred.","title":"Flank exit codes and exceptions"},{"location":"flank_corellium/","text":"Flank - Corellium Run mobile tests in parallel on virtual devices driven by Corellium backend. Status The Flank - Corellium integration is at the MVP stage, so only the core and most important features are available. Read more about Minimum Viable Product. Supported API levels The following Android API levels are supported: 25, 27, 28, 29, and 30. Additional API levels may be added based on customer feedback. Why Corellium Flank is just a client-side application that can prepare a time-efficient parallel test plan to run on several devices. It requires a third-party provider that can serve a huge amount of devices to run tests on them. Corellium is solving this problem as follows: Provides technology to virtualize mobile operating systems on servers powered by ARM cores. - This gives an incredible ability for scaling with regards to the amount of devices. Gives access to the bare operating system (Android or iOS). - Which allows a run optimized sharding algorithm that improves test execution time and reduces costs. How to get The Latest build Flank - Corellium integration is built in the flank.jar executable, so the latest Flank build gives you access to the features driven on the Corellium backend. Manual compilation Clone the repository and go to dir: git clone git@github.com:Flank/flank.git cd flank Build flank using flank-scripts (this method will give you access to flank.jar through flank shell command): . .env flankScripts assemble flank Or build directly using gradle command: ./gradlew :test_runner:build :test_runner:shadowJar How to run To call the root command for Corellium related features: locate your flank.jar in terminal and run: flank.jar corellium or if flank was build using . .env && flankScripts assemble flank , just type: flank corellium Authorization To allow Flank working with Corellium backend is necessary to provide authentication data. By default, Flank is recognizing corellium_auth.yml as an authentication file, which should look as follows: host : your.corellium.backend.host username : your_username password : your_password Android test execution To execute android instrumented tests, run following command: $ flank.jar corellium test android run -c = \"./flank_corellium.yml\" Config The example configuration file looks following: ### Test apks ## The list of app and test apks. ## Each app apk must have one or more corresponding test apks. apks : - path : \"app1.apk\" tests : - path : \"app1-test1.apk\" - path : \"app2.apk\" tests : - path : \"app2-test1.apk\" - path : \"app2-test2.apk\" ### Test Targets ## A list of one or more test target filters to apply (default: run all test targets). ## Each target filter must be fully qualified with the package name, class name, or test annotation desired. ## Supported test filters by am instrument -e \u2026 include: ## class, notClass, size, annotation, notAnnotation, package, notPackage, testFile, notTestFile ## See https://developer.android.com/reference/android/support/test/runner/AndroidJUnitRunner for more information. # test-targets: # - class com.example.app.ExampleUiTest#testPasses # - package com.example.app.foo # - notPackage com.example.app.bar ### Authorization file ## Path to YAML file with host address and credentials. ## default: corellium_auth.yml # auth: auth_file.yml ### Project name ## The name of Corellium project. ## default: \"Default project\" # project: \"project name\" ### Max Test Shards ## The amount of groups to split the test suite into. ## default: 1 # max-test-shards: 10000 ### Results Directory ## The name of a local directory where test results will be stored. ## default: results/corellium/android/yyyy-MM-dd_HH-mm-ss-SSS # local-result-dir: test-results ### Obfuscate dump ## Replace internal test names with unique identifiers, before dumping them to \"android-shards.json\". ## This option is hiding sensitive details about tests. ## default: false # obfuscate: true ### Configure JUnit reports ## A map of name suffixes related to set of result types required to include in custom junit report. ## Available result types to include are: [Skipped, Passed, Failed, Flaky] (values are not case-sensitive). ## As results, this option will generate additional amount of junit reports named `JUnitReport-$suffix.xml`. ## For example the default configuration will generate JUnitReport-failures.xml. ## default: failures: [Failed, Flaky] # junit-report-config: # skipped: [Skipped] # passed: [PASSED] # failures: [failed, flaky] Command-line arguments These can be included alongside the configuration file. They will override or supplement the configuration file depending on the usage. Execution The test execution is composing small steps to fulfill operations like: Calculating time-efficient sharding. Preparing (creating or reusing) virtual devices for test execution. Executing tests on prepared virtual devices. Generating report. To learn more about the execution process check the: Architecture design - which is explaining some core abstract concepts. Corellium integration diagrams - to visualise relations and dependencies between layers. Domain module design & implementation - for detailed information about business logic and low-level implementation design. Results The command execution process is generating different types of output. Console logs The process during its runtime is printing detailed information about execution steps to console log. Output files The successful run should generate the following files: JUnitReport.xml - raw report with reruns. JUnitReport_failures.xml - only failed and flaky tests. or any report files generated according to junit-report-config option. android_shards.json adb_log Directory that contains dumped log from am instrument commands. Each dump name is related instance id. Errors The execution or its part can fail due to exceptions occur. Typically, most of the errors can be sourced in incorrect initial arguments or network issues. List of known possible errors: Test result parsing Flank is using am instrument command to execute tests on Corellium devices. The console output of the device is collected and parsed until all expected tests return their results. Due to an invalid apk file, the console can print unexpected output that cannot be parsed by Flank. In this case, Flank will print an error message similar to the following: Error while parsing results from instance { instance id }. For details check \"results/corellium/android/{ report_dir }/adb_log/{ instance id }\" lines { from..to }. java.lang.Exception at flank.corellium.cli.RunTestCorelliumAndroidCommandTest.outputTest(RunTestCorelliumAndroidCommandTest.kt:242) { ... } The visible exception is related directly to the parsing issue. To see the source of the problem check the log file referenced in the error message. The file should contain a direct dump from am instrument command . Features Filtering tests using test-targets. Calculating multi-module shards. Reusing test cases duration for sharding. Creating or reusing instances (devices). Installing APKs on remote devices. Running android tests. Flaky test detection. Dumping shards to file. Parsing adb am instrument logs. Generating JUnit report. Roadmap Cleaning devices after test execution. iOS support. and much more...","title":"Corellium"},{"location":"flank_corellium/#flank-corellium","text":"Run mobile tests in parallel on virtual devices driven by Corellium backend.","title":"Flank - Corellium"},{"location":"flank_corellium/#status","text":"The Flank - Corellium integration is at the MVP stage, so only the core and most important features are available. Read more about Minimum Viable Product.","title":"Status"},{"location":"flank_corellium/#supported-api-levels","text":"The following Android API levels are supported: 25, 27, 28, 29, and 30. Additional API levels may be added based on customer feedback.","title":"Supported API levels"},{"location":"flank_corellium/#why-corellium","text":"Flank is just a client-side application that can prepare a time-efficient parallel test plan to run on several devices. It requires a third-party provider that can serve a huge amount of devices to run tests on them. Corellium is solving this problem as follows: Provides technology to virtualize mobile operating systems on servers powered by ARM cores. - This gives an incredible ability for scaling with regards to the amount of devices. Gives access to the bare operating system (Android or iOS). - Which allows a run optimized sharding algorithm that improves test execution time and reduces costs.","title":"Why Corellium"},{"location":"flank_corellium/#how-to-get","text":"","title":"How to get"},{"location":"flank_corellium/#the-latest-build","text":"Flank - Corellium integration is built in the flank.jar executable, so the latest Flank build gives you access to the features driven on the Corellium backend.","title":"The Latest build"},{"location":"flank_corellium/#manual-compilation","text":"Clone the repository and go to dir: git clone git@github.com:Flank/flank.git cd flank Build flank using flank-scripts (this method will give you access to flank.jar through flank shell command): . .env flankScripts assemble flank Or build directly using gradle command: ./gradlew :test_runner:build :test_runner:shadowJar","title":"Manual compilation"},{"location":"flank_corellium/#how-to-run","text":"To call the root command for Corellium related features: locate your flank.jar in terminal and run: flank.jar corellium or if flank was build using . .env && flankScripts assemble flank , just type: flank corellium","title":"How to run"},{"location":"flank_corellium/#authorization","text":"To allow Flank working with Corellium backend is necessary to provide authentication data. By default, Flank is recognizing corellium_auth.yml as an authentication file, which should look as follows: host : your.corellium.backend.host username : your_username password : your_password","title":"Authorization"},{"location":"flank_corellium/#android-test-execution","text":"To execute android instrumented tests, run following command: $ flank.jar corellium test android run -c = \"./flank_corellium.yml\"","title":"Android test execution"},{"location":"flank_corellium/#config","text":"The example configuration file looks following: ### Test apks ## The list of app and test apks. ## Each app apk must have one or more corresponding test apks. apks : - path : \"app1.apk\" tests : - path : \"app1-test1.apk\" - path : \"app2.apk\" tests : - path : \"app2-test1.apk\" - path : \"app2-test2.apk\" ### Test Targets ## A list of one or more test target filters to apply (default: run all test targets). ## Each target filter must be fully qualified with the package name, class name, or test annotation desired. ## Supported test filters by am instrument -e \u2026 include: ## class, notClass, size, annotation, notAnnotation, package, notPackage, testFile, notTestFile ## See https://developer.android.com/reference/android/support/test/runner/AndroidJUnitRunner for more information. # test-targets: # - class com.example.app.ExampleUiTest#testPasses # - package com.example.app.foo # - notPackage com.example.app.bar ### Authorization file ## Path to YAML file with host address and credentials. ## default: corellium_auth.yml # auth: auth_file.yml ### Project name ## The name of Corellium project. ## default: \"Default project\" # project: \"project name\" ### Max Test Shards ## The amount of groups to split the test suite into. ## default: 1 # max-test-shards: 10000 ### Results Directory ## The name of a local directory where test results will be stored. ## default: results/corellium/android/yyyy-MM-dd_HH-mm-ss-SSS # local-result-dir: test-results ### Obfuscate dump ## Replace internal test names with unique identifiers, before dumping them to \"android-shards.json\". ## This option is hiding sensitive details about tests. ## default: false # obfuscate: true ### Configure JUnit reports ## A map of name suffixes related to set of result types required to include in custom junit report. ## Available result types to include are: [Skipped, Passed, Failed, Flaky] (values are not case-sensitive). ## As results, this option will generate additional amount of junit reports named `JUnitReport-$suffix.xml`. ## For example the default configuration will generate JUnitReport-failures.xml. ## default: failures: [Failed, Flaky] # junit-report-config: # skipped: [Skipped] # passed: [PASSED] # failures: [failed, flaky]","title":"Config"},{"location":"flank_corellium/#command-line-arguments","text":"These can be included alongside the configuration file. They will override or supplement the configuration file depending on the usage.","title":"Command-line arguments"},{"location":"flank_corellium/#execution","text":"The test execution is composing small steps to fulfill operations like: Calculating time-efficient sharding. Preparing (creating or reusing) virtual devices for test execution. Executing tests on prepared virtual devices. Generating report. To learn more about the execution process check the: Architecture design - which is explaining some core abstract concepts. Corellium integration diagrams - to visualise relations and dependencies between layers. Domain module design & implementation - for detailed information about business logic and low-level implementation design.","title":"Execution"},{"location":"flank_corellium/#results","text":"The command execution process is generating different types of output.","title":"Results"},{"location":"flank_corellium/#console-logs","text":"The process during its runtime is printing detailed information about execution steps to console log.","title":"Console logs"},{"location":"flank_corellium/#output-files","text":"The successful run should generate the following files: JUnitReport.xml - raw report with reruns. JUnitReport_failures.xml - only failed and flaky tests. or any report files generated according to junit-report-config option. android_shards.json adb_log Directory that contains dumped log from am instrument commands. Each dump name is related instance id.","title":"Output files"},{"location":"flank_corellium/#errors","text":"The execution or its part can fail due to exceptions occur. Typically, most of the errors can be sourced in incorrect initial arguments or network issues. List of known possible errors:","title":"Errors"},{"location":"flank_corellium/#test-result-parsing","text":"Flank is using am instrument command to execute tests on Corellium devices. The console output of the device is collected and parsed until all expected tests return their results. Due to an invalid apk file, the console can print unexpected output that cannot be parsed by Flank. In this case, Flank will print an error message similar to the following: Error while parsing results from instance { instance id }. For details check \"results/corellium/android/{ report_dir }/adb_log/{ instance id }\" lines { from..to }. java.lang.Exception at flank.corellium.cli.RunTestCorelliumAndroidCommandTest.outputTest(RunTestCorelliumAndroidCommandTest.kt:242) { ... } The visible exception is related directly to the parsing issue. To see the source of the problem check the log file referenced in the error message. The file should contain a direct dump from am instrument command .","title":"Test result parsing"},{"location":"flank_corellium/#features","text":"Filtering tests using test-targets. Calculating multi-module shards. Reusing test cases duration for sharding. Creating or reusing instances (devices). Installing APKs on remote devices. Running android tests. Flaky test detection. Dumping shards to file. Parsing adb am instrument logs. Generating JUnit report.","title":"Features"},{"location":"flank_corellium/#roadmap","text":"Cleaning devices after test execution. iOS support. and much more...","title":"Roadmap"},{"location":"flank_mixpanel_metrics/","text":"Mixpanel events in Flank Document version: 1.0 Flank is currently tracking events with the following properties: schema_version flank_version project_id app_id device_types cost virtual physical total shard_count tests total successful failed flaky test_duration total physical virtual outcome configuration Every event contains a session id and project id. By project id you can find all events from every execution with specific project id . By session id you can find events from specific Flank execution. Schema version [schema_version] This property contains a schema version, it should be equal to this document version. Flank Version [flank_version] With these event's we can check how frequently users upgrade their Flank to a new version. Project id [project_id] This property contains the id of the project used in firebase. Bundle and package id [app_id] This event contains information about the bundle id for ios and the package id for android project. Additionally, contains platform type (android, ios). This event allows us to implement a Who uses Flank report with additional breakdown by device type . Device types [device_types] This property contains information about used devices types. Shards count [shard_count] This property contains information about count of shards. Tests result information [test] This property contains information about overall count of tests and the count of successful, failed and flaky tests. Fields: total successful failed flaky Tests duration [test_duration] This property contains information about the time of execution of tests by device type (virtual, physical, total) Fields: virtual physical total Outcome [outcome] This property contains information about the outcome of the entire run. More information you can find here: https://github.com/Flank/flank/blob/3a213579b0b8ed7ca018314f0619146408292b35/docs/feature/summary_output.md#possible-outputs Total cost and cost per device type [cost] With these event's we can realize the report How many millions per month is spent on Firebase Test lab by way of using Flank? FTL Fields: virtual physical total Corellium Cost is denominated in cents. Fields: testing - cost of tests. device - cost of device usage including device initialization, tests execution, idling, etc. Configuration [configuration] This event contains information about the configuration executed in Flank. This event allows us to track information about devices and features used in Flank. It could be useful to check what features are most important for the community. Fields reflect configuration names.","title":"Mixpanel events in Flank"},{"location":"flank_mixpanel_metrics/#mixpanel-events-in-flank","text":"","title":"Mixpanel events in Flank"},{"location":"flank_mixpanel_metrics/#document-version-10","text":"Flank is currently tracking events with the following properties: schema_version flank_version project_id app_id device_types cost virtual physical total shard_count tests total successful failed flaky test_duration total physical virtual outcome configuration Every event contains a session id and project id. By project id you can find all events from every execution with specific project id . By session id you can find events from specific Flank execution.","title":"Document version: 1.0"},{"location":"flank_mixpanel_metrics/#schema-version-schema_version","text":"This property contains a schema version, it should be equal to this document version.","title":"Schema version [schema_version]"},{"location":"flank_mixpanel_metrics/#flank-version-flank_version","text":"With these event's we can check how frequently users upgrade their Flank to a new version.","title":"Flank Version [flank_version]"},{"location":"flank_mixpanel_metrics/#project-id-project_id","text":"This property contains the id of the project used in firebase.","title":"Project id [project_id]"},{"location":"flank_mixpanel_metrics/#bundle-and-package-id-app_id","text":"This event contains information about the bundle id for ios and the package id for android project. Additionally, contains platform type (android, ios). This event allows us to implement a Who uses Flank report with additional breakdown by device type .","title":"Bundle and package id [app_id]"},{"location":"flank_mixpanel_metrics/#device-types-device_types","text":"This property contains information about used devices types.","title":"Device types [device_types]"},{"location":"flank_mixpanel_metrics/#shards-count-shard_count","text":"This property contains information about count of shards.","title":"Shards count [shard_count]"},{"location":"flank_mixpanel_metrics/#tests-result-information-test","text":"This property contains information about overall count of tests and the count of successful, failed and flaky tests. Fields: total successful failed flaky","title":"Tests result information [test]"},{"location":"flank_mixpanel_metrics/#tests-duration-test_duration","text":"This property contains information about the time of execution of tests by device type (virtual, physical, total) Fields: virtual physical total","title":"Tests duration [test_duration]"},{"location":"flank_mixpanel_metrics/#outcome-outcome","text":"This property contains information about the outcome of the entire run. More information you can find here: https://github.com/Flank/flank/blob/3a213579b0b8ed7ca018314f0619146408292b35/docs/feature/summary_output.md#possible-outputs","title":"Outcome [outcome]"},{"location":"flank_mixpanel_metrics/#total-cost-and-cost-per-device-type-cost","text":"With these event's we can realize the report How many millions per month is spent on Firebase Test lab by way of using Flank?","title":"Total cost and cost per device type [cost]"},{"location":"flank_mixpanel_metrics/#ftl","text":"Fields: virtual physical total","title":"FTL"},{"location":"flank_mixpanel_metrics/#corellium","text":"Cost is denominated in cents. Fields: testing - cost of tests. device - cost of device usage including device initialization, tests execution, idling, etc.","title":"Corellium"},{"location":"flank_mixpanel_metrics/#configuration-configuration","text":"This event contains information about the configuration executed in Flank. This event allows us to track information about devices and features used in Flank. It could be useful to check what features are most important for the community. Fields reflect configuration names.","title":"Configuration [configuration]"},{"location":"flank_secrets/","text":"Flank Secrets Flank securely communicates to Firebase Test Lab using Google's official Java SDKs for authorization. 2FA Two-factor authentication is required for everyone in the Flank organization. Running Flank Flank runs require authorization with either a Google user account or a service account. Service accounts are the recommended way to authenticate to Flank instead of using a personal account. The authorization credential is saved by default to: $HOME/.config/gcloud/application_default_credentials.json Developing Flank The Flank release job requires secrets as part of continuous delivery. A flankbot account with Member level permission is used to release artifacts on bintray. GITHUB_TOKEN - Provided by GitHub Actions JFROG_USER - Username for jfrog authentication JFROG_API_KEY - API key for jfrog used to publish releases. The API key is found in https://bintray.com/profile/edit","title":"Secrets"},{"location":"flank_secrets/#flank-secrets","text":"Flank securely communicates to Firebase Test Lab using Google's official Java SDKs for authorization.","title":"Flank Secrets"},{"location":"flank_secrets/#2fa","text":"Two-factor authentication is required for everyone in the Flank organization.","title":"2FA"},{"location":"flank_secrets/#running-flank","text":"Flank runs require authorization with either a Google user account or a service account. Service accounts are the recommended way to authenticate to Flank instead of using a personal account. The authorization credential is saved by default to: $HOME/.config/gcloud/application_default_credentials.json","title":"Running Flank"},{"location":"flank_secrets/#developing-flank","text":"The Flank release job requires secrets as part of continuous delivery. A flankbot account with Member level permission is used to release artifacts on bintray. GITHUB_TOKEN - Provided by GitHub Actions JFROG_USER - Username for jfrog authentication JFROG_API_KEY - API key for jfrog used to publish releases. The API key is found in https://bintray.com/profile/edit","title":"Developing Flank"},{"location":"flank_vision/","text":"Flank Vision Flank is a massively parallel Android and iOS test runner for Firebase Test Lab. GCloud Compatible Flank is a Kotlin reimplementation of gcloud firebase test commands. Flank strives to implement all gcloud test commands using compatible YAML syntax. The CLI flags match when possible. The same flank.yml file can be run with both gcloud and Flank. gcloud firebase test android run flank.yml:gcloud flank firebase test android run -c flank.yml Industry enabling features Flank adds features on top of gcloud CLI, such as test sharding for iOS, to improve the developer experience. The goal of Flank's features is to be implemented in the server so that all test lab customers may benefit, not just those who use Flank. By upstreaming Flank features, a new set of even more awesome capabilities can be developed on top of the server API. Vision Today the Flank team is focused on bug fixes and stabilization in both the test runner and the Fladle plugin. In the future Flank may adopt a gRPC API to enable other software to easily be built on top of Flank. Examples include test analytics and test flakiness management.","title":"Vision"},{"location":"flank_vision/#flank-vision","text":"Flank is a massively parallel Android and iOS test runner for Firebase Test Lab.","title":"Flank Vision"},{"location":"flank_vision/#gcloud-compatible","text":"Flank is a Kotlin reimplementation of gcloud firebase test commands. Flank strives to implement all gcloud test commands using compatible YAML syntax. The CLI flags match when possible. The same flank.yml file can be run with both gcloud and Flank. gcloud firebase test android run flank.yml:gcloud flank firebase test android run -c flank.yml","title":"GCloud Compatible"},{"location":"flank_vision/#industry-enabling-features","text":"Flank adds features on top of gcloud CLI, such as test sharding for iOS, to improve the developer experience. The goal of Flank's features is to be implemented in the server so that all test lab customers may benefit, not just those who use Flank. By upstreaming Flank features, a new set of even more awesome capabilities can be developed on top of the server API.","title":"Industry enabling features"},{"location":"flank_vision/#vision","text":"Today the Flank team is focused on bug fixes and stabilization in both the test runner and the Fladle plugin. In the future Flank may adopt a gRPC API to enable other software to easily be built on top of Flank. Examples include test analytics and test flakiness management.","title":"Vision"},{"location":"flutter_status/","text":"Current Flutter status Investigation of flutter-android support on gcloud including test sharding. Gcloud In the gcloud we can use following commands to run tests in shards: --num-uniform-shards --test-targets-for-shard --test-targets - this option is not designed directly for sharding but filtering Preparing flutter app All following tests require a flutter app for testing that can be build using following script: flutter build apk dir = $( pwd ) pushd android ./gradlew app:assembleAndroidTest ./gradlew app:assembleDebug -Ptarget = $dir \"/integration_tests/integration_tests.dart\" popd --num-uniform-shards Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --num-uniform-shards = 3 \\ --timeout 5m Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 1 test cases failed, 5 passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Expected behaviour The Flutter example app contains 6 test methods, so according to doc, the gcloud should create 3 shards, each shard should contain 2 methods. Investigation results One shard contains all test's Two other shards are being empty. Conclusions The result is different from expected behaviour. --test-targets-for-shard Using this option you can specify shards targets by: metod - test method name class - test class name package - test package name annotation Sharding by method Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets-for-shard \"class org.flank.flutter_example.MainActivityTest#success_test_example_5\" \\ --timeout 5m Where: success_test_example_5 is a dart test . Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 Test failed to run \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Expected behaviour The gcloud should run only one shard with one test method: org.flank.flutter_example.MainActivityTest#success_test_example_5 Investigation results Gcloud is returning Test failed to run as test details, no test are being run. Conclusions gcloud can't find dart tests. gcloud will create an empty shard. Sharding by class or package Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets-for-shard \"class org.flank.flutter_example.MainActivityTest\" \\ --timeout 5m or gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets-for-shard \"package org.flank.flutter_example\" \\ --timeout 5m Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 1 test cases failed, 5 passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Expected behaviour The gcloud should run all tests, one tests is failing intentionally. Investigation results Test results are same as expected. Conclusions The gcloud can run tests by class or package if there are exists in test apk as normal android test. --test-targets This option works similar to Using this option you can filter test cases for running only the specified units: metod - test method name class - test class name package - test package name annotation Filtering by method: Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets \"class org.flank.flutter_example.MainActivityTest#success_test_example_5\" \\ --timeout 5m Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 Test failed to run \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Filtering by class or package: Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets \"class org.flank.flutter_example.MainActivityTest\" \\ --timeout 5m or gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets \"package org.flank.flutter_example\" \\ --timeout 5m Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 1 test cases failed, 5 passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Expected behaviour The gcloud should run all tests, one tests is failing intentionally. Investigation results Test results are same as expected. Conclusions The gcloud can filter tests by class or package if there are exists in test apk as normal android test. Summary conclusion Gcloud can run flutter tests without sharding. You can find example in flutter_example , simple run build_and_run_tests_firebase.sh . Gcloud is not supporting sharding for Flutter, because all dart tests according to the flutter integration tests doc are hidden behind android test class and are not visible for gcloud. Flank gcloud : app : ./test_projects/flutter/flutter_example/build/app/outputs/apk/debug/app-debug.apk test : ./test_projects/flutter/flutter_example/build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk flank : disable-sharding : true Result: Saved 0 shards to [ ... ] /android_shards.json Uploading [ android_shards.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-23_12-02-26.392553_lYnS/... There are no tests to run. Expected behaviour The gcloud should run all tests, one tests is failing intentionally. Investigation results Currently, Flank cannot run any flutter tests. That happens because flank is calculating test methods using Flank using DexParser even with option disable-sharding . This behaviour probably could be fixed but requires more investigation. How to create flutter tests for firebase You can find Flutter Integration test plugin documentation with example here . Android side @RunWith ( FlutterTestRunner . class ) public class MainActivityTest { @Rule public ActivityTestRule < MainActivity > rule = new ActivityTestRule <> ( MainActivity . class , true , false ); } Flutter side This is the test app entry point. void main () { IntegrationTestWidgetsFlutterBinding . ensureInitialized (); testWidgets ( \"success test example\" , ( WidgetTester tester ) async { // Wait for app widget await tester . pumpWidget ( MyApp ()); // Verify that our counter starts at 0. expect ( find . text ( '0' ), findsOneWidget ); expect ( find . text ( '1' ), findsNothing ); // Tap the '+' icon and trigger a frame. await tester . tap ( find . byIcon ( Icons . add )); await tester . pump (); // Verify that our counter has incremented. expect ( find . text ( '0' ), findsNothing ); expect ( find . text ( '1' ), findsOneWidget ); }); } You need to compile the Flutter app with few steps flutter build apk ./gradlew app:assembleAndroidTest ./gradlew app:assembleDebug -Ptarget = \"path to test entry point eg. $dir /integration_tests/integration_tests.dart\" All dart tests are placed in app-debug.apk instead of app-debug-androidTest.apk . That's why Flank doesn't see flutter tests. How flutter tests work in firebase Flutter Integration Test plugin use channel to communicate between FlutterTestRunner on android side and dart code. When dart tests was finished, flutter send information about test results to native code. Native code receive information's in IntegrationTestPlugin.onMethodCall() . FlutterTestRunner sets test statuses in method FlutterTestRunner.run() in lines: notifier.fireTestStarted(d); notifier.fireTestFailure(new Failure(d, dummyException)); notifier.fireTestFinished(d); Hypothetical solution to allow flank run flutter tests Run test without sharding We can try to turn off fetching test methods by dexparser when disable-sharding is set to true Sharding support Create a channel to communicate native tests with dart code. Send from native code information about the test to run. Receive the test name and run it on dart code. After the test end send results to native code. Report results. Probably this solution needs flutter plugin development. We need to create POC to check if it's possible to do.","title":"Current Flutter status"},{"location":"flutter_status/#current-flutter-status","text":"Investigation of flutter-android support on gcloud including test sharding.","title":"Current Flutter status"},{"location":"flutter_status/#gcloud","text":"In the gcloud we can use following commands to run tests in shards: --num-uniform-shards --test-targets-for-shard --test-targets - this option is not designed directly for sharding but filtering","title":"Gcloud"},{"location":"flutter_status/#preparing-flutter-app","text":"All following tests require a flutter app for testing that can be build using following script: flutter build apk dir = $( pwd ) pushd android ./gradlew app:assembleAndroidTest ./gradlew app:assembleDebug -Ptarget = $dir \"/integration_tests/integration_tests.dart\" popd","title":"Preparing flutter app"},{"location":"flutter_status/#-num-uniform-shards","text":"Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --num-uniform-shards = 3 \\ --timeout 5m Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 1 test cases failed, 5 passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"--num-uniform-shards"},{"location":"flutter_status/#expected-behaviour","text":"The Flutter example app contains 6 test methods, so according to doc, the gcloud should create 3 shards, each shard should contain 2 methods.","title":"Expected behaviour"},{"location":"flutter_status/#investigation-results","text":"One shard contains all test's Two other shards are being empty.","title":"Investigation results"},{"location":"flutter_status/#conclusions","text":"The result is different from expected behaviour.","title":"Conclusions"},{"location":"flutter_status/#-test-targets-for-shard","text":"Using this option you can specify shards targets by: metod - test method name class - test class name package - test package name annotation","title":"--test-targets-for-shard"},{"location":"flutter_status/#sharding-by-method","text":"Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets-for-shard \"class org.flank.flutter_example.MainActivityTest#success_test_example_5\" \\ --timeout 5m Where: success_test_example_5 is a dart test . Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 Test failed to run \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Sharding by method"},{"location":"flutter_status/#expected-behaviour_1","text":"The gcloud should run only one shard with one test method: org.flank.flutter_example.MainActivityTest#success_test_example_5","title":"Expected behaviour"},{"location":"flutter_status/#investigation-results_1","text":"Gcloud is returning Test failed to run as test details, no test are being run.","title":"Investigation results"},{"location":"flutter_status/#conclusions_1","text":"gcloud can't find dart tests. gcloud will create an empty shard.","title":"Conclusions"},{"location":"flutter_status/#sharding-by-class-or-package","text":"Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets-for-shard \"class org.flank.flutter_example.MainActivityTest\" \\ --timeout 5m or gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets-for-shard \"package org.flank.flutter_example\" \\ --timeout 5m Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 1 test cases failed, 5 passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Sharding by class or package"},{"location":"flutter_status/#expected-behaviour_2","text":"The gcloud should run all tests, one tests is failing intentionally.","title":"Expected behaviour"},{"location":"flutter_status/#investigation-results_2","text":"Test results are same as expected.","title":"Investigation results"},{"location":"flutter_status/#conclusions_2","text":"The gcloud can run tests by class or package if there are exists in test apk as normal android test.","title":"Conclusions"},{"location":"flutter_status/#-test-targets","text":"This option works similar to Using this option you can filter test cases for running only the specified units: metod - test method name class - test class name package - test package name annotation","title":"--test-targets"},{"location":"flutter_status/#filtering-by-method","text":"Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets \"class org.flank.flutter_example.MainActivityTest#success_test_example_5\" \\ --timeout 5m Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 Test failed to run \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Filtering by method:"},{"location":"flutter_status/#filtering-by-class-or-package","text":"Test: gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets \"class org.flank.flutter_example.MainActivityTest\" \\ --timeout 5m or gcloud alpha firebase test android run \\ --project flank-open-source \\ --type instrumentation \\ --app build/app/outputs/apk/debug/app-debug.apk \\ --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \\ --test-targets \"package org.flank.flutter_example\" \\ --timeout 5m Result: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Failed \u2502 walleye-27-en-portrait \u2502 1 test cases failed, 5 passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Filtering by class or package:"},{"location":"flutter_status/#expected-behaviour_3","text":"The gcloud should run all tests, one tests is failing intentionally.","title":"Expected behaviour"},{"location":"flutter_status/#investigation-results_3","text":"Test results are same as expected.","title":"Investigation results"},{"location":"flutter_status/#conclusions_3","text":"The gcloud can filter tests by class or package if there are exists in test apk as normal android test.","title":"Conclusions"},{"location":"flutter_status/#summary-conclusion","text":"Gcloud can run flutter tests without sharding. You can find example in flutter_example , simple run build_and_run_tests_firebase.sh . Gcloud is not supporting sharding for Flutter, because all dart tests according to the flutter integration tests doc are hidden behind android test class and are not visible for gcloud.","title":"Summary conclusion"},{"location":"flutter_status/#flank","text":"gcloud : app : ./test_projects/flutter/flutter_example/build/app/outputs/apk/debug/app-debug.apk test : ./test_projects/flutter/flutter_example/build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk flank : disable-sharding : true Result: Saved 0 shards to [ ... ] /android_shards.json Uploading [ android_shards.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-23_12-02-26.392553_lYnS/... There are no tests to run.","title":"Flank"},{"location":"flutter_status/#expected-behaviour_4","text":"The gcloud should run all tests, one tests is failing intentionally.","title":"Expected behaviour"},{"location":"flutter_status/#investigation-results_4","text":"Currently, Flank cannot run any flutter tests. That happens because flank is calculating test methods using Flank using DexParser even with option disable-sharding . This behaviour probably could be fixed but requires more investigation.","title":"Investigation results"},{"location":"flutter_status/#how-to-create-flutter-tests-for-firebase","text":"You can find Flutter Integration test plugin documentation with example here .","title":"How to create flutter tests for firebase"},{"location":"flutter_status/#android-side","text":"@RunWith ( FlutterTestRunner . class ) public class MainActivityTest { @Rule public ActivityTestRule < MainActivity > rule = new ActivityTestRule <> ( MainActivity . class , true , false ); }","title":"Android side"},{"location":"flutter_status/#flutter-side","text":"This is the test app entry point. void main () { IntegrationTestWidgetsFlutterBinding . ensureInitialized (); testWidgets ( \"success test example\" , ( WidgetTester tester ) async { // Wait for app widget await tester . pumpWidget ( MyApp ()); // Verify that our counter starts at 0. expect ( find . text ( '0' ), findsOneWidget ); expect ( find . text ( '1' ), findsNothing ); // Tap the '+' icon and trigger a frame. await tester . tap ( find . byIcon ( Icons . add )); await tester . pump (); // Verify that our counter has incremented. expect ( find . text ( '0' ), findsNothing ); expect ( find . text ( '1' ), findsOneWidget ); }); } You need to compile the Flutter app with few steps flutter build apk ./gradlew app:assembleAndroidTest ./gradlew app:assembleDebug -Ptarget = \"path to test entry point eg. $dir /integration_tests/integration_tests.dart\" All dart tests are placed in app-debug.apk instead of app-debug-androidTest.apk . That's why Flank doesn't see flutter tests.","title":"Flutter side"},{"location":"flutter_status/#how-flutter-tests-work-in-firebase","text":"Flutter Integration Test plugin use channel to communicate between FlutterTestRunner on android side and dart code. When dart tests was finished, flutter send information about test results to native code. Native code receive information's in IntegrationTestPlugin.onMethodCall() . FlutterTestRunner sets test statuses in method FlutterTestRunner.run() in lines: notifier.fireTestStarted(d); notifier.fireTestFailure(new Failure(d, dummyException)); notifier.fireTestFinished(d);","title":"How flutter tests work in firebase"},{"location":"flutter_status/#hypothetical-solution-to-allow-flank-run-flutter-tests","text":"","title":"Hypothetical solution to allow flank run flutter tests"},{"location":"flutter_status/#run-test-without-sharding","text":"We can try to turn off fetching test methods by dexparser when disable-sharding is set to true","title":"Run test without sharding"},{"location":"flutter_status/#sharding-support","text":"Create a channel to communicate native tests with dart code. Send from native code information about the test to run. Receive the test name and run it on dart code. After the test end send results to native code. Report results. Probably this solution needs flutter plugin development. We need to create POC to check if it's possible to do.","title":"Sharding support"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/","text":"Getting delete permission exception when using results-dir option #790 Changelog Date Who? Action 7th July 2020 pawelpasterz created Description I am trying enable code-coverage and seeing this error java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug-androidTest.apk. Does the service account need higher permissions? wondering why delete is needed though (edited) I also tried with orchestrator, that didn't change anything. User's yml file gcloud: results-bucket: cloud-test-sofi-test record-video: true app: ../app/build/outputs/apk/debug/app-debug.apk test: ../app/build/outputs/apk/androidTest/debug/app-debug-androidTest.apk use-orchestrator: false environment-variables: endpoint: https://testendponit.com/ performance-metrics: false num-flaky-test-attempts: 1 device: - model: Nexus5X version: 26 locale: en orientation: portrait flank: local-result-dir: ../app/build/test-results/ui-tests/ Stacktrace: RunTests Uploading app-debug-androidTest.apk . Uploading app-debug.apk . java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug-androidTest.apk. at ftl.gc.GcStorage.upload(GcStorage.kt:144) at ftl.gc.GcStorage.upload(GcStorage.kt:50) at ftl.run.AndroidTestRunner$resolveApks$2$invokeSuspend$$inlined$forEach$lambda$2.invokeSuspend(AndroidTestRunner.kt:77) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely(CoroutineScheduler.kt:594) at kotlinx.coroutines.scheduling.CoroutineScheduler.access$runSafely(CoroutineScheduler.kt:60) at kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run(CoroutineScheduler.kt:740) Suppressed: java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug.apk. at ftl.gc.GcStorage.upload(GcStorage.kt:144) at ftl.gc.GcStorage.upload(GcStorage.kt:50) at ftl.run.AndroidTestRunner$resolveApks$2$invokeSuspend$$inlined$forEach$lambda$1.invokeSuspend(AndroidTestRunner.kt:76) ... 5 more Caused by: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug.apk. Steps to reproduce Unfortunately I was unable to reproduce this error (state for 7th July 2020). If any new information will be available -- this paragraph should be updated. What was done: 1. new FTL project with admin account + one service account (default editor permissions) 1. two flank's run with the same apks and result-dir (for both admin and service accounts) 2. first flank run as admin account and second as service account 2. new FTL project with two service accounts (default editor permissions) -- let's call them A & B 1. first flank run as A and second as B 2. repeat above but with reversed order 3. all above with the same apks and results-dir Comments and thoughts According to google docs --results-dir it is recommended use unique value for each run. Google Docs Caution: if specified, this argument must be unique for each test matrix you create, otherwise results from multiple test matrices will be overwritten or intermingled. -- that indicates there is deletion action performed when file with the same name is being uploaded. Therefor storage.object.delete is required to do so. Google Cloud IAM Permissions : Note: In order to overwrite existing objects, both storage.objects.create and storage.objects.delete permissions are required. -- as confirmation of above Service account with Editor role does not have storage.object.delete Permissions Flank uploads files to storage with the Google client and its logic is rather simple (no additional logic with creating/changing/modifying roles and permissions): val fileBlob = BlobInfo.newBuilder(rootGcsBucket, validGcsPath).build() storage.create(fileBlob, fileBytes) There might have been some changes on the Google Cloud Storage side with permissions (guessing) There were lots of changes since flank 8.1.0 . We know 8.1.0 version had some issues with upload,caching and overlapping results. Conclusion Some time was spent on this issue but no results, with given input and info, were achieved. Team should track any future issues similar to this one and update doc if any new input will be available.","title":"Getting delete permission exception when using results-dir option [#790](https://github.com/Flank/flank/issues/790)"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#getting-delete-permission-exception-when-using-results-dir-option-790","text":"","title":"Getting delete permission exception when using results-dir option #790"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#changelog","text":"Date Who? Action 7th July 2020 pawelpasterz created","title":"Changelog"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#description","text":"I am trying enable code-coverage and seeing this error java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug-androidTest.apk. Does the service account need higher permissions? wondering why delete is needed though (edited) I also tried with orchestrator, that didn't change anything. User's yml file gcloud: results-bucket: cloud-test-sofi-test record-video: true app: ../app/build/outputs/apk/debug/app-debug.apk test: ../app/build/outputs/apk/androidTest/debug/app-debug-androidTest.apk use-orchestrator: false environment-variables: endpoint: https://testendponit.com/ performance-metrics: false num-flaky-test-attempts: 1 device: - model: Nexus5X version: 26 locale: en orientation: portrait flank: local-result-dir: ../app/build/test-results/ui-tests/ Stacktrace: RunTests Uploading app-debug-androidTest.apk . Uploading app-debug.apk . java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug-androidTest.apk. at ftl.gc.GcStorage.upload(GcStorage.kt:144) at ftl.gc.GcStorage.upload(GcStorage.kt:50) at ftl.run.AndroidTestRunner$resolveApks$2$invokeSuspend$$inlined$forEach$lambda$2.invokeSuspend(AndroidTestRunner.kt:77) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely(CoroutineScheduler.kt:594) at kotlinx.coroutines.scheduling.CoroutineScheduler.access$runSafely(CoroutineScheduler.kt:60) at kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run(CoroutineScheduler.kt:740) Suppressed: java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug.apk. at ftl.gc.GcStorage.upload(GcStorage.kt:144) at ftl.gc.GcStorage.upload(GcStorage.kt:50) at ftl.run.AndroidTestRunner$resolveApks$2$invokeSuspend$$inlined$forEach$lambda$1.invokeSuspend(AndroidTestRunner.kt:76) ... 5 more Caused by: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug.apk.","title":"Description"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#steps-to-reproduce","text":"Unfortunately I was unable to reproduce this error (state for 7th July 2020). If any new information will be available -- this paragraph should be updated. What was done: 1. new FTL project with admin account + one service account (default editor permissions) 1. two flank's run with the same apks and result-dir (for both admin and service accounts) 2. first flank run as admin account and second as service account 2. new FTL project with two service accounts (default editor permissions) -- let's call them A & B 1. first flank run as A and second as B 2. repeat above but with reversed order 3. all above with the same apks and results-dir","title":"Steps to reproduce"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#comments-and-thoughts","text":"According to google docs --results-dir it is recommended use unique value for each run. Google Docs Caution: if specified, this argument must be unique for each test matrix you create, otherwise results from multiple test matrices will be overwritten or intermingled. -- that indicates there is deletion action performed when file with the same name is being uploaded. Therefor storage.object.delete is required to do so. Google Cloud IAM Permissions : Note: In order to overwrite existing objects, both storage.objects.create and storage.objects.delete permissions are required. -- as confirmation of above Service account with Editor role does not have storage.object.delete Permissions Flank uploads files to storage with the Google client and its logic is rather simple (no additional logic with creating/changing/modifying roles and permissions): val fileBlob = BlobInfo.newBuilder(rootGcsBucket, validGcsPath).build() storage.create(fileBlob, fileBytes) There might have been some changes on the Google Cloud Storage side with permissions (guessing) There were lots of changes since flank 8.1.0 . We know 8.1.0 version had some issues with upload,caching and overlapping results.","title":"Comments and thoughts"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#conclusion","text":"Some time was spent on this issue but no results, with given input and info, were achieved. Team should track any future issues similar to this one and update doc if any new input will be available.","title":"Conclusion"},{"location":"host_binaries_solutions_comparison/","text":"Host binaries solutions comparison git-lfs Docs can be found here Costs about-storage-and-bandwidth-usage Each data pack cost is $5 per month. It contains: 50gb bandwidth 50gb storage Is git lfs versioning files? Yes. Git LFS versioning files and show changed md5 sum and size. On main repository git diff looks like: diff --git a/app/build/outputs/apk/release/app-release-unsigned.apk b/app/build/outputs/apk/release/app-release-unsigned.apk index bba31d0..d165ca1 100644 --- a/app/build/outputs/apk/release/app-release-unsigned.apk +++ b/app/build/outputs/apk/release/app-release-unsigned.apk @@ -1,3 +1,3 @@ version https://git-lfs.github.com/spec/v1 -oid sha256:bf48d577836f07a3d625ee30f04eb356c0b1158770f613df0d82b6ef40d300d3 -size 1938709 +oid sha256:3349370934f8a1695b6ace6db53e58c0f24a1d9c02ce703b4a78870175c8c066 +size 1938769 How to configure git lfs? To add a file extension to git lfs you should execute the following command: git lfs track \"*.apk\" Now all files with .apk extensions will be added to git lfs How to add file The file should be added in normal way to git git add app-debug.apk git commit -m \"Add apk file\" git push origin master How to play with branches There is nothing to configure. When you modify files on branch changes not shown on the master. git-submodules Docs can be found here Is git submodules versioning files? Git submodule is another git repository linked to the main repository. On main repository git diff looks like: diff --git a/apks b/apks --- a/apks +++ b/apks @@ -1 +1 @@ -Subproject commit 7036bcb56be19490b4445a7e31e821e80b9ff870 +Subproject commit 7036bcb56be19490b4445a7e31e821e80b9ff870-dirty So from the main repository, we don't see what files changed. To check details we need to execute git diff from the submodule. Is git submodule versioning files? Yes. Git Submodule versioning binary files like standard git repository. How to configure git submodule? Create a submodule repository On main execute repository git submodule add git@github.com:url_to/awesome_submodule.git path_to_awesome_submodule. Execute git submodule init How to add file Goto submodule directory git add app-debug.apk git commit -m \"Add apk file\" git push origin master How to play with branches Actually there is an option to track specific branch on submodule. https://stackoverflow.com/questions/9189575/git-submodule-tracking-latest git-annex Docs can be found here Docs for github here We can configure git-annex to store files in places like ftp, amazon s3, etc. Probably it can reduce costs on many large files. link Costs Depends on the chosen storage provider Google drive Limitation to 15gb (free quota) After 15gb need to switch to google one or use Gsuite (10$/month and quota 100gb per user) OneDrive Pricing table: https://www.opendrive.com/pricing Free quota 5gb From table: Custom Plan: 500gb storage and 25gb daily bandwidth : 5$/month 50$/year Droplet/VPS - depend on the provider FTP/SFTP - depend on provider Is git annex versioning files? Git-annex like git lfs versioning control sum Git diff looks like: @ -1 +1 @@ ../../../../../.git/annex/objects/QJ/ZX/SHA256E-s1938733--a5bd978c2a6a9ff32bdf0ad5bd94c1362d6904c80c8f6d7890a40303a5d1d703.apk/SHA256E-s1938733--a5bd978c2a6a9ff32bdf0ad5bd94c1362d6904c80c8f6d7890a40303a5d1d703.apk ../../../../../.git/annex/objects/xG/5q/SHA256E-s1938761--038902c65338873f5936f7c5d764fc98839746036a9fffb46223bb742fd1556f.apk/SHA256E-s1938761--038902c65338873f5936f7c5d764fc98839746036a9fffb46223bb742fd1556f.apk How to configure git annex? First, you need to install git-annex brew install git-annex If you don't have brew installed. Install it from this page Installing on other systems can be found here Initialize On root repository git annex init How to add file git annex add app-debug.apk git commit -m \"Add apk file\" git push origin master How to override file Unlock file git annex unlock app-debug.apk Change file Add file to annex git annex add app-debug.apk Where to host Git-annex allows to host binary files in different locations, after a little discussion with @jan-gogo we chose top 3: Google Drive OneDrive Droplet/VPS FTP/SFTP This list is only our idea feel free to suggest another one. All of the supported storage types can be here How to play with branches When you change branch you can override file and commit changes (remember to unlock file first). File will be uploaded on configured storage. How to configure new git-annex repository with google drive Install rclone from here Install git-annex-rclone from here Go to repository directory and in console enter rclone config Init remote by git annex initremote CONFIG_NAME type=external externaltype=rclone target=CONFIG_NAME prefix=git-annex chunk=50MiB encryption=shared mac=HMACSHA512 rclone_layout=lower You can test remote by git annex testremote How to push changes to remote Unlock file git-annex unlock file_name Modify file Add file git annex add file_name --force Sync changes git annex sync file_name --content How to sync changes from remote If you don't have configured git clone and git-annex with rclone check 1 to 3 points from How to configure new git-annex repository with google drive section execute command git annex enableremote CONFIG_NAME After clone repo sync with git-annex git annex sync --content conclusion GIT LFS GIT SUBMODULES GIT ANNEX Costs 5$/month per pack (50gb) Cannot find clear answer Depend on storage provider Versioning Files Yes Yes Yes Branch support Yes Now there is an option to track specific branch on submodule. https://stackoverflow.com/questions/9189575/git-submodule-tracking-latest Yes Flexibility Yes, you can set your lfs server. Check here No Yes, can use different storage providers. Our requirement is to have remote storage for test artifacts that should cover the following points: * Allow having dedicated artifacts for specific branches. * Free to use * Public read access and restricted write Git annex was the most promising but we didn't find remote storage that will meet all our requirements. By comparing these three solutions, we decide to stay with GitHub releases and automate the release process using custom Gradle tasks.","title":"Host binaries solutions comparison"},{"location":"host_binaries_solutions_comparison/#host-binaries-solutions-comparison","text":"","title":"Host binaries solutions comparison"},{"location":"host_binaries_solutions_comparison/#git-lfs","text":"Docs can be found here","title":"git-lfs"},{"location":"host_binaries_solutions_comparison/#costs","text":"about-storage-and-bandwidth-usage Each data pack cost is $5 per month. It contains: 50gb bandwidth 50gb storage","title":"Costs"},{"location":"host_binaries_solutions_comparison/#is-git-lfs-versioning-files","text":"Yes. Git LFS versioning files and show changed md5 sum and size. On main repository git diff looks like: diff --git a/app/build/outputs/apk/release/app-release-unsigned.apk b/app/build/outputs/apk/release/app-release-unsigned.apk index bba31d0..d165ca1 100644 --- a/app/build/outputs/apk/release/app-release-unsigned.apk +++ b/app/build/outputs/apk/release/app-release-unsigned.apk @@ -1,3 +1,3 @@ version https://git-lfs.github.com/spec/v1 -oid sha256:bf48d577836f07a3d625ee30f04eb356c0b1158770f613df0d82b6ef40d300d3 -size 1938709 +oid sha256:3349370934f8a1695b6ace6db53e58c0f24a1d9c02ce703b4a78870175c8c066 +size 1938769","title":"Is git lfs versioning files?"},{"location":"host_binaries_solutions_comparison/#how-to-configure-git-lfs","text":"To add a file extension to git lfs you should execute the following command: git lfs track \"*.apk\" Now all files with .apk extensions will be added to git lfs","title":"How to configure git lfs?"},{"location":"host_binaries_solutions_comparison/#how-to-add-file","text":"The file should be added in normal way to git git add app-debug.apk git commit -m \"Add apk file\" git push origin master","title":"How to add file"},{"location":"host_binaries_solutions_comparison/#how-to-play-with-branches","text":"There is nothing to configure. When you modify files on branch changes not shown on the master.","title":"How to play with branches"},{"location":"host_binaries_solutions_comparison/#git-submodules","text":"Docs can be found here","title":"git-submodules"},{"location":"host_binaries_solutions_comparison/#is-git-submodules-versioning-files","text":"Git submodule is another git repository linked to the main repository. On main repository git diff looks like: diff --git a/apks b/apks --- a/apks +++ b/apks @@ -1 +1 @@ -Subproject commit 7036bcb56be19490b4445a7e31e821e80b9ff870 +Subproject commit 7036bcb56be19490b4445a7e31e821e80b9ff870-dirty So from the main repository, we don't see what files changed. To check details we need to execute git diff from the submodule.","title":"Is git submodules versioning files?"},{"location":"host_binaries_solutions_comparison/#is-git-submodule-versioning-files","text":"Yes. Git Submodule versioning binary files like standard git repository.","title":"Is git submodule versioning files?"},{"location":"host_binaries_solutions_comparison/#how-to-configure-git-submodule","text":"Create a submodule repository On main execute repository git submodule add git@github.com:url_to/awesome_submodule.git path_to_awesome_submodule. Execute git submodule init","title":"How to configure git submodule?"},{"location":"host_binaries_solutions_comparison/#how-to-add-file_1","text":"Goto submodule directory git add app-debug.apk git commit -m \"Add apk file\" git push origin master","title":"How to add file"},{"location":"host_binaries_solutions_comparison/#how-to-play-with-branches_1","text":"Actually there is an option to track specific branch on submodule. https://stackoverflow.com/questions/9189575/git-submodule-tracking-latest","title":"How to play with branches"},{"location":"host_binaries_solutions_comparison/#git-annex","text":"Docs can be found here Docs for github here We can configure git-annex to store files in places like ftp, amazon s3, etc. Probably it can reduce costs on many large files. link","title":"git-annex"},{"location":"host_binaries_solutions_comparison/#costs_1","text":"Depends on the chosen storage provider Google drive Limitation to 15gb (free quota) After 15gb need to switch to google one or use Gsuite (10$/month and quota 100gb per user) OneDrive Pricing table: https://www.opendrive.com/pricing Free quota 5gb From table: Custom Plan: 500gb storage and 25gb daily bandwidth : 5$/month 50$/year Droplet/VPS - depend on the provider FTP/SFTP - depend on provider","title":"Costs"},{"location":"host_binaries_solutions_comparison/#is-git-annex-versioning-files","text":"Git-annex like git lfs versioning control sum Git diff looks like: @ -1 +1 @@ ../../../../../.git/annex/objects/QJ/ZX/SHA256E-s1938733--a5bd978c2a6a9ff32bdf0ad5bd94c1362d6904c80c8f6d7890a40303a5d1d703.apk/SHA256E-s1938733--a5bd978c2a6a9ff32bdf0ad5bd94c1362d6904c80c8f6d7890a40303a5d1d703.apk ../../../../../.git/annex/objects/xG/5q/SHA256E-s1938761--038902c65338873f5936f7c5d764fc98839746036a9fffb46223bb742fd1556f.apk/SHA256E-s1938761--038902c65338873f5936f7c5d764fc98839746036a9fffb46223bb742fd1556f.apk","title":"Is git annex versioning files?"},{"location":"host_binaries_solutions_comparison/#how-to-configure-git-annex","text":"First, you need to install git-annex brew install git-annex If you don't have brew installed. Install it from this page Installing on other systems can be found here Initialize On root repository git annex init","title":"How to configure git annex?"},{"location":"host_binaries_solutions_comparison/#how-to-add-file_2","text":"git annex add app-debug.apk git commit -m \"Add apk file\" git push origin master","title":"How to add file"},{"location":"host_binaries_solutions_comparison/#how-to-override-file","text":"Unlock file git annex unlock app-debug.apk Change file Add file to annex git annex add app-debug.apk","title":"How to override file"},{"location":"host_binaries_solutions_comparison/#where-to-host","text":"Git-annex allows to host binary files in different locations, after a little discussion with @jan-gogo we chose top 3: Google Drive OneDrive Droplet/VPS FTP/SFTP This list is only our idea feel free to suggest another one. All of the supported storage types can be here","title":"Where to host"},{"location":"host_binaries_solutions_comparison/#how-to-play-with-branches_2","text":"When you change branch you can override file and commit changes (remember to unlock file first). File will be uploaded on configured storage.","title":"How to play with branches"},{"location":"host_binaries_solutions_comparison/#how-to-configure-new-git-annex-repository-with-google-drive","text":"Install rclone from here Install git-annex-rclone from here Go to repository directory and in console enter rclone config Init remote by git annex initremote CONFIG_NAME type=external externaltype=rclone target=CONFIG_NAME prefix=git-annex chunk=50MiB encryption=shared mac=HMACSHA512 rclone_layout=lower You can test remote by git annex testremote","title":"How to configure new git-annex repository with google drive"},{"location":"host_binaries_solutions_comparison/#how-to-push-changes-to-remote","text":"Unlock file git-annex unlock file_name Modify file Add file git annex add file_name --force Sync changes git annex sync file_name --content","title":"How to push changes to remote"},{"location":"host_binaries_solutions_comparison/#how-to-sync-changes-from-remote","text":"If you don't have configured git clone and git-annex with rclone check 1 to 3 points from How to configure new git-annex repository with google drive section execute command git annex enableremote CONFIG_NAME After clone repo sync with git-annex git annex sync --content","title":"How to sync changes from remote"},{"location":"host_binaries_solutions_comparison/#conclusion","text":"GIT LFS GIT SUBMODULES GIT ANNEX Costs 5$/month per pack (50gb) Cannot find clear answer Depend on storage provider Versioning Files Yes Yes Yes Branch support Yes Now there is an option to track specific branch on submodule. https://stackoverflow.com/questions/9189575/git-submodule-tracking-latest Yes Flexibility Yes, you can set your lfs server. Check here No Yes, can use different storage providers. Our requirement is to have remote storage for test artifacts that should cover the following points: * Allow having dedicated artifacts for specific branches. * Free to use * Public read access and restricted write Git annex was the most promising but we didn't find remote storage that will meet all our requirements. By comparing these three solutions, we decide to stay with GitHub releases and automate the release process using custom Gradle tasks.","title":"conclusion"},{"location":"hypershard_android/","text":"Hypershard Android This is an introduction and breakdown of the steps required to make use of Hypershard-android . It should be used in cases where an application has a significant number of UI tests that need to be sharded correctly to run efficiently. On Android UI tests sharding require the entire application to be built first. The hypershard library removes the need for building and outputs a list of all available tests. More information on this can be found at the official library: Hypershard-android Installation Releases can be found downloaded from Maven Central Ensure that you download the correct suffixed as follows *-all.jar library Alternatively cloning the repository found here Then running ./gradlew install will produce a valid jarfile in the build directory. Running Open Terminal/cmd/bash Make sure that the hypershard jar file is either copied to the correct location and/or available on the path. Hypershard options are as follows: ```bash java -jar hypershard-1.1.2-all.jar --help Usage: hypershardcommand [OPTIONS] [dirs]... Hypershard is a fast and simple test collector that uses the Kotlin and Java ASTs. Hypershard CLI will print full qualified test names found in dir(s). Options: --annotation-name TEXT Class annotation name to process. For example, if this was set to 'UiTest', then Hypershard will only process classes annotated with @UiTest. --not-annotation-name TEXT Class annotation name not to process. For example, if this was set to 'UiTest', then Hypershard will not process classes annotated with @UiTest. -h, --help Show this message and exit Arguments: dirs Dir(s) to process. The location of the test classes to parse ``` An example run of hypershard for flank where the the current directory is flank root: >java -jar hypershard-1.1.2-all.jar ./test_runner/src/test/kotlin/ftl/reports/util/ Results in the following output ftl.reports.util.EndsWithTextWithOptionalSlashAtTheEndTest.should properly found end suffix matching text 4. Which can be saved by simple bash commands such as the > character for example: >java -jar hypershard-1.1.2-all.jar ./test_runner/src/test/kotlin/ftl/reports/util/ > hypershardtests More examples A python usage example can be found: https://github.com/dropbox/hypershard-android/tree/master/example Resources https://github.com/dropbox/hypershard-android","title":"Android"},{"location":"hypershard_android/#hypershard-android","text":"This is an introduction and breakdown of the steps required to make use of Hypershard-android . It should be used in cases where an application has a significant number of UI tests that need to be sharded correctly to run efficiently. On Android UI tests sharding require the entire application to be built first. The hypershard library removes the need for building and outputs a list of all available tests. More information on this can be found at the official library: Hypershard-android","title":"Hypershard Android"},{"location":"hypershard_android/#installation","text":"Releases can be found downloaded from Maven Central Ensure that you download the correct suffixed as follows *-all.jar library Alternatively cloning the repository found here Then running ./gradlew install will produce a valid jarfile in the build directory.","title":"Installation"},{"location":"hypershard_android/#running","text":"Open Terminal/cmd/bash Make sure that the hypershard jar file is either copied to the correct location and/or available on the path. Hypershard options are as follows: ```bash java -jar hypershard-1.1.2-all.jar --help Usage: hypershardcommand [OPTIONS] [dirs]... Hypershard is a fast and simple test collector that uses the Kotlin and Java ASTs. Hypershard CLI will print full qualified test names found in dir(s). Options: --annotation-name TEXT Class annotation name to process. For example, if this was set to 'UiTest', then Hypershard will only process classes annotated with @UiTest. --not-annotation-name TEXT Class annotation name not to process. For example, if this was set to 'UiTest', then Hypershard will not process classes annotated with @UiTest. -h, --help Show this message and exit Arguments: dirs Dir(s) to process. The location of the test classes to parse ``` An example run of hypershard for flank where the the current directory is flank root: >java -jar hypershard-1.1.2-all.jar ./test_runner/src/test/kotlin/ftl/reports/util/ Results in the following output ftl.reports.util.EndsWithTextWithOptionalSlashAtTheEndTest.should properly found end suffix matching text 4. Which can be saved by simple bash commands such as the > character for example: >java -jar hypershard-1.1.2-all.jar ./test_runner/src/test/kotlin/ftl/reports/util/ > hypershardtests","title":"Running"},{"location":"hypershard_android/#more-examples","text":"A python usage example can be found: https://github.com/dropbox/hypershard-android/tree/master/example","title":"More examples"},{"location":"hypershard_android/#resources","text":"https://github.com/dropbox/hypershard-android","title":"Resources"},{"location":"hypershard_ios/","text":"Hypershard iOS This is a step-by-step guide of using Hypershard for companies that are unable to use symbol table dumping on iOS. Installation Clone Github repository using https - git clone https://github.com/dropbox/hypershard-ios.git SSH - git clone git@github.com:dropbox/hypershard-ios.git Github CLI - gh repo clone dropbox/hypershard-ios Install swift if you are not using macOS https://swift.org/download/#releases Open Terminal/cmd Change working directory to the path where you clone hypershard-ios Build hyper shard-ios with command swift build -c release It will build the binary into the .build folder The resulting binary will be placed in the .build/release/hypershard . Running Open Terminal/cmd change the working directory to .build/release or add it to PATH To run hypershard make CLI invocation with the command: hypershard TEST_TARGET_NAME ROOT_PATH where - `TEST_TARGET_NAME` - the name of the Xcode test target containing the UI tests - `ROOT_PATH` - either a path where all the `XCUITest` s classes are stored , or the path of the Xcode project containing `TEST_TARGET_NAME` for example based on Flank test project ./hypershard EarlGreyExample test_projects/ios/EarlGreyExample/EarlGreyExampleSwiftTests The output will be printed to console { \"path\" : \"\" , \"phase\" : \"XCUI test shard\" , \"env\" :{}, \"cmd\" : \"\" , \"tests\" :[ \"EarlGreyExample.EarlGreyExampleSwiftTests.testThatThrows\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testBasicSelection\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testBasicSelectionAndAction\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testBasicSelectionAndAssert\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testBasicSelectionActionAssert\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testSelectionOnMultipleElements\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testCollectionMatchers\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithInRoot\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithCustomMatcher\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testTableCellOutOfScreen\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testCatchErrorOnFailure\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testCustomAction\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithCustomAssertion\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithCustomFailureHandler\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testLayout\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithCondition\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithGreyAssertions\" ]} If you would like to store the output to a file just add a path to the file with --path option For example: ./hypershard EarlGreyExample test_projects/ios/EarlGreyExample/EarlGreyExampleSwiftTests --path results.json Resources https://github.com/dropbox/hypershard-ios","title":"iOS"},{"location":"hypershard_ios/#hypershard-ios","text":"This is a step-by-step guide of using Hypershard for companies that are unable to use symbol table dumping on iOS.","title":"Hypershard iOS"},{"location":"hypershard_ios/#installation","text":"Clone Github repository using https - git clone https://github.com/dropbox/hypershard-ios.git SSH - git clone git@github.com:dropbox/hypershard-ios.git Github CLI - gh repo clone dropbox/hypershard-ios Install swift if you are not using macOS https://swift.org/download/#releases Open Terminal/cmd Change working directory to the path where you clone hypershard-ios Build hyper shard-ios with command swift build -c release It will build the binary into the .build folder The resulting binary will be placed in the .build/release/hypershard .","title":"Installation"},{"location":"hypershard_ios/#running","text":"Open Terminal/cmd change the working directory to .build/release or add it to PATH To run hypershard make CLI invocation with the command: hypershard TEST_TARGET_NAME ROOT_PATH where - `TEST_TARGET_NAME` - the name of the Xcode test target containing the UI tests - `ROOT_PATH` - either a path where all the `XCUITest` s classes are stored , or the path of the Xcode project containing `TEST_TARGET_NAME` for example based on Flank test project ./hypershard EarlGreyExample test_projects/ios/EarlGreyExample/EarlGreyExampleSwiftTests The output will be printed to console { \"path\" : \"\" , \"phase\" : \"XCUI test shard\" , \"env\" :{}, \"cmd\" : \"\" , \"tests\" :[ \"EarlGreyExample.EarlGreyExampleSwiftTests.testThatThrows\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testBasicSelection\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testBasicSelectionAndAction\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testBasicSelectionAndAssert\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testBasicSelectionActionAssert\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testSelectionOnMultipleElements\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testCollectionMatchers\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithInRoot\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithCustomMatcher\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testTableCellOutOfScreen\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testCatchErrorOnFailure\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testCustomAction\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithCustomAssertion\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithCustomFailureHandler\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testLayout\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithCondition\" , \"EarlGreyExample.EarlGreyExampleSwiftTests.testWithGreyAssertions\" ]} If you would like to store the output to a file just add a path to the file with --path option For example: ./hypershard EarlGreyExample test_projects/ios/EarlGreyExample/EarlGreyExampleSwiftTests --path results.json","title":"Running"},{"location":"hypershard_ios/#resources","text":"https://github.com/dropbox/hypershard-ios","title":"Resources"},{"location":"instrumentation_tests/","text":"Flank instrumentation tests Flank contains a project for instrumentation tests placed on flank_tests directory. Tests can be run with Gradle wrapper and parametrized by command-line arguments Commands flank-path location of flank.jar yml-path location of test yml run-params optional additional run parameters, default parameters depend on the platform for iOS is firebase, test, ios, run for android firebase, test, ios, run working-directory optional parameter for set working directory default is: ./ output-pattern optional parameter for set regex to compare output expected-output-code optional parameter for for veryfi output code default: 0 Example of run android test ./gradlew test --tests IntegrationTests.shouldMatchAndroidSuccessExitCodeAndPattern -Dflank-path=../test_runner/build/libs/flank.jar -Dyml-path=./src/test/resources/flank_android.yml Example of run ios test ./gradlew test --tests IntegrationTests.shouldMatchIosSuccessExitCodeAndPattern -Dflank-path=../test_runner/build/libs/flank.jar -Dyml-path=./src/test/resources/flank_ios.yml","title":"Flank instrumentation tests"},{"location":"instrumentation_tests/#flank-instrumentation-tests","text":"Flank contains a project for instrumentation tests placed on flank_tests directory. Tests can be run with Gradle wrapper and parametrized by command-line arguments","title":"Flank instrumentation tests"},{"location":"instrumentation_tests/#commands","text":"flank-path location of flank.jar yml-path location of test yml run-params optional additional run parameters, default parameters depend on the platform for iOS is firebase, test, ios, run for android firebase, test, ios, run working-directory optional parameter for set working directory default is: ./ output-pattern optional parameter for set regex to compare output expected-output-code optional parameter for for veryfi output code default: 0","title":"Commands"},{"location":"instrumentation_tests/#example-of-run-android-test","text":"./gradlew test --tests IntegrationTests.shouldMatchAndroidSuccessExitCodeAndPattern -Dflank-path=../test_runner/build/libs/flank.jar -Dyml-path=./src/test/resources/flank_android.yml","title":"Example of run android test"},{"location":"instrumentation_tests/#example-of-run-ios-test","text":"./gradlew test --tests IntegrationTests.shouldMatchIosSuccessExitCodeAndPattern -Dflank-path=../test_runner/build/libs/flank.jar -Dyml-path=./src/test/resources/flank_ios.yml","title":"Example of run ios test"},{"location":"investigate_flank_options/","text":"Investigate flank options List of options android gcloud app test additional-apks auto-google-login no-auto-google-login use-orchestrator no-use-orchestrator environment-variables directories-to-pull other-files performance-metrics no-performance-metrics num-uniform-shards test-runner-class test-targets robo-directives robo-script results-bucket results-dir record-video no-record-video timeout async client-details network-profile results-history-name num-flaky-test-attempts device flank additional-app-test-apks legacy-junit-result max-test-shards shard-time num-test-runs smart-flank-gcs-path smart-flank-disable-upload disable-sharding test-targets-always-run files-to-download project local-result-dir run-timeout full-junit-result ignore-failed-tests keep-file-path output-style disable-results-upload default-test-time default-class-test-time use-average-test-time-for-new-tests List of options ios gcloud test xctestrun-file xcode-version results-bucket results-dir record-video no-record-video timeout async client-details network-profile results-history-name num-flaky-test-attempts device flank test-targets max-test-shards shard-time num-test-runs smart-flank-gcs-path smart-flank-disable-upload disable-sharding test-targets-always-run files-to-download project local-result-dir run-timeout full-junit-result ignore-failed-tests keep-file-path output-style disable-results-upload default-test-time default-class-test-time use-average-test-time-for-new-tests Investigation report environment-variables (Android) Set the directories-to-pull variable to pull from the device directory with coverage report. There will be no warnings or failure messages when environment-variables is set without directories-to-pull A warning has been added about this. files-to-download (Android) In the case where coverage reports need to be downloaded set the directories-to-pull variable. There will be no warnings or failures when files-to-download is set without directories-to-pull . A warning is added regarding this. disable-sharding (Common) Can be enabled by setting max-test-shards to greater than one. In this case flank will disable sharding A warning is added regarding this. num-uniform-shards (Android) When set with max-test-shards Flank will fail fast. When set with disable-sharding , Flank will disable sharding without any warning Warning added about this.","title":"Investigate flank options"},{"location":"investigate_flank_options/#investigate-flank-options","text":"","title":"Investigate flank options"},{"location":"investigate_flank_options/#list-of-options-android","text":"","title":"List of options android"},{"location":"investigate_flank_options/#gcloud","text":"app test additional-apks auto-google-login no-auto-google-login use-orchestrator no-use-orchestrator environment-variables directories-to-pull other-files performance-metrics no-performance-metrics num-uniform-shards test-runner-class test-targets robo-directives robo-script results-bucket results-dir record-video no-record-video timeout async client-details network-profile results-history-name num-flaky-test-attempts device","title":"gcloud"},{"location":"investigate_flank_options/#flank","text":"additional-app-test-apks legacy-junit-result max-test-shards shard-time num-test-runs smart-flank-gcs-path smart-flank-disable-upload disable-sharding test-targets-always-run files-to-download project local-result-dir run-timeout full-junit-result ignore-failed-tests keep-file-path output-style disable-results-upload default-test-time default-class-test-time use-average-test-time-for-new-tests","title":"flank"},{"location":"investigate_flank_options/#list-of-options-ios","text":"","title":"List of options ios"},{"location":"investigate_flank_options/#gcloud_1","text":"test xctestrun-file xcode-version results-bucket results-dir record-video no-record-video timeout async client-details network-profile results-history-name num-flaky-test-attempts device","title":"gcloud"},{"location":"investigate_flank_options/#flank_1","text":"test-targets max-test-shards shard-time num-test-runs smart-flank-gcs-path smart-flank-disable-upload disable-sharding test-targets-always-run files-to-download project local-result-dir run-timeout full-junit-result ignore-failed-tests keep-file-path output-style disable-results-upload default-test-time default-class-test-time use-average-test-time-for-new-tests","title":"flank"},{"location":"investigate_flank_options/#investigation-report","text":"","title":"Investigation report"},{"location":"investigate_flank_options/#environment-variables-android","text":"Set the directories-to-pull variable to pull from the device directory with coverage report. There will be no warnings or failure messages when environment-variables is set without directories-to-pull A warning has been added about this.","title":"environment-variables (Android)"},{"location":"investigate_flank_options/#files-to-download-android","text":"In the case where coverage reports need to be downloaded set the directories-to-pull variable. There will be no warnings or failures when files-to-download is set without directories-to-pull . A warning is added regarding this.","title":"files-to-download (Android)"},{"location":"investigate_flank_options/#disable-sharding-common","text":"Can be enabled by setting max-test-shards to greater than one. In this case flank will disable sharding A warning is added regarding this.","title":"disable-sharding (Common)"},{"location":"investigate_flank_options/#num-uniform-shards-android","text":"When set with max-test-shards Flank will fail fast. When set with disable-sharding , Flank will disable sharding without any warning Warning added about this.","title":"num-uniform-shards (Android)"},{"location":"junit_xml_comparison/","text":"iOS pass: <testsuite name= \"EarlGreyExampleSwiftTests\" hostname= \"localhost\" tests= \"16\" failures= \"0\" errors= \"0\" time= \"25.892\" > <testcase name= \"testBasicSelection()\" classname= \"EarlGreyExampleSwiftTests\" time= \"2.0\" /> <system-out/> <system-err/> iOS fail: <testsuite name= \"EarlGreyExampleSwiftTests\" hostname= \"localhost\" tests= \"17\" failures= \"1\" errors= \"0\" time= \"25.881\" > <properties/> <testcase name= \"testBasicSelectionAndAction()\" classname= \"EarlGreyExampleSwiftTests\" time= \"0.584\" > <failure> ... Android fail: <testsuite name= \"\" tests= \"2\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"3.87\" timestamp= \"2018-09-09T00:16:36\" hostname= \"localhost\" > <properties/> <testcase name= \"testFails\" classname= \"com.example.app.ExampleUiTest\" time= \"0.857\" > <failure> ... Android pass: <testsuite name= \"\" tests= \"1\" failures= \"0\" errors= \"0\" skipped= \"0\" time= \"2.278\" timestamp= \"2018-09-14T20:45:55\" hostname= \"localhost\" > <properties/> <testcase name= \"testPasses\" classname= \"com.example.app.ExampleUiTest\" time= \"0.328\" /> </testsuite> results / analysis: testsuite name - test target name tests - count of total tests failures - count of failures (test assertion failed) errors - count of errors (unhandled exceptions) time - overall time of test suite in seconds hostname - always localhost testcase name - name of test method classname - name of class that defines the test time - time in seconds Android testsuite name - always empty string tests failures errors skipped* Android only. time timestamp* Android only. hostname - always localhost testcase name classname time","title":"Junit xml comparison"},{"location":"logging/","text":"Logs in Flank Log level depends on the output style. Simple, multi and verbose output style prints logs from SIMPLE and DETAILED levels. Compact style prints log only from SIMPLE level. If you want a print message for all output styles uses log or logLn with only message parameter. If you want print message more detailed message use log or logLn and set level to OutputLogLevel.DETAILED","title":"Logs in Flank"},{"location":"logging/#logs-in-flank","text":"Log level depends on the output style. Simple, multi and verbose output style prints logs from SIMPLE and DETAILED levels. Compact style prints log only from SIMPLE level. If you want a print message for all output styles uses log or logLn with only message parameter. If you want print message more detailed message use log or logLn and set level to OutputLogLevel.DETAILED","title":"Logs in Flank"},{"location":"mock_server/","text":"Mock Server Goal: Test Flank using a mock server so we don't have to spend $$ to verify the test runner works. Proof of concepts prism prism consumes Swagger v2 and uses json-schema-faker to return generated default responses. The fields are randomly null which crashes the runner. Prism is exceptionally slow and closed sourced. There's no way to customize the behavior. Not a viable option. mock_server_inflector swagger-codegen consumes OpenAPI v3 and generates a fake server using swagger-inflector . Static canned responses are used based on the OpenAPI description. The responses aren't useful since they don't mimic a real server. Swagger generates a massive amount of code. The build system lacks gradle support . mock_server_vertx vertx-web-api-contract consumes OpenAPI v3 and generates a fake server using slush-vertx . Stub implementations for routes are provided. There are no precanned responses. Unfortunately the code generation is incompatible with the variable naming used in Firebase Test Lab. I opened an issue upstream . Of the three generated options, this looks like the nicest one. However the Google API surface is enormous and we don't need stubs for everything. mock_server ktor is a nice Kotlin server based on Netty. In a single file, all the necessary routes are implemented for each API. The routes are easy to define manually. The complexity of OpenAPI / code generation is avoided. This is the approach that's working on the kotlin_poc runner. Known issues Using a flag for mocked mode isn't ideal. Dependency injection with interfaces would be more robust.","title":"Mock Server"},{"location":"mock_server/#mock-server","text":"Goal: Test Flank using a mock server so we don't have to spend $$ to verify the test runner works.","title":"Mock Server"},{"location":"mock_server/#proof-of-concepts","text":"","title":"Proof of concepts"},{"location":"mock_server/#prism","text":"prism consumes Swagger v2 and uses json-schema-faker to return generated default responses. The fields are randomly null which crashes the runner. Prism is exceptionally slow and closed sourced. There's no way to customize the behavior. Not a viable option.","title":"prism"},{"location":"mock_server/#mock_server_inflector","text":"swagger-codegen consumes OpenAPI v3 and generates a fake server using swagger-inflector . Static canned responses are used based on the OpenAPI description. The responses aren't useful since they don't mimic a real server. Swagger generates a massive amount of code. The build system lacks gradle support .","title":"mock_server_inflector"},{"location":"mock_server/#mock_server_vertx","text":"vertx-web-api-contract consumes OpenAPI v3 and generates a fake server using slush-vertx . Stub implementations for routes are provided. There are no precanned responses. Unfortunately the code generation is incompatible with the variable naming used in Firebase Test Lab. I opened an issue upstream . Of the three generated options, this looks like the nicest one. However the Google API surface is enormous and we don't need stubs for everything.","title":"mock_server_vertx"},{"location":"mock_server/#mock_server","text":"ktor is a nice Kotlin server based on Netty. In a single file, all the necessary routes are implemented for each API. The routes are easy to define manually. The complexity of OpenAPI / code generation is avoided. This is the approach that's working on the kotlin_poc runner.","title":"mock_server"},{"location":"mock_server/#known-issues","text":"Using a flag for mocked mode isn't ideal. Dependency injection with interfaces would be more robust.","title":"Known issues"},{"location":"permissions_denied_behavior/","text":"Flank permissions denied behavior Reported on: Clean flank not authorized error messages #874 Changed on: Enhance permission denied exception logs #875 1. User don't have permission to project (403) When user don't have permission to project Flank should returns message like: Fla n k e n cou ntere d a 403 error whe n ru nn i n g o n projec t $projec t _ na me. Please veri f y t his crede nt ial is au t horized f or t he projec t . Co ns ider au t he nt ica t io n a wi t h Service Accou nt h tt ps : //github.com/Flank/flank#authenticate-with-a-service-account or wi t h a Google accou nt h tt ps : //github.com/Flank/flank#authenticate-with-a-google-account Caused by : com.google.api.clie nt .googleapis.jso n .GoogleJso n Respo nse Excep t io n : 403 Forbidde n { \"code\" : 403 , \"errors\" : [ { \"domain\" : \"global\" , \"message\" : \"The caller does not have permission\" , \"reason\" : \"forbidden\" } ], \"message\" : \"The caller does not have permission\" , \"status\" : \"PERMISSION_DENIED\" } You can reproduce the error by setting PROJECT_ID to a project that the firebase account doesn't have permission to access. 2. Project not found (404) When project not found on firebase Flank should return message like: Fla n k was u na ble t o f i n d projec t $projec t _ na me. Please veri f y t he projec t id. Caused by : com.google.api.clie nt .googleapis.jso n .GoogleJso n Respo nse Excep t io n : 404 No t Fou n d { \"code\" : 404 , \"errors\" : [ { \"domain\" : \"global\" , \"message\" : \"Project not found: $project_name\" , \"reason\" : \"notFound\" } ], \"message\" : \"Project not found: $project_name\" , \"status\" : \"NOT_FOUND\" } You can reproduce the error by setting PROJECT_ID to a project that doesn't exist. 3. On this two cases Flank throws FlankCommonException and exit with code: 1","title":"Flank permissions denied behavior"},{"location":"permissions_denied_behavior/#flank-permissions-denied-behavior","text":"Reported on: Clean flank not authorized error messages #874 Changed on: Enhance permission denied exception logs #875","title":"Flank permissions denied behavior"},{"location":"permissions_denied_behavior/#1-user-dont-have-permission-to-project-403","text":"When user don't have permission to project Flank should returns message like: Fla n k e n cou ntere d a 403 error whe n ru nn i n g o n projec t $projec t _ na me. Please veri f y t his crede nt ial is au t horized f or t he projec t . Co ns ider au t he nt ica t io n a wi t h Service Accou nt h tt ps : //github.com/Flank/flank#authenticate-with-a-service-account or wi t h a Google accou nt h tt ps : //github.com/Flank/flank#authenticate-with-a-google-account Caused by : com.google.api.clie nt .googleapis.jso n .GoogleJso n Respo nse Excep t io n : 403 Forbidde n { \"code\" : 403 , \"errors\" : [ { \"domain\" : \"global\" , \"message\" : \"The caller does not have permission\" , \"reason\" : \"forbidden\" } ], \"message\" : \"The caller does not have permission\" , \"status\" : \"PERMISSION_DENIED\" } You can reproduce the error by setting PROJECT_ID to a project that the firebase account doesn't have permission to access.","title":"1. User don't have permission to project (403)"},{"location":"permissions_denied_behavior/#2-project-not-found-404","text":"When project not found on firebase Flank should return message like: Fla n k was u na ble t o f i n d projec t $projec t _ na me. Please veri f y t he projec t id. Caused by : com.google.api.clie nt .googleapis.jso n .GoogleJso n Respo nse Excep t io n : 404 No t Fou n d { \"code\" : 404 , \"errors\" : [ { \"domain\" : \"global\" , \"message\" : \"Project not found: $project_name\" , \"reason\" : \"notFound\" } ], \"message\" : \"Project not found: $project_name\" , \"status\" : \"NOT_FOUND\" } You can reproduce the error by setting PROJECT_ID to a project that doesn't exist.","title":"2. Project not found (404)"},{"location":"permissions_denied_behavior/#3-on-this-two-cases-flank-throws-flankcommonexception-and-exit-with-code-1","text":"","title":"3. On this two cases Flank throws FlankCommonException and exit with code: 1"},{"location":"pr_titles/","text":"PR title using conventional commits Introduction The Conventional Commits specification is a lightweight convention on top of commit messages. It provides an easy set of rules for creating an explicit commit history; which makes it easier to write automated tools on top of. Usage Every PR which is not in draft mode should follow conventional commit convention for PR title. It allows us to generate release notes and avoid merge conflicts in release_notes.md file PR title Pull request title should be: <type>([optional scope]): <description> where <type> - one of following [optional scope] - additional information <description> - description of pr Type build - Changes that affect the build system or external dependencies (dependencies update) ci - Changes to our CI configuration files and scripts (basically directory .github/workflows ) docs - Documentation only changes feat - A new feature fix - A bug fix chore - Changes which does not touch the code (ex. manual update of release notes). It will not generate release notes changes refactor - A code change that contains refactor style - Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) test - Adding missing tests or correcting existing tests and also changes for our test app perf - A code change that improves performance (I do not think we will use it) Examples feat: Add locales description command for ios and android -> https://github.com/Flank/flank/pull/969 fix: rate limit exceeded -> https://github.com/Flank/flank/pull/919 ci: Added leading V to version name -> https://github.com/Flank/flank/pull/980 refactor: config entities and arguments -> https://github.com/Flank/flank/pull/831 docs: Add secrets and vision doc -> https://github.com/Flank/flank/pull/922 build: Disable Auto Doc Generation -> https://github.com/Flank/flank/pull/942 test: added multi modules to test app -> https://github.com/Flank/flank/pull/857 chore: Release v20.08.1 -> https://github.com/Flank/flank/pull/982","title":"Pull request"},{"location":"pr_titles/#pr-title-using-conventional-commits","text":"","title":"PR title using conventional commits"},{"location":"pr_titles/#introduction","text":"The Conventional Commits specification is a lightweight convention on top of commit messages. It provides an easy set of rules for creating an explicit commit history; which makes it easier to write automated tools on top of.","title":"Introduction"},{"location":"pr_titles/#usage","text":"Every PR which is not in draft mode should follow conventional commit convention for PR title. It allows us to generate release notes and avoid merge conflicts in release_notes.md file","title":"Usage"},{"location":"pr_titles/#pr-title","text":"Pull request title should be: <type>([optional scope]): <description> where <type> - one of following [optional scope] - additional information <description> - description of pr","title":"PR title"},{"location":"pr_titles/#type","text":"build - Changes that affect the build system or external dependencies (dependencies update) ci - Changes to our CI configuration files and scripts (basically directory .github/workflows ) docs - Documentation only changes feat - A new feature fix - A bug fix chore - Changes which does not touch the code (ex. manual update of release notes). It will not generate release notes changes refactor - A code change that contains refactor style - Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) test - Adding missing tests or correcting existing tests and also changes for our test app perf - A code change that improves performance (I do not think we will use it)","title":"Type"},{"location":"pr_titles/#examples","text":"feat: Add locales description command for ios and android -> https://github.com/Flank/flank/pull/969 fix: rate limit exceeded -> https://github.com/Flank/flank/pull/919 ci: Added leading V to version name -> https://github.com/Flank/flank/pull/980 refactor: config entities and arguments -> https://github.com/Flank/flank/pull/831 docs: Add secrets and vision doc -> https://github.com/Flank/flank/pull/922 build: Disable Auto Doc Generation -> https://github.com/Flank/flank/pull/942 test: added multi modules to test app -> https://github.com/Flank/flank/pull/857 chore: Release v20.08.1 -> https://github.com/Flank/flank/pull/982","title":"Examples"},{"location":"readme/","text":"Flank Kotlin Rewrite of Flank in Kotlin Design decisions. APIs are used for all Google Cloud interactions Firebase Test Lab transparently retries VMs 3x Infrastructure Failures may lead to 4 hours or more of wait times as a result, retrying VMs on failure isn't super helpful. VM timeout applies only once the VM has booted. Hours spent waiting for the VM to boot aren't included in the wait time. Test failures are best retried using a retry JUnit rule combined with Android Test Orchestrator and an app reset rule On device retry is free since we don't need to spin up a new VM. Each VM created is another opportunity for FTL to error. Even when VMs boot correctly, it's at least a 5m wait time. Client libraries Google provides two types of client libraries: Google API Client Libraries and Google Cloud Client Libraries . google-api-services-* - legacy google-api-services-storage google-api-services-toolresults google-api-services-testing google-cloud-* - newest google-cloud-storage Discovery JSON The discovery JSON is useful for understanding APIs and producing clients automatically. The discovery JSON contains the latest changes. toolresults/v1beta3/ testing/v1 or testing storage/v1 google-cloud-sdk\\lib\\googlecloudsdk\\third_party\\apis has older JSON versions. Not recommended for use. storage_v1.json toolresults_v1beta3.json testing_v1.json See client generation for more information. google-cloud-storage Published on maven central com.google.cloud:google-cloud-storage:1.15.0 google-cloud-storage github google-cloud-storage JavaDoc google-api-services-toolresults Published on maven central com.google.apis:google-api-services-toolresults:v1beta3-rev351-1.21.0 Not on GitHub google-api-services-toolresults website google-api-services-toolresults javadoc google-api-services-testing Client may be generated manually using testing_v1.json and the master branch of apis-client-generator . Google also publishes official clients: Java Python .NET Ruby Go Clone git clone https://android.googlesource.com/platform/tools/studio/google/cloud/testing git checkout studio-master-dev Android Studio repo Build gradle build Mock Servers API Discovery JSON may be converted to OpenAPI 3 using apimatic.io/transformer . See mock_server.md for details.","title":"Flank Kotlin [![Build Status](https://app.bitrise.io/app/9767f3e19047d4db/status.svg?token=uDM3wCumR2xTd0axh4bjDQ&branch=master)](https://app.bitrise.io/app/9767f3e19047d4db)"},{"location":"readme/#flank-kotlin","text":"Rewrite of Flank in Kotlin","title":"Flank Kotlin"},{"location":"readme/#design-decisions","text":"APIs are used for all Google Cloud interactions Firebase Test Lab transparently retries VMs 3x Infrastructure Failures may lead to 4 hours or more of wait times as a result, retrying VMs on failure isn't super helpful. VM timeout applies only once the VM has booted. Hours spent waiting for the VM to boot aren't included in the wait time. Test failures are best retried using a retry JUnit rule combined with Android Test Orchestrator and an app reset rule On device retry is free since we don't need to spin up a new VM. Each VM created is another opportunity for FTL to error. Even when VMs boot correctly, it's at least a 5m wait time.","title":"Design decisions."},{"location":"readme/#client-libraries","text":"Google provides two types of client libraries: Google API Client Libraries and Google Cloud Client Libraries . google-api-services-* - legacy google-api-services-storage google-api-services-toolresults google-api-services-testing google-cloud-* - newest google-cloud-storage","title":"Client libraries"},{"location":"readme/#discovery-json","text":"The discovery JSON is useful for understanding APIs and producing clients automatically. The discovery JSON contains the latest changes. toolresults/v1beta3/ testing/v1 or testing storage/v1 google-cloud-sdk\\lib\\googlecloudsdk\\third_party\\apis has older JSON versions. Not recommended for use. storage_v1.json toolresults_v1beta3.json testing_v1.json See client generation for more information.","title":"Discovery JSON"},{"location":"readme/#google-cloud-storage","text":"Published on maven central com.google.cloud:google-cloud-storage:1.15.0 google-cloud-storage github google-cloud-storage JavaDoc","title":"google-cloud-storage"},{"location":"readme/#google-api-services-toolresults","text":"Published on maven central com.google.apis:google-api-services-toolresults:v1beta3-rev351-1.21.0 Not on GitHub google-api-services-toolresults website google-api-services-toolresults javadoc","title":"google-api-services-toolresults"},{"location":"readme/#google-api-services-testing","text":"Client may be generated manually using testing_v1.json and the master branch of apis-client-generator . Google also publishes official clients: Java Python .NET Ruby Go","title":"google-api-services-testing"},{"location":"readme/#clone","text":"git clone https://android.googlesource.com/platform/tools/studio/google/cloud/testing git checkout studio-master-dev Android Studio repo","title":"Clone"},{"location":"readme/#build","text":"gradle build","title":"Build"},{"location":"readme/#mock-servers","text":"API Discovery JSON may be converted to OpenAPI 3 using apimatic.io/transformer . See mock_server.md for details.","title":"Mock Servers"},{"location":"release_process/","text":"Release process Requirements A release process should be run withing macOS environment The machine should contain: homebrew - Package manager hub - Github CLI tool Current setup Current scripts run on GitHub actions environment with macos-latest os. Script could be found on path Each push: - to master branch run Snapshot release - of tag v* run regular release Triggering release Manually Navigate to Github Actions Run job Generate release notes for next commit by using Run Workflow button After merging PR, the next tag will be pushed to repository Wait for CI job to finish Automatically Release job will run each 1st day of month After merging PR, the next tag will be pushed to repository Wait for CI job to finish CI Steps Gradle Build flankScripts and add it to PATH Set environment variables Delete old snapshot Gradle Build Flank Authenticate to hub Remove old release Rename old tag Release flank Snapshot for snapshot flow (push to master) Stable for regular flow (push tag v* ) Publish binary for Maven Central Publish binary to GithubPackages (non snapshot only)","title":"Releasing"},{"location":"release_process/#release-process","text":"","title":"Release process"},{"location":"release_process/#requirements","text":"A release process should be run withing macOS environment The machine should contain: homebrew - Package manager hub - Github CLI tool","title":"Requirements"},{"location":"release_process/#current-setup","text":"Current scripts run on GitHub actions environment with macos-latest os. Script could be found on path Each push: - to master branch run Snapshot release - of tag v* run regular release","title":"Current setup"},{"location":"release_process/#triggering-release","text":"","title":"Triggering release"},{"location":"release_process/#manually","text":"Navigate to Github Actions Run job Generate release notes for next commit by using Run Workflow button After merging PR, the next tag will be pushed to repository Wait for CI job to finish","title":"Manually"},{"location":"release_process/#automatically","text":"Release job will run each 1st day of month After merging PR, the next tag will be pushed to repository Wait for CI job to finish","title":"Automatically"},{"location":"release_process/#ci-steps","text":"Gradle Build flankScripts and add it to PATH Set environment variables Delete old snapshot Gradle Build Flank Authenticate to hub Remove old release Rename old tag Release flank Snapshot for snapshot flow (push to master) Stable for regular flow (push tag v* ) Publish binary for Maven Central Publish binary to GithubPackages (non snapshot only)","title":"CI Steps"},{"location":"smart_flank/","text":"Smart Flank Design Doc Smart Flank is a sharding algorithm that groups tests into equally sized buckets. Current implementation At the start of a run, Flank checks to see if there's a JUnit XML with timing info from the previous run. If there's no previous test time recorded, then the test is estimated to be 10 seconds. The tests are then grouped into equally timed buckets. The bucket count is set by the user provided maxTestShards count. After each test run, the aggregated JUnit XML from the new run is merged with the old run. Any tests not in the new run are discarded. Tests that were skipped, errored, failed, or empty are discarded. The test time value of successful tests is set using the time from the latest run. If a test failed in the new run and passed in the old run, the timing info from the old run is carried over. The merged XML is uploaded to the user defined smartFlankGcsPath . If smartFlankGcsPath is not defined, then the smart flank feature will not activate. Empty results are not uploaded. Example: <!-- Old Run --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"4\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"51.773\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"5.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"6.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"7.0\" /> <testcase name= \"d()\" classname= \"d\" time= \"8.0\" /> </testsuite> </testsuites> <!-- New run --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"4\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"51.773\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"1.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"2.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"0.584\" > <failure> Exception: NoMatchingElementException </failure> <failure> failed: caught \"EarlGreyInternalTestInterruptException\", \"Immediately halt execution of testcase\" </failure> </testcase> <testcase name= \"d()\" classname= \"d\" time= \"0.0\" > <skipped/> </testcase> </testsuite> </testsuites> <!-- Merged --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"3\" failures= \"0\" errors= \"0\" skipped= \"0\" time= \"10.0\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"1.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"2.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"7.0\" /> </testsuite> </testsuites> Issues Merging XML files is complicated A local run of 1 test will upload a new XML file that contains only that one test. That will discard timing info for all other tests. Does not integrate properly with the typical local/PR/master workflow. Ideas for improvement Keep a user configurable rolling number of aggregated xmls (1.xml, 2.xml, 3.xml) and shard based on the average time. Average time is expected to be more reliable than always using the last time in isolation. Identify a way of translating app binary to a default xml name (bundle id/package name) so that smart flank works out of the box for users. Talk with Firebase on how to do this locally and/or expose an API.","title":"Smart Flank"},{"location":"smart_flank/#smart-flank-design-doc","text":"Smart Flank is a sharding algorithm that groups tests into equally sized buckets.","title":"Smart Flank Design Doc"},{"location":"smart_flank/#current-implementation","text":"At the start of a run, Flank checks to see if there's a JUnit XML with timing info from the previous run. If there's no previous test time recorded, then the test is estimated to be 10 seconds. The tests are then grouped into equally timed buckets. The bucket count is set by the user provided maxTestShards count. After each test run, the aggregated JUnit XML from the new run is merged with the old run. Any tests not in the new run are discarded. Tests that were skipped, errored, failed, or empty are discarded. The test time value of successful tests is set using the time from the latest run. If a test failed in the new run and passed in the old run, the timing info from the old run is carried over. The merged XML is uploaded to the user defined smartFlankGcsPath . If smartFlankGcsPath is not defined, then the smart flank feature will not activate. Empty results are not uploaded. Example: <!-- Old Run --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"4\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"51.773\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"5.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"6.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"7.0\" /> <testcase name= \"d()\" classname= \"d\" time= \"8.0\" /> </testsuite> </testsuites> <!-- New run --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"4\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"51.773\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"1.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"2.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"0.584\" > <failure> Exception: NoMatchingElementException </failure> <failure> failed: caught \"EarlGreyInternalTestInterruptException\", \"Immediately halt execution of testcase\" </failure> </testcase> <testcase name= \"d()\" classname= \"d\" time= \"0.0\" > <skipped/> </testcase> </testsuite> </testsuites> <!-- Merged --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"3\" failures= \"0\" errors= \"0\" skipped= \"0\" time= \"10.0\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"1.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"2.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"7.0\" /> </testsuite> </testsuites>","title":"Current implementation"},{"location":"smart_flank/#issues","text":"Merging XML files is complicated A local run of 1 test will upload a new XML file that contains only that one test. That will discard timing info for all other tests. Does not integrate properly with the typical local/PR/master workflow.","title":"Issues"},{"location":"smart_flank/#ideas-for-improvement","text":"Keep a user configurable rolling number of aggregated xmls (1.xml, 2.xml, 3.xml) and shard based on the average time. Average time is expected to be more reliable than always using the last time in isolation. Identify a way of translating app binary to a default xml name (bundle id/package name) so that smart flank works out of the box for users. Talk with Firebase on how to do this locally and/or expose an API.","title":"Ideas for improvement"},{"location":"test_artifacts/","text":"Test artifacts Test artifacts are precompiled binaries necessary for CI and local testing. They can change over time so developers should be able to maintain and share them easy as possible. Overview Storage Local Local working copy of test artifacts can be find in test_artifacts directory inside local flank repository. However, this directory MUST be added to .gitignore to prevent committing binaries to git. We accept few restrictions about storing test artifacts in test_artifacts directory. * test_artifacts directory CAN contain directories with binaries required for testing. * Names of directories inside test_artifacts SHOULD reflect names of working branches. * test_artifacts directory CAN contain test artifacts archives bundles. * The name of artifact archive should match following format branchName-unixTimestamp.zip Remote The remote copy of test artifacts can be find at test_artifacts/releases We accept few restrictions about using github releases as storage for test artifacts. * The release name and tag should reflect the name of related working branch. * Each release should contain only one asset with artifacts. * The name of test artifact assets archive should be [ ~~mdp5 checksum~~ | unix timestamp ] suffixed with zip extensions. Linking artifacts For convenient switching between test artifacts we are using symbolic links. Ensure you have symbolic link to correct directory inside test_artifacts directory, otherwise the unit tests will fail because of lack of binaries. Test projects All source code of test artifacts binaries can be find in test_projects directory. Generating test artifacts source .env update_test_artifacts android ios # [ android | go | ios | all ] Working with artifacts All testArtifacts subcommands can be configured using base options. * testArtifacts -b {name} used to specify branch name for test artifacts. For example if you want to run any subcommand on artifacts dedicated for your working branch feature123 run testArtifacts -b feature123 {subcommand} . By default, it is the name of current working git branch. testArtifacts -p {path} used to specify the path to local flank repository. By default, the path is read from FLANK_ROOT env variable. To export path to your local flank repository just source .env file. Developer scenarios As a developer I want to download test artifacts before test run. Just run ./gradlew test command, this should trigger resolveArtifacts task which will update artifacts if needed. As a developer I want to switch between local test artifacts Run flankScripts testArtifacts link to create a symbolic link to test artifacts for the current branch. Run flankScripts testArtifacts -b {name} link to create a symbolic link to test artifacts for the specific branch. As a developer I want to edit existing test artifacts Edit required project in test_artifacts/releases directory. Ensure you have sourced .env file. Build required project and copy binaries using update_test_artifacts shell function or do it manually. Ensure you have linked a correct directory with artifacts. Your local tests now should you use updated artifacts. As a developer I want to upload new test artifacts to remote repository. Make sure you have directory with artifacts in test_artifacts and the name same as working branch. Run flankScripts testArtifacts zip to create zip archive. Run flankScripts testArtifacts upload to upload zip as remote copy. The script will automatically resolve the latest archive and upload it. As a developer I want to remove test artifacts Run flankScripts testArtifacts remove_remote this will remove the remote copy of test artifacts for current working branch. iOS test artifacts Currently we have 4 different iOS test projects: EarlGreyExample FlankExample FlankGameLoopExample FlankTestPlansExample Source code of each of them is located under: test_projects/ios Test artifacts for each project contains: * build output in Debug-iphoneos * zipped build output in PROJECT_NAME.zip, * .xctestrun file for each test target EarlGreyExample This project is basically clone of EarlGrey . Source project contains two test targets: EarlGreyExampleSwiftTests, EarlGreyExampleTests. Generate Run: flankScripts assemble ios earl_grey . Source Code test_projects/ios/EarlGreyExample FlankExample Simple project with two test targets: FlankExampleTests, FlankExampleSecondTests. Generate Run: flankScripts assemble ios flank_example . Source Code test_projects/ios/EarlGreyExample FlankGameLoopExample Simple SpriteKit app to test gameloop mode. It doesn't contain any test target, so test artifacts contains only IPA file. Generate Run: flankScripts assemble ios game_loop . Source Code test_projects/ios/EarlGreyExample \u26a0\ufe0f NOTE: Generating IPA requires Apple distribution certificate therefore for now it's not possible to generate it without correct Apple Developer Account. game_loop is excluded when building all iOS artifacts: update_test_artifacts ios FlankTestPlansExample iOS project with XCTestPlans. Contains AllTests test plan. Generated .xctestrun is using V2 format. More details about test plans: docs/feature/ios_test_plans.md Generate Run: flankScripts assemble ios test_plans . Source Code test_projects/ios/EarlGreyExample","title":"Test artifacts"},{"location":"test_artifacts/#test-artifacts","text":"Test artifacts are precompiled binaries necessary for CI and local testing. They can change over time so developers should be able to maintain and share them easy as possible.","title":"Test artifacts"},{"location":"test_artifacts/#overview","text":"","title":"Overview"},{"location":"test_artifacts/#storage","text":"","title":"Storage"},{"location":"test_artifacts/#local","text":"Local working copy of test artifacts can be find in test_artifacts directory inside local flank repository. However, this directory MUST be added to .gitignore to prevent committing binaries to git. We accept few restrictions about storing test artifacts in test_artifacts directory. * test_artifacts directory CAN contain directories with binaries required for testing. * Names of directories inside test_artifacts SHOULD reflect names of working branches. * test_artifacts directory CAN contain test artifacts archives bundles. * The name of artifact archive should match following format branchName-unixTimestamp.zip","title":"Local"},{"location":"test_artifacts/#remote","text":"The remote copy of test artifacts can be find at test_artifacts/releases We accept few restrictions about using github releases as storage for test artifacts. * The release name and tag should reflect the name of related working branch. * Each release should contain only one asset with artifacts. * The name of test artifact assets archive should be [ ~~mdp5 checksum~~ | unix timestamp ] suffixed with zip extensions.","title":"Remote"},{"location":"test_artifacts/#linking-artifacts","text":"For convenient switching between test artifacts we are using symbolic links. Ensure you have symbolic link to correct directory inside test_artifacts directory, otherwise the unit tests will fail because of lack of binaries.","title":"Linking artifacts"},{"location":"test_artifacts/#test-projects","text":"All source code of test artifacts binaries can be find in test_projects directory.","title":"Test projects"},{"location":"test_artifacts/#generating-test-artifacts","text":"source .env update_test_artifacts android ios # [ android | go | ios | all ]","title":"Generating test artifacts"},{"location":"test_artifacts/#working-with-artifacts","text":"All testArtifacts subcommands can be configured using base options. * testArtifacts -b {name} used to specify branch name for test artifacts. For example if you want to run any subcommand on artifacts dedicated for your working branch feature123 run testArtifacts -b feature123 {subcommand} . By default, it is the name of current working git branch. testArtifacts -p {path} used to specify the path to local flank repository. By default, the path is read from FLANK_ROOT env variable. To export path to your local flank repository just source .env file.","title":"Working with artifacts"},{"location":"test_artifacts/#developer-scenarios","text":"As a developer I want to download test artifacts before test run. Just run ./gradlew test command, this should trigger resolveArtifacts task which will update artifacts if needed. As a developer I want to switch between local test artifacts Run flankScripts testArtifacts link to create a symbolic link to test artifacts for the current branch. Run flankScripts testArtifacts -b {name} link to create a symbolic link to test artifacts for the specific branch. As a developer I want to edit existing test artifacts Edit required project in test_artifacts/releases directory. Ensure you have sourced .env file. Build required project and copy binaries using update_test_artifacts shell function or do it manually. Ensure you have linked a correct directory with artifacts. Your local tests now should you use updated artifacts. As a developer I want to upload new test artifacts to remote repository. Make sure you have directory with artifacts in test_artifacts and the name same as working branch. Run flankScripts testArtifacts zip to create zip archive. Run flankScripts testArtifacts upload to upload zip as remote copy. The script will automatically resolve the latest archive and upload it. As a developer I want to remove test artifacts Run flankScripts testArtifacts remove_remote this will remove the remote copy of test artifacts for current working branch.","title":"Developer scenarios"},{"location":"test_artifacts/#ios-test-artifacts","text":"Currently we have 4 different iOS test projects: EarlGreyExample FlankExample FlankGameLoopExample FlankTestPlansExample Source code of each of them is located under: test_projects/ios Test artifacts for each project contains: * build output in Debug-iphoneos * zipped build output in PROJECT_NAME.zip, * .xctestrun file for each test target","title":"iOS test artifacts"},{"location":"test_artifacts/#earlgreyexample","text":"This project is basically clone of EarlGrey . Source project contains two test targets: EarlGreyExampleSwiftTests, EarlGreyExampleTests.","title":"EarlGreyExample"},{"location":"test_artifacts/#generate","text":"Run: flankScripts assemble ios earl_grey .","title":"Generate"},{"location":"test_artifacts/#source-code","text":"test_projects/ios/EarlGreyExample","title":"Source Code"},{"location":"test_artifacts/#flankexample","text":"Simple project with two test targets: FlankExampleTests, FlankExampleSecondTests.","title":"FlankExample"},{"location":"test_artifacts/#generate_1","text":"Run: flankScripts assemble ios flank_example .","title":"Generate"},{"location":"test_artifacts/#source-code_1","text":"test_projects/ios/EarlGreyExample","title":"Source Code"},{"location":"test_artifacts/#flankgameloopexample","text":"Simple SpriteKit app to test gameloop mode. It doesn't contain any test target, so test artifacts contains only IPA file.","title":"FlankGameLoopExample"},{"location":"test_artifacts/#generate_2","text":"Run: flankScripts assemble ios game_loop .","title":"Generate"},{"location":"test_artifacts/#source-code_2","text":"test_projects/ios/EarlGreyExample \u26a0\ufe0f NOTE: Generating IPA requires Apple distribution certificate therefore for now it's not possible to generate it without correct Apple Developer Account. game_loop is excluded when building all iOS artifacts: update_test_artifacts ios","title":"Source Code"},{"location":"test_artifacts/#flanktestplansexample","text":"iOS project with XCTestPlans. Contains AllTests test plan. Generated .xctestrun is using V2 format. More details about test plans: docs/feature/ios_test_plans.md","title":"FlankTestPlansExample"},{"location":"test_artifacts/#generate_3","text":"Run: flankScripts assemble ios test_plans .","title":"Generate"},{"location":"test_artifacts/#source-code_3","text":"test_projects/ios/EarlGreyExample","title":"Source Code"},{"location":"test_sharding/","text":"Test Sharding Orchestrator Android Test Orchestrator removes shared state and isolates crashes. Orchestrator trades performance for stability. Tests run slower when orchestrator is enabled. Orchestrator ensures each tests runs Each test runs in a new Instrumentation instance to ensure there's no shared state. It's recommended to use clearPackageData as well to remove file system state. When a test crashes, only that tests instrumentation instance crashes which enables other tests in the suite to continue execution. Without orchestrator, a test crash will break the entire test suite. gcloud : use-orchestrator : true Orchestrator + Firebase Test lab Orchestrator is enabled by default in Flank and disabled by default in gcloud CLI. AndroidX Test 1.3.0 Beta02 or later is required to run parameterized tests with orchestrator. Both the runner and orchestrator must be updated. Local execution will use the version of Orchestrator defined in your gradle file. Firebase Test Lab currently uses Orchestrator v1.3.0. Firebase must manually upgrade Orchestrator on their devices for customers to use the new version. GCloud Sharding The gcloud CLI supports two types of sharding, uniform sharding and manual sharding via test targets for shard. --num-uniform-shards is translated to \u201c-e numShard\u201d \u201c-e shardIndex\u201d AndroidJUnitRunner arguments. This uses the native adb am instrument sharding feature which randomly shards all tests. When using uniform shards, it's possible to have shards with empty tests. Firebase will mark shards as failed when execution consists of skipped/empty tests, as this is likely an indication the tests are not configured correctly. This firebase design choice is incompatable with num-uniform-shards as you will randomly get failures when shards are empty. Using num-uniform-shards is therefor not recommended. --test-targets-for-shard allows manually specifying tests to be run. On FTL --test-targets or --test-targets-for-shard are passed as arguments to adb shell am instrument . adb shell am instrument -r -w -e class com.example.test_app.ParameterizedTest#shouldHopefullyPass com.example.test_app.test/androidx.test.runner.AndroidJUnitRunner Flank The Flank supports two types of sharding, automatic sharding and uniform sharding. Automatic sharding is enabled by default and can be disabled by --disable-sharding=true . Under the hood automatic sharding uses same API as gcloud's --test-targets-for-shard but in difference to gcloud it creates shards automatically. --dump-shards can help with verifying if calculates shards are correct. Automated sharding can work with --test-targets option. --num-uniform-shards provides same functionality as gcloud's option. Is not compatible with automatic sharding. Parameterized tests Flank v20.06.1 fix some compatibility issues with named parameterized tests, when running with sharding. \\ Table below bases on report from running parameterized tests with different configurations. Flank uses same API as gcloud so everything supported by gcloud should be also supported by flank. orchestrator disabled disabled 1.3.0-rc01 1.3.0-rc01 sharding disabled enabled disabled enabled local @RunWith(Parameterized::class) OK OK OK OK local @RunWith(Parameterized::class) {named} OK OK OK OK local @RunWith(JUnitParamsRunner::class) OK OK OK OK gcloud @RunWith(Parameterized::class) OK OK OK OK gcloud @RunWith(Parameterized::class) {named} OK OK OK OK gcloud @RunWith(JUnitParamsRunner::class) OK OK null null flank v20.06.0 @RunWith(Parameterized::class) OK OK OK OK flank v20.06.0 @RunWith(Parameterized::class) {named} OK missing OK missing flank v20.06.0 @RunWith(JUnitParamsRunner::class) OK missing null missing flank v20.06.1 @RunWith(Parameterized::class) OK OK OK OK flank v20.06.1 @RunWith(Parameterized::class) {named} OK OK OK OK flank v20.06.1 @RunWith(JUnitParamsRunner::class) OK OK null null JUnit5 Android instrumented tests has no official support for JUnit5. There is third-party support","title":"Sharding"},{"location":"test_sharding/#test-sharding","text":"","title":"Test Sharding"},{"location":"test_sharding/#orchestrator","text":"Android Test Orchestrator removes shared state and isolates crashes. Orchestrator trades performance for stability. Tests run slower when orchestrator is enabled. Orchestrator ensures each tests runs Each test runs in a new Instrumentation instance to ensure there's no shared state. It's recommended to use clearPackageData as well to remove file system state. When a test crashes, only that tests instrumentation instance crashes which enables other tests in the suite to continue execution. Without orchestrator, a test crash will break the entire test suite. gcloud : use-orchestrator : true","title":"Orchestrator"},{"location":"test_sharding/#orchestrator-firebase-test-lab","text":"Orchestrator is enabled by default in Flank and disabled by default in gcloud CLI. AndroidX Test 1.3.0 Beta02 or later is required to run parameterized tests with orchestrator. Both the runner and orchestrator must be updated. Local execution will use the version of Orchestrator defined in your gradle file. Firebase Test Lab currently uses Orchestrator v1.3.0. Firebase must manually upgrade Orchestrator on their devices for customers to use the new version.","title":"Orchestrator + Firebase Test lab"},{"location":"test_sharding/#gcloud-sharding","text":"The gcloud CLI supports two types of sharding, uniform sharding and manual sharding via test targets for shard. --num-uniform-shards is translated to \u201c-e numShard\u201d \u201c-e shardIndex\u201d AndroidJUnitRunner arguments. This uses the native adb am instrument sharding feature which randomly shards all tests. When using uniform shards, it's possible to have shards with empty tests. Firebase will mark shards as failed when execution consists of skipped/empty tests, as this is likely an indication the tests are not configured correctly. This firebase design choice is incompatable with num-uniform-shards as you will randomly get failures when shards are empty. Using num-uniform-shards is therefor not recommended. --test-targets-for-shard allows manually specifying tests to be run. On FTL --test-targets or --test-targets-for-shard are passed as arguments to adb shell am instrument . adb shell am instrument -r -w -e class com.example.test_app.ParameterizedTest#shouldHopefullyPass com.example.test_app.test/androidx.test.runner.AndroidJUnitRunner","title":"GCloud Sharding"},{"location":"test_sharding/#flank","text":"The Flank supports two types of sharding, automatic sharding and uniform sharding. Automatic sharding is enabled by default and can be disabled by --disable-sharding=true . Under the hood automatic sharding uses same API as gcloud's --test-targets-for-shard but in difference to gcloud it creates shards automatically. --dump-shards can help with verifying if calculates shards are correct. Automated sharding can work with --test-targets option. --num-uniform-shards provides same functionality as gcloud's option. Is not compatible with automatic sharding.","title":"Flank"},{"location":"test_sharding/#parameterized-tests","text":"Flank v20.06.1 fix some compatibility issues with named parameterized tests, when running with sharding. \\ Table below bases on report from running parameterized tests with different configurations. Flank uses same API as gcloud so everything supported by gcloud should be also supported by flank. orchestrator disabled disabled 1.3.0-rc01 1.3.0-rc01 sharding disabled enabled disabled enabled local @RunWith(Parameterized::class) OK OK OK OK local @RunWith(Parameterized::class) {named} OK OK OK OK local @RunWith(JUnitParamsRunner::class) OK OK OK OK gcloud @RunWith(Parameterized::class) OK OK OK OK gcloud @RunWith(Parameterized::class) {named} OK OK OK OK gcloud @RunWith(JUnitParamsRunner::class) OK OK null null flank v20.06.0 @RunWith(Parameterized::class) OK OK OK OK flank v20.06.0 @RunWith(Parameterized::class) {named} OK missing OK missing flank v20.06.0 @RunWith(JUnitParamsRunner::class) OK missing null missing flank v20.06.1 @RunWith(Parameterized::class) OK OK OK OK flank v20.06.1 @RunWith(Parameterized::class) {named} OK OK OK OK flank v20.06.1 @RunWith(JUnitParamsRunner::class) OK OK null null","title":"Parameterized tests"},{"location":"test_sharding/#junit5","text":"Android instrumented tests has no official support for JUnit5. There is third-party support","title":"JUnit5"},{"location":"update_gradle/","text":"Gradle Updating Gradle brew upgrade gradle gradle --version gradle wrapper --distribution-type all Specifiy the gradle distribution type in build.gradle : wrapper { distributionType = Wrapper.DistributionType.ALL }","title":"Gradle"},{"location":"update_gradle/#gradle","text":"","title":"Gradle"},{"location":"update_gradle/#updating-gradle","text":"brew upgrade gradle gradle --version gradle wrapper --distribution-type all Specifiy the gradle distribution type in build.gradle : wrapper { distributionType = Wrapper.DistributionType.ALL }","title":"Updating Gradle"},{"location":"using_different_environment_variables_in_different_matrices/","text":"Using different environment variables per test apk The problem Environment variables are used to configure test coverage. When you configure this in the global scope (gcloud:environment-variables) all of the matrices have the same test coverage file name. Example: gcloud: app: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environment-variables: coverage: true coverageFile: /sdcard/coverage.ec clearPackageData: true directories-to-pull: - /sdcard/ use-orchestrator: false In the case where you have configured additional test apks, all of the matrices have a coverage file named coverage.ec Solution You can override environment variables by configuring it in additional-app-test-apks Example: gcloud: app: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environment-variables: coverage: true coverageFile: /sdcard/main.ec clearPackageData: true directories-to-pull: - /sdcard/ use-orchestrator: false flank: disable-sharding: false max-test-shards: 2 files-to-download: - .*/sdcard/[^/]+\\.ec$ additional-app-test-apks: - test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environmentVariables: coverageFile: /sdcard/module_1.ec - test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environmentVariables: coverageFile: /sdcard/module_2.ec Now Flank override coverageFile in matrices and you can identify what matrix run what test","title":"Using different environment variables per test apk"},{"location":"using_different_environment_variables_in_different_matrices/#using-different-environment-variables-per-test-apk","text":"","title":"Using different environment variables per test apk"},{"location":"using_different_environment_variables_in_different_matrices/#the-problem","text":"Environment variables are used to configure test coverage. When you configure this in the global scope (gcloud:environment-variables) all of the matrices have the same test coverage file name. Example: gcloud: app: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environment-variables: coverage: true coverageFile: /sdcard/coverage.ec clearPackageData: true directories-to-pull: - /sdcard/ use-orchestrator: false In the case where you have configured additional test apks, all of the matrices have a coverage file named coverage.ec","title":"The problem"},{"location":"using_different_environment_variables_in_different_matrices/#solution","text":"You can override environment variables by configuring it in additional-app-test-apks Example: gcloud: app: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environment-variables: coverage: true coverageFile: /sdcard/main.ec clearPackageData: true directories-to-pull: - /sdcard/ use-orchestrator: false flank: disable-sharding: false max-test-shards: 2 files-to-download: - .*/sdcard/[^/]+\\.ec$ additional-app-test-apks: - test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environmentVariables: coverageFile: /sdcard/module_1.ec - test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environmentVariables: coverageFile: /sdcard/module_2.ec Now Flank override coverageFile in matrices and you can identify what matrix run what test","title":"Solution"},{"location":"windows_and_github_actions/","text":"Working with Windows on Github actions Differences between system property user.home and environment variable HOMEPATH %HOMEPATH% in a batch scripts returns D:\\Users\\runneradmin\\ In Java System.getProperty(\"user.home\") returns C:\\Users\\runneradmin\\ For Windows recommended is using System.getenv(\"HOMEPATH\") instead of System.getProperty(\"user.home\")","title":"Working with Windows on Github actions"},{"location":"windows_and_github_actions/#working-with-windows-on-github-actions","text":"","title":"Working with Windows on Github actions"},{"location":"windows_and_github_actions/#differences-between-system-property-userhome-and-environment-variable-homepath","text":"%HOMEPATH% in a batch scripts returns D:\\Users\\runneradmin\\ In Java System.getProperty(\"user.home\") returns C:\\Users\\runneradmin\\ For Windows recommended is using System.getenv(\"HOMEPATH\") instead of System.getProperty(\"user.home\")","title":"Differences between system property user.home and environment variable HOMEPATH"},{"location":"windows_wsl_guide/","text":"Building and running Flank on Windows WSL It is possible to build and run Flank on Windows through WSL. You could configure it using your own Windows machine or you could use GitHub actions to do this. Building Own Windows machine Install WSL on Windows using Microsoft Guide Launch WSL console Install JDK if you do not have any installed sudo apt-get install openjdk-15-jre sudo apt-get install openjdk-15-jdk export JAVA_HOME=/usr/lib/jvm/openjdk-15-jdk export PATH=$PATH:$JAVA_HOME/bin Install dos2unix using the command sudo apt-get install dos2unix Convert files to UNIX in your Flank repository directory find . -type f -print0 | xargs -0 -n 1 -P 4 dos2unix Make Gradlew wrapper executable chmod +x gradlew Build Flank using command ./gradlew clean build GitHub actions Setup WSL on windows-2019 runner runs-on: windows-2019 Setup default shell to wsl-bash {0} this will allow avoiding typing wsl-bash on start each command defaults: run: shell: wsl-bash {0} Use GitHub action for setup WSL - uses: Vampire/setup-wsl@v1 with: distribution: Ubuntu-20.04 additional-packages: dos2unix In order to build flank you should install java, add permission to Gradle wrapper file and prepare each file using dos2unix - name: Configure WSL for flank run: | find . -type f -print0 | xargs -0 -n 1 -P 4 dos2unix chmod +x gradlew sudo apt-get -y install openjdk-8-jdk After this step you could build flank like in every UNIX machine ./gradlew clean build For reference please check Flank team implementation of WSL workflow Running After building using the above steps or downloading using the command wget --quiet https://github.com/Flank/flank/releases/download/XXX/flank.jar -O ./flank.jar where XXX is the latest version of flank from Flank releases on GitHub You could run Flank both on your own machine or GitHub actions typing the command: java -jar <PATH TO FLANK> <COMMANDS> <OPTIONS> You could also add Flank's bash helper folder to your $PATH environment variable. This will allow you to call the shell scripts in that helper folder from anywhere.","title":"Windows WSL"},{"location":"windows_wsl_guide/#building-and-running-flank-on-windows-wsl","text":"It is possible to build and run Flank on Windows through WSL. You could configure it using your own Windows machine or you could use GitHub actions to do this.","title":"Building and running Flank on Windows WSL"},{"location":"windows_wsl_guide/#building","text":"","title":"Building"},{"location":"windows_wsl_guide/#own-windows-machine","text":"Install WSL on Windows using Microsoft Guide Launch WSL console Install JDK if you do not have any installed sudo apt-get install openjdk-15-jre sudo apt-get install openjdk-15-jdk export JAVA_HOME=/usr/lib/jvm/openjdk-15-jdk export PATH=$PATH:$JAVA_HOME/bin Install dos2unix using the command sudo apt-get install dos2unix Convert files to UNIX in your Flank repository directory find . -type f -print0 | xargs -0 -n 1 -P 4 dos2unix Make Gradlew wrapper executable chmod +x gradlew Build Flank using command ./gradlew clean build","title":"Own Windows machine"},{"location":"windows_wsl_guide/#github-actions","text":"Setup WSL on windows-2019 runner runs-on: windows-2019 Setup default shell to wsl-bash {0} this will allow avoiding typing wsl-bash on start each command defaults: run: shell: wsl-bash {0} Use GitHub action for setup WSL - uses: Vampire/setup-wsl@v1 with: distribution: Ubuntu-20.04 additional-packages: dos2unix In order to build flank you should install java, add permission to Gradle wrapper file and prepare each file using dos2unix - name: Configure WSL for flank run: | find . -type f -print0 | xargs -0 -n 1 -P 4 dos2unix chmod +x gradlew sudo apt-get -y install openjdk-8-jdk After this step you could build flank like in every UNIX machine ./gradlew clean build For reference please check Flank team implementation of WSL workflow","title":"GitHub actions"},{"location":"windows_wsl_guide/#running","text":"After building using the above steps or downloading using the command wget --quiet https://github.com/Flank/flank/releases/download/XXX/flank.jar -O ./flank.jar where XXX is the latest version of flank from Flank releases on GitHub You could run Flank both on your own machine or GitHub actions typing the command: java -jar <PATH TO FLANK> <COMMANDS> <OPTIONS> You could also add Flank's bash helper folder to your $PATH environment variable. This will allow you to call the shell scripts in that helper folder from anywhere.","title":"Running"},{"location":"bugs/1374_invalid_class/","text":"[Parametrized tests] #1374 - java.lang.ClassNotFoundException: Invalid name: Bug description Test code with parametrized code and custom name @RunWith ( Parameterized :: class ) class BrokenTestName2 ( private val testCase : TestCase ) { @Test fun test () { val expectedRemainder = if ( testCase . odd ) 1 else 0 for ( value in testCase . values ) { assertEquals ( expectedRemainder , abs ( value % 2 )) } } data class TestCase ( val values : List < Int > , val odd : Boolean , ) companion object { @ [ JvmStatic Parameterized . Parameters ( name = \"{0}\" ) ] fun parameters (): List < TestCase > = listOf ( TestCase ( listOf ( - 4 , - 2 , 0 , 2 , 4 ), odd = false ), TestCase ( listOf ( - 3 , - 5 , - 1 , 1 , 3 , 5 ), odd = true ), ) } } Will produce failures such as: java.lang.ClassNotFoundException: Invalid name: 3 at java.lang.Class.classForName(Native Method) at java.lang.Class.forName(Class.java:324) at androidx.test.internal.runner.TestLoader.doCreateRunner(TestLoader.java:72) at androidx.test.internal.runner.TestLoader.getRunnersFor(TestLoader.java:105) at androidx.test.internal.runner.TestRequestBuilder.build(TestRequestBuilder.java:804) at androidx.test.runner.AndroidJUnitRunner.buildRequest(AndroidJUnitRunner.java:575) at androidx.test.runner.AndroidJUnitRunner.onStart(AndroidJUnitRunner.java:393) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1879) and similar failures for other values interspersed with commas, e.g. java.lang.ClassNotFoundException: Invalid name: -2 , etc. Such failures appear like this in the FTL UI: Test details No matter if sharding is disabled or not, tests will always fail both on GCloud and Flank Solution Disable test orchestrator with yaml option: gcloud : ... use-orchestrator : false ... flank : ... or CLI command: --no-use-orchestrator More info Please take a look at documentation about sharding for more info","title":"[Parametrized tests] #1374 - java.lang.ClassNotFoundException: Invalid name:"},{"location":"bugs/1374_invalid_class/#parametrized-tests-1374-javalangclassnotfoundexception-invalid-name","text":"","title":"[Parametrized tests] #1374 - java.lang.ClassNotFoundException: Invalid name:"},{"location":"bugs/1374_invalid_class/#bug-description","text":"Test code with parametrized code and custom name @RunWith ( Parameterized :: class ) class BrokenTestName2 ( private val testCase : TestCase ) { @Test fun test () { val expectedRemainder = if ( testCase . odd ) 1 else 0 for ( value in testCase . values ) { assertEquals ( expectedRemainder , abs ( value % 2 )) } } data class TestCase ( val values : List < Int > , val odd : Boolean , ) companion object { @ [ JvmStatic Parameterized . Parameters ( name = \"{0}\" ) ] fun parameters (): List < TestCase > = listOf ( TestCase ( listOf ( - 4 , - 2 , 0 , 2 , 4 ), odd = false ), TestCase ( listOf ( - 3 , - 5 , - 1 , 1 , 3 , 5 ), odd = true ), ) } } Will produce failures such as: java.lang.ClassNotFoundException: Invalid name: 3 at java.lang.Class.classForName(Native Method) at java.lang.Class.forName(Class.java:324) at androidx.test.internal.runner.TestLoader.doCreateRunner(TestLoader.java:72) at androidx.test.internal.runner.TestLoader.getRunnersFor(TestLoader.java:105) at androidx.test.internal.runner.TestRequestBuilder.build(TestRequestBuilder.java:804) at androidx.test.runner.AndroidJUnitRunner.buildRequest(AndroidJUnitRunner.java:575) at androidx.test.runner.AndroidJUnitRunner.onStart(AndroidJUnitRunner.java:393) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1879) and similar failures for other values interspersed with commas, e.g. java.lang.ClassNotFoundException: Invalid name: -2 , etc. Such failures appear like this in the FTL UI:","title":"Bug description"},{"location":"bugs/1374_invalid_class/#test-details","text":"No matter if sharding is disabled or not, tests will always fail both on GCloud and Flank","title":"Test details"},{"location":"bugs/1374_invalid_class/#solution","text":"Disable test orchestrator with yaml option: gcloud : ... use-orchestrator : false ... flank : ... or CLI command: --no-use-orchestrator","title":"Solution"},{"location":"bugs/1374_invalid_class/#more-info","text":"Please take a look at documentation about sharding for more info","title":"More info"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/","text":"Non-deterministic issue when parsing JUnitReport.xml generated by Flank 20.06.2 #872 Description After updating from Flank 8.0.1 to 20.06.2 and switching to server-side sharding sometimes on our CI (TeamCity) we see following error when parsing JUnitReport.xml : [17:00:00]Ant JUnit report watcher [17:00:00][Ant JUnit report watcher] 1 report found for paths: [17:00:00][Ant JUnit report watcher] results/**/JUnitReport.xml [17:00:00][Ant JUnit report watcher] Parsing errors [17:00:00][Parsing errors] Failed to parse 1 report [17:00:00][Parsing errors] results/2020-06-25_16-47-33.675000_QnCW/JUnitReport.xml: Content is not allowed in prolog. [17:00:00][Parsing errors] jetbrains.buildServer.util.XmlXppAbstractParser$3: Content is not allowed in prolog. at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:39) at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:31) at jetbrains.buildServer.xmlReportPlugin.parsers.antJUnit.AntJUnitReportParser.parse(AntJUnitReportParser.java:179) at jetbrains.buildServer.xmlReportPlugin.ParseReportCommand.run(ParseReportCommand.java:62) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog. at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source) at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source) at jetbrains.buildServer.util.XmlXppAbstractParser.parseWithSAX(XmlXppAbstractParser.java:240) at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:37) ... 8 more The issue is non-deterministic, in most cases the report gets parsed, but this never happened to us when using Flank 8. Did anyone already run into the problem? Any ideas what could be a cause? The XML files look correct for me, I compared two files from failed and successful runs, the difference is only in reported time of running tests, both files do not contain BOM marker (checked using vim). To Reproduce Run tests using Flank and then try to parse the report using TeamCity's report parsing feature. Expected behavior No error raised by TeamCity when parsing the JUnitReport.xml . Details (please complete the following information): Flank 20.06.2 Additional info Asked user for providing raport which fails on TeamCity CI and he replied with failing report fail: <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"Pixel2-27-en-portrait\" tests= \"2\" failures= \"0\" flakes= \"0\" errors= \"0\" skipped= \"0\" time= \"39.151\" timestamp= \"2020-07-01T16:46:41\" hostname= \"localhost\" > <testcase name= \"loginSuccessBetaEnvTest\" classname= \"com.miquido.play360.login.LoginBetaEnvTest\" time= \"19.133\" /> <testcase name= \"testPaymentsCard\" classname= \"com.miquido.play360.dashboard.PaymentCardBetaEnvTest\" time= \"20.019\" /> </testsuite> </testsuites> Problem investigation techniques Validate XML Description Attached XML report was checked using some online tools to validate XML files Execution Following sites were used to validate XML (https://www.xmlvalidation.com/, https://www.w3schools.com/xml/xml_validator.asp, https://www.freeformatter.com/xml-validator-xsd.html, https://codebeautify.org/xmlvalidator, https://www.liquid-technologies.com/online-xml-validator) Results All tools confirm that provided XML report is valid Clear XML buffer unnecessary characters Description During research about stack trace provided in the task description, user suggest on stack overflow that some XML files could contain bad whitespace characters which are bytes order marked Execution Change JUnitXML.kt file to trim characters suggested in post fun JUnitTestResult ?. xmlToString (): String { if ( this == null ) return \"\" val prefix = \"<?xml version='1.0' encoding='UTF-8'?>\" return ( prefix + System . lineSeparator () + xmlPrettyWriter . writeValueAsString ( this )). clearByteOrderMarkersFromBuffer () } // According to occasional problem with parsing byte order markers should be cleared from buffer // https://stackoverflow.com/a/3030913 private fun String . clearByteOrderMarkersFromBuffer () = trim (). replaceFirst ( \"^([\\\\W]+)<\" , \"<\" ) Try following code when reading file and compare it without using option to clear buffer Results No characters were deleted, XML has the same length with and without suggested \"fix\" Checking hex code of provided raport Description XML report hex code was checked to make sure that no unnecessary characters are added to the file. Execution The report was parsed to hex representation of chars and then in revert side. Used websites (https://tomeko.net/online_tools/file_to_hex.php, https://tomeko.net/online_tools/hex_to_file.php) The file was checked using online hex viewer Results No unnecessary characters found on file. Compare reading report with using Kotlin file loading Description By using java.nio.File(\"JUnitReport.xml\") methods readLines() / length() / readText() checking is provided file is different than generated similar one (also with clearing byte order markers) Results No issues found in the report Checking git history Description Checked git history of XML reporting code. Results No breaking changes were introduced between Flank 8.0.1 and 20.06.2 Ask user about TeamCity configuration Description User was asked about agents count and configuration Results All agents are docker's containers so they are always the same and the problem isn't here. User have 2 types of tests Using MockWebServer and have ~140 tests executed on multiple shards Using real backend, not many tests executed on single shards - this type of tests sometime falling When user back to old configuration, he can still reproduce problem Between versions user changed config from: num-uniform-shards: 1 to num-test-runs: 1 But when user back to old configuration, he can still reproduce problem. Current situation Issue reported to TeamCity https://youtrack.jetbrains.com/issue/TW-66753 TeamCity team cannot reproduce the problem, thay asked about additional logs, but User could deliver them in next week (starting 13-07-2020), until that we must wait. User is testing TeamCity CI with Flank 8 to make sure that the problem is related to Flank","title":"Non-deterministic issue when parsing JUnitReport.xml generated by Flank 20.06.2 [#872](https://github.com/Flank/flank/issues/872)"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#non-deterministic-issue-when-parsing-junitreportxml-generated-by-flank-20062-872","text":"","title":"Non-deterministic issue when parsing JUnitReport.xml generated by Flank 20.06.2 #872"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description","text":"After updating from Flank 8.0.1 to 20.06.2 and switching to server-side sharding sometimes on our CI (TeamCity) we see following error when parsing JUnitReport.xml : [17:00:00]Ant JUnit report watcher [17:00:00][Ant JUnit report watcher] 1 report found for paths: [17:00:00][Ant JUnit report watcher] results/**/JUnitReport.xml [17:00:00][Ant JUnit report watcher] Parsing errors [17:00:00][Parsing errors] Failed to parse 1 report [17:00:00][Parsing errors] results/2020-06-25_16-47-33.675000_QnCW/JUnitReport.xml: Content is not allowed in prolog. [17:00:00][Parsing errors] jetbrains.buildServer.util.XmlXppAbstractParser$3: Content is not allowed in prolog. at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:39) at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:31) at jetbrains.buildServer.xmlReportPlugin.parsers.antJUnit.AntJUnitReportParser.parse(AntJUnitReportParser.java:179) at jetbrains.buildServer.xmlReportPlugin.ParseReportCommand.run(ParseReportCommand.java:62) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog. at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source) at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source) at jetbrains.buildServer.util.XmlXppAbstractParser.parseWithSAX(XmlXppAbstractParser.java:240) at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:37) ... 8 more The issue is non-deterministic, in most cases the report gets parsed, but this never happened to us when using Flank 8. Did anyone already run into the problem? Any ideas what could be a cause? The XML files look correct for me, I compared two files from failed and successful runs, the difference is only in reported time of running tests, both files do not contain BOM marker (checked using vim). To Reproduce Run tests using Flank and then try to parse the report using TeamCity's report parsing feature. Expected behavior No error raised by TeamCity when parsing the JUnitReport.xml . Details (please complete the following information): Flank 20.06.2","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#additional-info","text":"Asked user for providing raport which fails on TeamCity CI and he replied with failing report fail: <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"Pixel2-27-en-portrait\" tests= \"2\" failures= \"0\" flakes= \"0\" errors= \"0\" skipped= \"0\" time= \"39.151\" timestamp= \"2020-07-01T16:46:41\" hostname= \"localhost\" > <testcase name= \"loginSuccessBetaEnvTest\" classname= \"com.miquido.play360.login.LoginBetaEnvTest\" time= \"19.133\" /> <testcase name= \"testPaymentsCard\" classname= \"com.miquido.play360.dashboard.PaymentCardBetaEnvTest\" time= \"20.019\" /> </testsuite> </testsuites>","title":"Additional info"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#problem-investigation-techniques","text":"","title":"Problem investigation techniques"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#validate-xml","text":"","title":"Validate XML"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_1","text":"Attached XML report was checked using some online tools to validate XML files","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#execution","text":"Following sites were used to validate XML (https://www.xmlvalidation.com/, https://www.w3schools.com/xml/xml_validator.asp, https://www.freeformatter.com/xml-validator-xsd.html, https://codebeautify.org/xmlvalidator, https://www.liquid-technologies.com/online-xml-validator)","title":"Execution"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results","text":"All tools confirm that provided XML report is valid","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#clear-xml-buffer-unnecessary-characters","text":"","title":"Clear XML buffer unnecessary characters"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_2","text":"During research about stack trace provided in the task description, user suggest on stack overflow that some XML files could contain bad whitespace characters which are bytes order marked","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#execution_1","text":"Change JUnitXML.kt file to trim characters suggested in post fun JUnitTestResult ?. xmlToString (): String { if ( this == null ) return \"\" val prefix = \"<?xml version='1.0' encoding='UTF-8'?>\" return ( prefix + System . lineSeparator () + xmlPrettyWriter . writeValueAsString ( this )). clearByteOrderMarkersFromBuffer () } // According to occasional problem with parsing byte order markers should be cleared from buffer // https://stackoverflow.com/a/3030913 private fun String . clearByteOrderMarkersFromBuffer () = trim (). replaceFirst ( \"^([\\\\W]+)<\" , \"<\" ) Try following code when reading file and compare it without using option to clear buffer","title":"Execution"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_1","text":"No characters were deleted, XML has the same length with and without suggested \"fix\"","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#checking-hex-code-of-provided-raport","text":"","title":"Checking hex code of provided raport"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_3","text":"XML report hex code was checked to make sure that no unnecessary characters are added to the file.","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#execution_2","text":"The report was parsed to hex representation of chars and then in revert side. Used websites (https://tomeko.net/online_tools/file_to_hex.php, https://tomeko.net/online_tools/hex_to_file.php) The file was checked using online hex viewer","title":"Execution"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_2","text":"No unnecessary characters found on file.","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#compare-reading-report-with-using-kotlin-file-loading","text":"","title":"Compare reading report with using Kotlin file loading"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_4","text":"By using java.nio.File(\"JUnitReport.xml\") methods readLines() / length() / readText() checking is provided file is different than generated similar one (also with clearing byte order markers)","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_3","text":"No issues found in the report","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#checking-git-history","text":"","title":"Checking git history"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_5","text":"Checked git history of XML reporting code.","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_4","text":"No breaking changes were introduced between Flank 8.0.1 and 20.06.2","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#ask-user-about-teamcity-configuration","text":"","title":"Ask user about TeamCity configuration"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_6","text":"User was asked about agents count and configuration","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_5","text":"All agents are docker's containers so they are always the same and the problem isn't here. User have 2 types of tests Using MockWebServer and have ~140 tests executed on multiple shards Using real backend, not many tests executed on single shards - this type of tests sometime falling When user back to old configuration, he can still reproduce problem Between versions user changed config from: num-uniform-shards: 1 to num-test-runs: 1 But when user back to old configuration, he can still reproduce problem.","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#current-situation","text":"","title":"Current situation"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#issue-reported-to-teamcity-httpsyoutrackjetbrainscomissuetw-66753","text":"TeamCity team cannot reproduce the problem, thay asked about additional logs, but User could deliver them in next week (starting 13-07-2020), until that we must wait.","title":"Issue reported to TeamCity https://youtrack.jetbrains.com/issue/TW-66753"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#user-is-testing-teamcity-ci-with-flank-8-to-make-sure-that-the-problem-is-related-to-flank","text":"","title":"User is testing TeamCity CI with Flank 8 to make sure that the problem is related to Flank"},{"location":"bugs/891-rate-limit-exceeded/","text":"Rate limit exceeded #891 Reported description: After bump from 20.05.2 to 20.06.2 I started to see some issues related to rate limit. Also, there are no significant changes on the shard size or test amount. We are running another 7 flank execution in parallel. Total ~6k tests. Stack trace java.io.IOException: Request failed at ftl.http.ExecuteWithRetryKt$executeWithRetry$$inlined$withRetry$1.invokeSuspend(ExecuteWithRetry.kt:36) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:270) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:79) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:54) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:36) at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source) at ftl.http.ExecuteWithRetryKt.executeWithRetry(ExecuteWithRetry.kt:41) at ftl.gc.GcToolResults.getStepResult(GcToolResults.kt:98) at ftl.json.SavedMatrix.finished(SavedMatrix.kt:90) at ftl.json.SavedMatrix.update(SavedMatrix.kt:65) at ftl.json.MatrixMapKt.update(MatrixMap.kt:47) at ftl.run.NewTestRunKt$newTestRun$2$2.invokeSuspend(NewTestRun.kt:24) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.ResumeModeKt.resumeUninterceptedMode(ResumeMode.kt:45) at kotlinx.coroutines.internal.ScopeCoroutine.afterCompletionInternal(Scopes.kt:32) at kotlinx.coroutines.JobSupport.completeStateFinalization(JobSupport.kt:310) at kotlinx.coroutines.JobSupport.tryFinalizeSimpleState(JobSupport.kt:276) at kotlinx.coroutines.JobSupport.tryMakeCompleting(JobSupport.kt:807) at kotlinx.coroutines.JobSupport.makeCompletingOnce$kotlinx_coroutines_core(JobSupport.kt:787) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:111) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.ResumeModeKt.resumeUninterceptedMode(ResumeMode.kt:45) at kotlinx.coroutines.internal.ScopeCoroutine.afterCompletionInternal(Scopes.kt:32) at kotlinx.coroutines.JobSupport.completeStateFinalization(JobSupport.kt:310) at kotlinx.coroutines.JobSupport.tryFinalizeFinishingState(JobSupport.kt:236) at kotlinx.coroutines.JobSupport.tryMakeCompletingSlowPath(JobSupport.kt:849) at kotlinx.coroutines.JobSupport.tryMakeCompleting(JobSupport.kt:811) at kotlinx.coroutines.JobSupport.makeCompletingOnce$kotlinx_coroutines_core(JobSupport.kt:787) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:111) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:270) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:79) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:54) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:36) at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source) at ftl.cli.firebase.test.android.AndroidRunCommand.run(AndroidRunCommand.kt:48) at picocli.CommandLine.executeUserObject(CommandLine.java:1769) at picocli.CommandLine.access$900(CommandLine.java:145) at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2150) at picocli.CommandLine$RunLast.handle(CommandLine.java:2144) at picocli.CommandLine$RunLast.handle(CommandLine.java:2108) at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:1975) at picocli.CommandLine.execute(CommandLine.java:1904) at ftl.Main$Companion$main$1.invoke(Main.kt:53) at ftl.Main$Companion$main$1.invoke(Main.kt:43) at ftl.util.Utils.withGlobalExceptionHandling(Utils.kt:130) at ftl.Main$Companion.main(Main.kt:49) at ftl.Main.main(Main.kt) C used by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests { \"code\" : 429, \"errors\" : [ { \"domain\" : \"global\", \"message\" : \"Quota exceeded for quota group 'default' and limit 'Queries per user per 100 seconds' of service 'toolresults.googleapis.com' for consumer 'project_number:x'.\", \"reason\" : \"rateLimitExceeded\" } ], \"message\" : \"Quota exceeded for quota group 'default' and limit 'Queries per user per 100 seconds' of service 'toolresults.googleapis.com' for consumer 'project_number:x'.\", \"status\" : \"RESOURCE_EXHAUSTED\" } at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150) at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:443) at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1092) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:541) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:474) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:591) at ftl.http.ExecuteWithRetryKt$executeWithRetry$$inlined$withRetry$1.invokeSuspend(ExecuteWithRetry.kt:41) ... 52 more Reported flank config flank : max-test-shards : 15 shard-time : 160 num-test-runs : 1 smart-flank-gcs-path : gs://x/unit-tests.xml smart-flank-disable-upload : false files-to-download : test-targets-always-run : disable-sharding : false project : x local-result-dir : results full-junit-result : true # Android Flank Yml keep-file-path : false additional-app-test-apks : run-timeout : -1 legacy-junit-result : false ignore-failed-tests : false output-style : multi RunTests Smart Flank cache hit : 100% (88 / 88) Shard times : 93s, 93s Uploadingx-app-debug.apk . Uploading x-androidTest.apk . 88 tests / 2 shards To Reproduce shell script Flank.jar firebase test android run --num-flaky-test-attempts=0 --full-junit-result=true ## API usage outline Real examples simplified to pseudo code that outlines only API call usage #### Gcloud Case for disabled sharding * [MonitorTestExecutionProgress](https://github.com/Flank/gcloud_cli/blob/137d864acd5928baf25434cf59b0225c4d1f9319/google-cloud-sdk/lib/googlecloudsdk/api_lib/firebase/test/matrix_ops.py#L164) while (matrix status is not completed) { _client.projects_testMatrices.Get( TestingProjectsTestMatricesGetRequest(projectId, testMatrixId) ) wait(6s) } * [MonitorTestMatrixProgress](https://github.com/Flank/gcloud_cli/blob/137d864acd5928baf25434cf59b0225c4d1f9319/google-cloud-sdk/lib/googlecloudsdk/api_lib/firebase/test/matrix_ops.py#L248) Case for enabled sharding while (matrix status is not completed) { _client.projects_testMatrices.Get( TestingProjectsTestMatricesGetRequest(projectId, testMatrixId) ) wait(6s) } #### Flank v20.06.2 * [pollMatrices](https://github.com/Flank/flank/blob/6ee6939263923953edf67afa7218cf2c496c2ef2/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L19) forEach(test matrix) { async while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } * [SavedMatrix.finished](https://github.com/Flank/flank/blob/c88cb2786de67c0a114fc31a7b25917a035e145b/test_runner/src/main/kotlin/ftl/json/SavedMatrix.kt#L75) forEach(test matrix) { sync forEach(matrix test execution) { GcToolResults.listTestCases(toolResultsStep) GcToolResults.getStepResult(toolResultsStep) } sync forEach(matrix test execution) { GcToolResults.getExecutionResult(testExecution) GcToolResults.getStepResult(toolResultsStep) } } #### Flank v20.05.2 * [pollMatrices](https://github.com/Flank/flank/blob/accca3b941874e9556eea6616b34a9f4319c8746/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L15) // Only the first matrix is getting status updates // the others are blocked until the first is getting completed. // As a result, it reduces the amount of requests to 1 per 5 secs. // That is why this version of the flank is not getting limit exceeded. forEach(test matrix) { sync while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } #### Flank v20.06.2 * [pollMatrices](https://github.com/Flank/flank/blob/6ee6939263923953edf67afa7218cf2c496c2ef2/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L19) forEach(test matrix) { async while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } * [SavedMatrix.finished](https://github.com/Flank/flank/blob/c88cb2786de67c0a114fc31a7b25917a035e145b/test_runner/src/main/kotlin/ftl/json/SavedMatrix.kt#L75) forEach(test matrix) { sync forEach(matrix test execution) { GcToolResults.getStepResult(toolResultsStep) } } ## API calls usage comparision table Following table should compare API calls complexity. | | execution status updates | |:-------:|:------------------------:| | Gcloud | 1 * r / 6s + 1 | | 20.05.2 | 1 * r / 5s + (E * r) | | 20.06.2 | M * r / 5s + (M * E * 4 * r) | r - request s - second M - count of matrixes E - count of test executions in matrix scope ## Conclusions #### Gclud Because of single matrix run gives only 1 request per 6 seconds #### Flank v20.05.2 Gets matrix status updates in synchronize way so only the first matrix is getting status updates, the others are blocked until the first is getting completed, so the amount of requests is reduced to 1 per 5 secs. Plus additional number of requests for each execution in one matrix scope. That is why this version of the flank is not getting a limit exceeded. #### Flank v20.06.2 Is polling results asynchronously for each matrix. At last, it is doing a request for each test execution for each matrix, and this is the place where flank is getting a rate limit exceeded. It is visible on stack trace. at ftl.gc.GcToolResults.getStepResult(GcToolResults.kt:98) at ftl.json.SavedMatrix.finished(SavedMatrix.kt:90)","title":"Rate limit exceeded [#891](https://github.com/Flank/flank/issues/891)"},{"location":"bugs/891-rate-limit-exceeded/#rate-limit-exceeded-891","text":"Reported description: After bump from 20.05.2 to 20.06.2 I started to see some issues related to rate limit. Also, there are no significant changes on the shard size or test amount. We are running another 7 flank execution in parallel. Total ~6k tests.","title":"Rate limit exceeded #891"},{"location":"bugs/891-rate-limit-exceeded/#stack-trace","text":"java.io.IOException: Request failed at ftl.http.ExecuteWithRetryKt$executeWithRetry$$inlined$withRetry$1.invokeSuspend(ExecuteWithRetry.kt:36) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:270) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:79) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:54) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:36) at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source) at ftl.http.ExecuteWithRetryKt.executeWithRetry(ExecuteWithRetry.kt:41) at ftl.gc.GcToolResults.getStepResult(GcToolResults.kt:98) at ftl.json.SavedMatrix.finished(SavedMatrix.kt:90) at ftl.json.SavedMatrix.update(SavedMatrix.kt:65) at ftl.json.MatrixMapKt.update(MatrixMap.kt:47) at ftl.run.NewTestRunKt$newTestRun$2$2.invokeSuspend(NewTestRun.kt:24) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.ResumeModeKt.resumeUninterceptedMode(ResumeMode.kt:45) at kotlinx.coroutines.internal.ScopeCoroutine.afterCompletionInternal(Scopes.kt:32) at kotlinx.coroutines.JobSupport.completeStateFinalization(JobSupport.kt:310) at kotlinx.coroutines.JobSupport.tryFinalizeSimpleState(JobSupport.kt:276) at kotlinx.coroutines.JobSupport.tryMakeCompleting(JobSupport.kt:807) at kotlinx.coroutines.JobSupport.makeCompletingOnce$kotlinx_coroutines_core(JobSupport.kt:787) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:111) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.ResumeModeKt.resumeUninterceptedMode(ResumeMode.kt:45) at kotlinx.coroutines.internal.ScopeCoroutine.afterCompletionInternal(Scopes.kt:32) at kotlinx.coroutines.JobSupport.completeStateFinalization(JobSupport.kt:310) at kotlinx.coroutines.JobSupport.tryFinalizeFinishingState(JobSupport.kt:236) at kotlinx.coroutines.JobSupport.tryMakeCompletingSlowPath(JobSupport.kt:849) at kotlinx.coroutines.JobSupport.tryMakeCompleting(JobSupport.kt:811) at kotlinx.coroutines.JobSupport.makeCompletingOnce$kotlinx_coroutines_core(JobSupport.kt:787) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:111) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:270) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:79) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:54) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:36) at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source) at ftl.cli.firebase.test.android.AndroidRunCommand.run(AndroidRunCommand.kt:48) at picocli.CommandLine.executeUserObject(CommandLine.java:1769) at picocli.CommandLine.access$900(CommandLine.java:145) at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2150) at picocli.CommandLine$RunLast.handle(CommandLine.java:2144) at picocli.CommandLine$RunLast.handle(CommandLine.java:2108) at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:1975) at picocli.CommandLine.execute(CommandLine.java:1904) at ftl.Main$Companion$main$1.invoke(Main.kt:53) at ftl.Main$Companion$main$1.invoke(Main.kt:43) at ftl.util.Utils.withGlobalExceptionHandling(Utils.kt:130) at ftl.Main$Companion.main(Main.kt:49) at ftl.Main.main(Main.kt) C used by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests { \"code\" : 429, \"errors\" : [ { \"domain\" : \"global\", \"message\" : \"Quota exceeded for quota group 'default' and limit 'Queries per user per 100 seconds' of service 'toolresults.googleapis.com' for consumer 'project_number:x'.\", \"reason\" : \"rateLimitExceeded\" } ], \"message\" : \"Quota exceeded for quota group 'default' and limit 'Queries per user per 100 seconds' of service 'toolresults.googleapis.com' for consumer 'project_number:x'.\", \"status\" : \"RESOURCE_EXHAUSTED\" } at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150) at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:443) at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1092) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:541) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:474) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:591) at ftl.http.ExecuteWithRetryKt$executeWithRetry$$inlined$withRetry$1.invokeSuspend(ExecuteWithRetry.kt:41) ... 52 more","title":"Stack trace"},{"location":"bugs/891-rate-limit-exceeded/#reported-flank-config","text":"flank : max-test-shards : 15 shard-time : 160 num-test-runs : 1 smart-flank-gcs-path : gs://x/unit-tests.xml smart-flank-disable-upload : false files-to-download : test-targets-always-run : disable-sharding : false project : x local-result-dir : results full-junit-result : true # Android Flank Yml keep-file-path : false additional-app-test-apks : run-timeout : -1 legacy-junit-result : false ignore-failed-tests : false output-style : multi RunTests Smart Flank cache hit : 100% (88 / 88) Shard times : 93s, 93s Uploadingx-app-debug.apk . Uploading x-androidTest.apk . 88 tests / 2 shards","title":"Reported flank config"},{"location":"bugs/891-rate-limit-exceeded/#to-reproduce","text":"shell script Flank.jar firebase test android run --num-flaky-test-attempts=0 --full-junit-result=true ## API usage outline Real examples simplified to pseudo code that outlines only API call usage #### Gcloud Case for disabled sharding * [MonitorTestExecutionProgress](https://github.com/Flank/gcloud_cli/blob/137d864acd5928baf25434cf59b0225c4d1f9319/google-cloud-sdk/lib/googlecloudsdk/api_lib/firebase/test/matrix_ops.py#L164) while (matrix status is not completed) { _client.projects_testMatrices.Get( TestingProjectsTestMatricesGetRequest(projectId, testMatrixId) ) wait(6s) } * [MonitorTestMatrixProgress](https://github.com/Flank/gcloud_cli/blob/137d864acd5928baf25434cf59b0225c4d1f9319/google-cloud-sdk/lib/googlecloudsdk/api_lib/firebase/test/matrix_ops.py#L248) Case for enabled sharding while (matrix status is not completed) { _client.projects_testMatrices.Get( TestingProjectsTestMatricesGetRequest(projectId, testMatrixId) ) wait(6s) } #### Flank v20.06.2 * [pollMatrices](https://github.com/Flank/flank/blob/6ee6939263923953edf67afa7218cf2c496c2ef2/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L19) forEach(test matrix) { async while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } * [SavedMatrix.finished](https://github.com/Flank/flank/blob/c88cb2786de67c0a114fc31a7b25917a035e145b/test_runner/src/main/kotlin/ftl/json/SavedMatrix.kt#L75) forEach(test matrix) { sync forEach(matrix test execution) { GcToolResults.listTestCases(toolResultsStep) GcToolResults.getStepResult(toolResultsStep) } sync forEach(matrix test execution) { GcToolResults.getExecutionResult(testExecution) GcToolResults.getStepResult(toolResultsStep) } } #### Flank v20.05.2 * [pollMatrices](https://github.com/Flank/flank/blob/accca3b941874e9556eea6616b34a9f4319c8746/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L15) // Only the first matrix is getting status updates // the others are blocked until the first is getting completed. // As a result, it reduces the amount of requests to 1 per 5 secs. // That is why this version of the flank is not getting limit exceeded. forEach(test matrix) { sync while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } #### Flank v20.06.2 * [pollMatrices](https://github.com/Flank/flank/blob/6ee6939263923953edf67afa7218cf2c496c2ef2/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L19) forEach(test matrix) { async while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } * [SavedMatrix.finished](https://github.com/Flank/flank/blob/c88cb2786de67c0a114fc31a7b25917a035e145b/test_runner/src/main/kotlin/ftl/json/SavedMatrix.kt#L75) forEach(test matrix) { sync forEach(matrix test execution) { GcToolResults.getStepResult(toolResultsStep) } } ## API calls usage comparision table Following table should compare API calls complexity. | | execution status updates | |:-------:|:------------------------:| | Gcloud | 1 * r / 6s + 1 | | 20.05.2 | 1 * r / 5s + (E * r) | | 20.06.2 | M * r / 5s + (M * E * 4 * r) | r - request s - second M - count of matrixes E - count of test executions in matrix scope ## Conclusions #### Gclud Because of single matrix run gives only 1 request per 6 seconds #### Flank v20.05.2 Gets matrix status updates in synchronize way so only the first matrix is getting status updates, the others are blocked until the first is getting completed, so the amount of requests is reduced to 1 per 5 secs. Plus additional number of requests for each execution in one matrix scope. That is why this version of the flank is not getting a limit exceeded. #### Flank v20.06.2 Is polling results asynchronously for each matrix. At last, it is doing a request for each test execution for each matrix, and this is the place where flank is getting a rate limit exceeded. It is visible on stack trace. at ftl.gc.GcToolResults.getStepResult(GcToolResults.kt:98) at ftl.json.SavedMatrix.finished(SavedMatrix.kt:90)","title":"To Reproduce"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/","text":"Outcome incorrectly set by Flank #914 Changelog Date Who? Action 31th July 2020 pawelpasterz created 1st October 2020 adamfilipow92 update Description [task 2020-07-23T20:29:15.152Z] \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 [task 2020-07-23T20:29:15.153Z] \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST DETAILS \u2502 [task 2020-07-23T20:29:15.153Z] \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 [task 2020-07-23T20:29:15.153Z] \u2502 flaky \u2502 matrix-1vj2pt9wih182 \u2502 1 test cases failed, 106 passed, 2 flaky \u2502 [task 2020-07-23T20:29:15.153Z] \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 FTL confirmed the matrix outcome is a failure. Flank reported flaky. Steps to reproduce So far no reliable way to reproduce it (with 10/10 rate) found. One can simulate error with the test: SavedMatrixTest#`savedMatrix should have failed outcome when at least one test is failed and the last one is flaky` Due to the incorrectly implemented logic regarding the status update, there were cases when flaky tests obscure failed results. The following must occur: 1. The matrix needs to have both flaky and failed tests (at least one of each) 2. TestExecutions need to be returned in a specific order: 1. Failed execution must be consumed by flank before flaky one 2. The following flaky execution must be actually failed but the outcome of all re-runs is success 3. Then flank does what is the following: 1. The outcome is set to failure when failed test/shards/executions are processed kotlin private fun Outcome?.updateOutcome( when { ... else -> this?.summary // at this point SavedMatrix.outcome == success, flank changes it to failure ... }) 2. When flank reaches case where step summary is failure but execution is success it sets SavedMatrix outcome to flaky kotlin updateOutcome(flakyOutcome = it.step.outcome?.summary != this?.summary) // flakyOutcome == true 3. Due to incorrect order in when condition failure check is never reached kotlin private fun Outcome?.updateOutcome( flakyOutcome: Boolean ) { outcome = when { flakyOutcome -> flaky // flank should escape here with failure status persisted, but since flakyOutcome == true SavedMatrix.outcome is changed to flaky outcome == failure || outcome == inconclusive -> return outcome == flaky -> this?.summary?.takeIf { it == failure || it == inconclusive } else -> this?.summary } ?: outcome } Proposed fix Change order in when condition to always check for failure first kotlin private fun Outcome?.updateOutcome( flakyOutcome: Boolean ) { outcome = when { outcome == failure || outcome == inconclusive -> return // escape when failure/inconclusive outcome is set flakyOutcome -> flaky outcome == flaky -> this?.summary?.takeIf { it == failure || it == inconclusive } else -> this?.summary } ?: outcome } The test below reflects potential but rare behavior. ``kotlin @Test fun savedMatrix should have flaky outcome when at least one test is flaky`() { val expectedOutcome = \"flaky\" val successStepExecution = createStepExecution(1) // success // https://github.com/Flank/flank/issues/918 // This test covers edge case where summary for both step and execution is null and outcome of // saved matrix was not changed and is set to success val malformed = createStepExecution(stepId = -666, executionId = -666) // flaky // below order in the list matters! val executions = listOf( successStepExecution, successStepExecution, malformed ) val testMatrix = testMatrix().apply { testMatrixId = \"123\" state = FINISHED resultStorage = createResultsStorage() testExecutions = executions } val savedMatrix = createSavedMatrix(testMatrix) assertEquals(expectedOutcome, savedMatrix.outcome) } ``` This issue is not deterministic and really difficult to reproduce by manual tests. The community not reporting this issue for some time.","title":"Outcome incorrectly set by Flank [#914](https://github.com/Flank/flank/issues/914)"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#outcome-incorrectly-set-by-flank-914","text":"","title":"Outcome incorrectly set by Flank #914"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#changelog","text":"Date Who? Action 31th July 2020 pawelpasterz created 1st October 2020 adamfilipow92 update","title":"Changelog"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#description","text":"[task 2020-07-23T20:29:15.152Z] \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 [task 2020-07-23T20:29:15.153Z] \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST DETAILS \u2502 [task 2020-07-23T20:29:15.153Z] \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 [task 2020-07-23T20:29:15.153Z] \u2502 flaky \u2502 matrix-1vj2pt9wih182 \u2502 1 test cases failed, 106 passed, 2 flaky \u2502 [task 2020-07-23T20:29:15.153Z] \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 FTL confirmed the matrix outcome is a failure. Flank reported flaky.","title":"Description"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#steps-to-reproduce","text":"So far no reliable way to reproduce it (with 10/10 rate) found. One can simulate error with the test: SavedMatrixTest#`savedMatrix should have failed outcome when at least one test is failed and the last one is flaky` Due to the incorrectly implemented logic regarding the status update, there were cases when flaky tests obscure failed results. The following must occur: 1. The matrix needs to have both flaky and failed tests (at least one of each) 2. TestExecutions need to be returned in a specific order: 1. Failed execution must be consumed by flank before flaky one 2. The following flaky execution must be actually failed but the outcome of all re-runs is success 3. Then flank does what is the following: 1. The outcome is set to failure when failed test/shards/executions are processed kotlin private fun Outcome?.updateOutcome( when { ... else -> this?.summary // at this point SavedMatrix.outcome == success, flank changes it to failure ... }) 2. When flank reaches case where step summary is failure but execution is success it sets SavedMatrix outcome to flaky kotlin updateOutcome(flakyOutcome = it.step.outcome?.summary != this?.summary) // flakyOutcome == true 3. Due to incorrect order in when condition failure check is never reached kotlin private fun Outcome?.updateOutcome( flakyOutcome: Boolean ) { outcome = when { flakyOutcome -> flaky // flank should escape here with failure status persisted, but since flakyOutcome == true SavedMatrix.outcome is changed to flaky outcome == failure || outcome == inconclusive -> return outcome == flaky -> this?.summary?.takeIf { it == failure || it == inconclusive } else -> this?.summary } ?: outcome }","title":"Steps to reproduce"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#proposed-fix","text":"Change order in when condition to always check for failure first kotlin private fun Outcome?.updateOutcome( flakyOutcome: Boolean ) { outcome = when { outcome == failure || outcome == inconclusive -> return // escape when failure/inconclusive outcome is set flakyOutcome -> flaky outcome == flaky -> this?.summary?.takeIf { it == failure || it == inconclusive } else -> this?.summary } ?: outcome } The test below reflects potential but rare behavior. ``kotlin @Test fun savedMatrix should have flaky outcome when at least one test is flaky`() { val expectedOutcome = \"flaky\" val successStepExecution = createStepExecution(1) // success // https://github.com/Flank/flank/issues/918 // This test covers edge case where summary for both step and execution is null and outcome of // saved matrix was not changed and is set to success val malformed = createStepExecution(stepId = -666, executionId = -666) // flaky // below order in the list matters! val executions = listOf( successStepExecution, successStepExecution, malformed ) val testMatrix = testMatrix().apply { testMatrixId = \"123\" state = FINISHED resultStorage = createResultsStorage() testExecutions = executions } val savedMatrix = createSavedMatrix(testMatrix) assertEquals(expectedOutcome, savedMatrix.outcome) } ``` This issue is not deterministic and really difficult to reproduce by manual tests. The community not reporting this issue for some time.","title":"Proposed fix"},{"location":"bugs/986_test_count_and_smart_sharding_%20count_dont_match/","text":"Test count and smart sharding count don't match Bug reported in this issue The problem Flank does not support parameterized tests sharding. Every class with parameterized is considered as one test during shard calculation. Flank is using DEX parser to decompile apks and gather info about all the tests inside. As for now, Flank is unable to determine how many times a test in a parameterized class is invoked. Due to this fact scans apks for any class with an annotation that contains JUnitParamsRunner or Parameterized : @RunWith ( JUnitParamsRunner :: class ) ... @RunWith ( Parameterized :: class ) Solution Flank knows how many tests and classes are being sent to Firebase. So we can inform the user of how many classes we have. Example: Smart Flank cache hit: 0% (0 / 9) Shard times: 240s, 240s, 240s, 360s Uploading app-debug.apk . Uploading app-multiple-flaky-debug-androidTest.apk . 5 tests + 4 parameterized classes / 4 shards Default test time for classes should be different from the default time for test You can set default test time for class with --default-class-test-time command If you did not set this time, the default value is 240s","title":"Test count and smart sharding count don't match"},{"location":"bugs/986_test_count_and_smart_sharding_%20count_dont_match/#test-count-and-smart-sharding-count-dont-match","text":"Bug reported in this issue","title":"Test count and smart sharding count don't match"},{"location":"bugs/986_test_count_and_smart_sharding_%20count_dont_match/#the-problem","text":"Flank does not support parameterized tests sharding. Every class with parameterized is considered as one test during shard calculation. Flank is using DEX parser to decompile apks and gather info about all the tests inside. As for now, Flank is unable to determine how many times a test in a parameterized class is invoked. Due to this fact scans apks for any class with an annotation that contains JUnitParamsRunner or Parameterized : @RunWith ( JUnitParamsRunner :: class ) ... @RunWith ( Parameterized :: class )","title":"The problem"},{"location":"bugs/986_test_count_and_smart_sharding_%20count_dont_match/#solution","text":"Flank knows how many tests and classes are being sent to Firebase. So we can inform the user of how many classes we have. Example: Smart Flank cache hit: 0% (0 / 9) Shard times: 240s, 240s, 240s, 360s Uploading app-debug.apk . Uploading app-multiple-flaky-debug-androidTest.apk . 5 tests + 4 parameterized classes / 4 shards Default test time for classes should be different from the default time for test You can set default test time for class with --default-class-test-time command If you did not set this time, the default value is 240s","title":"Solution"},{"location":"bugs/993_multiple_identical_lines_printing/","text":"Avoid multiple identical lines printing Related to #993 Sometimes Flank prints identical lines multiple times. This bug occurs rarely, there is no clear way to reproduce or force this by code changes. What was checked: [X] launched flank with a different configuration, different count of matrices [X] on PollMatrices.kt onEach { printMatrixStatus ( it ) } It always executes on the same thread so it is not a concurrency issue [X] on ExecutionStatusPrinter.kt -> MultiLinePrinter try to force remove less lines than output.size but in this case, this line does not update status, so the behaviour is different than on screen [X] on ExecutionStatusPrinter.kt -> MultiLinePrinter try to force add to output two same ExecutionStatus but no effect [X] on GcTestMatrix.kt -> refresh() try to add testSpecyfication with same id but without effect","title":"Avoid multiple identical lines printing"},{"location":"bugs/993_multiple_identical_lines_printing/#avoid-multiple-identical-lines-printing","text":"Related to #993 Sometimes Flank prints identical lines multiple times. This bug occurs rarely, there is no clear way to reproduce or force this by code changes. What was checked: [X] launched flank with a different configuration, different count of matrices [X] on PollMatrices.kt onEach { printMatrixStatus ( it ) } It always executes on the same thread so it is not a concurrency issue [X] on ExecutionStatusPrinter.kt -> MultiLinePrinter try to force remove less lines than output.size but in this case, this line does not update status, so the behaviour is different than on screen [X] on ExecutionStatusPrinter.kt -> MultiLinePrinter try to force add to output two same ExecutionStatus but no effect [X] on GcTestMatrix.kt -> refresh() try to add testSpecyfication with same id but without effect","title":"Avoid multiple identical lines printing"},{"location":"bugs/deviceUsageDuration_always_null/","text":"deviceUsageDuration always null deviceUsageDuration description How much the device resource is used to perform the test. This is the device usage used for billing purpose, which is different from the run_duration, for example, infrastructure failure won't be charged for device usage. PRECONDITION_FAILED will be returned if one attempts to set a device_usage on a step which already has this field set. In response: present if previously set. - In create request: optional - In update request: optional @return value or {@code null} for none Problem description Problem found on pull request: Flank needs to respect the timeout value as that's a cap for billing purposes. #865 deviceUsageDuration still is null even if we testing on blaze plan with free quota spent Next steps In future we should check problem status and if problem is fixed on testlab we should implement it on Flank","title":"deviceUsageDuration always null"},{"location":"bugs/deviceUsageDuration_always_null/#deviceusageduration-always-null","text":"","title":"deviceUsageDuration always null"},{"location":"bugs/deviceUsageDuration_always_null/#deviceusageduration-description","text":"How much the device resource is used to perform the test. This is the device usage used for billing purpose, which is different from the run_duration, for example, infrastructure failure won't be charged for device usage. PRECONDITION_FAILED will be returned if one attempts to set a device_usage on a step which already has this field set. In response: present if previously set. - In create request: optional - In update request: optional @return value or {@code null} for none","title":"deviceUsageDuration description"},{"location":"bugs/deviceUsageDuration_always_null/#problem-description","text":"Problem found on pull request: Flank needs to respect the timeout value as that's a cap for billing purposes. #865 deviceUsageDuration still is null even if we testing on blaze plan with free quota spent","title":"Problem description"},{"location":"bugs/deviceUsageDuration_always_null/#next-steps","text":"In future we should check problem status and if problem is fixed on testlab we should implement it on Flank","title":"Next steps"},{"location":"bugs/null_no_tests_found/","text":"The problem One or more shards failed because there are no test cases inside. This problem encounter a couple times on multi-module tests which use a lot of additional test apks. It was noticed only with the submodule tests. Stack trace java.lang.ClassNotFoundException: Invalid name: no tests found at java.lang.Class.classForName(Native Method) at java.lang.Class.forName(Class.java:324) at androidx.test.internal.runner.TestLoader.doCreateRunner(TestLoader.java:72) at androidx.test.internal.runner.TestLoader.getRunnersFor(TestLoader.java:104) at androidx.test.internal.runner.TestRequestBuilder.build(TestRequestBuilder.java:793) at androidx.test.runner.AndroidJUnitRunner.buildRequest(AndroidJUnitRunner.java:547) at androidx.test.runner.AndroidJUnitRunner.onStart(AndroidJUnitRunner.java:390) at com.work.android.test.view.ViewTestRunner.onStart(ViewTestRunner.kt:25) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1879) Firebase screens References github issue https://github.com/Flank/flank/issues/818 reported for Flank v20.05.2 Reported flank config flank : additional-app-test-apks : - test : feature/1/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/2/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/3/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/4/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/5/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/6/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/7/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/8/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/9/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/10/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/11/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/12/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/13/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/14/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/15/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/16/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/17/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/18/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/19/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/20/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/21/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/22/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/23/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/24/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/25/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/26/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/27/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk legacy-junit-result : 'false' local-result-dir : sub-modules-flank-results max-test-shards : 5 project : project-id-12345 run-timeout : 20m smart-flank-gcs-path : gs://project-id-12345-flank/submodules-timing.xml gcloud : app : ./sample-app/build/outputs/apk/debug/sample-app-tester-debug.apk async : false auto-google-login : false device : - model : NexusLowRes version : 23 environment-variables : clearPackageData : true performance-metrics : false record-video : false results-bucket : sub-modules-flank-results test : feature/28/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk test-targets : - notAnnotation org.junit.Ignore use-orchestrator : true Reported flade config fladle { serviceAccountCredentials = file ( \"pathToCredentials\" ) useOrchestrator = true environmentVariables = [ \"clearPackageData\" : \"true\" ] timeoutMin = 30 recordVideo = true performanceMetrics = false devices = [ [ \"model\" : \"Pixel2\" , \"version\" : \"27\" , \"orientation\" : \"portrait\" , \"locale\" : \"en\" ] ] projectId = \"id\" flankVersion = \"20.05.2\" testShards = 5 flakyTestAttempts = 0 } What we know from report issue Firebase shows error null no tests found Same error occurs with and without test-targets: - notAnnotation org.junit.Ignore With test-targets: - notAnnotation org.junit.Ignore firebase additionally shows One or more shards failed because there are no test cases inside. Please check your sharding configuration. Occurs with custom annotation-based filter. It was noticed only with the multi-module tests. If number of shards > 2 , there is a high chance one of the shards will have no tests. test-targets-always-run is not the case. Duplicated apk names probably are not the case, but we should ensure. How to reproduce it Use additional-test-app-apk option and set different apks with same names from different directories. Apks will overlap on bucket because of names collision. This should give similar result to reported. Proposals ~~Drop ignored tests before shard calculation and use them only for results. https://github.com/Flank/flank/pull/853~~ ~~Apks uploaded to bucket could overlap if has same names, fixing this could help. https://github.com/Flank/flank/pull/854~~ Create multi-module project which will provide many test apks and try to reproduce issue. Ensure that our knowledge about issue in What we know from report issue is correct. Based on: https://stackoverflow.com/questions/39241640/android-instrumented-test-no-tests-found try to make real multi dex app and try trun tests on it Play with @BefeoreClass and @Before annotations (exception on @BeforeClass produce null without test cases on test-lab).","title":"The problem"},{"location":"bugs/null_no_tests_found/#the-problem","text":"One or more shards failed because there are no test cases inside. This problem encounter a couple times on multi-module tests which use a lot of additional test apks. It was noticed only with the submodule tests.","title":"The problem"},{"location":"bugs/null_no_tests_found/#stack-trace","text":"java.lang.ClassNotFoundException: Invalid name: no tests found at java.lang.Class.classForName(Native Method) at java.lang.Class.forName(Class.java:324) at androidx.test.internal.runner.TestLoader.doCreateRunner(TestLoader.java:72) at androidx.test.internal.runner.TestLoader.getRunnersFor(TestLoader.java:104) at androidx.test.internal.runner.TestRequestBuilder.build(TestRequestBuilder.java:793) at androidx.test.runner.AndroidJUnitRunner.buildRequest(AndroidJUnitRunner.java:547) at androidx.test.runner.AndroidJUnitRunner.onStart(AndroidJUnitRunner.java:390) at com.work.android.test.view.ViewTestRunner.onStart(ViewTestRunner.kt:25) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1879)","title":"Stack trace"},{"location":"bugs/null_no_tests_found/#firebase-screens","text":"","title":"Firebase screens"},{"location":"bugs/null_no_tests_found/#references","text":"github issue https://github.com/Flank/flank/issues/818 reported for Flank v20.05.2","title":"References"},{"location":"bugs/null_no_tests_found/#reported-flank-config","text":"flank : additional-app-test-apks : - test : feature/1/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/2/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/3/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/4/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/5/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/6/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/7/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/8/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/9/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/10/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/11/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/12/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/13/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/14/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/15/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/16/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/17/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/18/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/19/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/20/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/21/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/22/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/23/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/24/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/25/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/26/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/27/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk legacy-junit-result : 'false' local-result-dir : sub-modules-flank-results max-test-shards : 5 project : project-id-12345 run-timeout : 20m smart-flank-gcs-path : gs://project-id-12345-flank/submodules-timing.xml gcloud : app : ./sample-app/build/outputs/apk/debug/sample-app-tester-debug.apk async : false auto-google-login : false device : - model : NexusLowRes version : 23 environment-variables : clearPackageData : true performance-metrics : false record-video : false results-bucket : sub-modules-flank-results test : feature/28/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk test-targets : - notAnnotation org.junit.Ignore use-orchestrator : true","title":"Reported flank config"},{"location":"bugs/null_no_tests_found/#reported-flade-config","text":"fladle { serviceAccountCredentials = file ( \"pathToCredentials\" ) useOrchestrator = true environmentVariables = [ \"clearPackageData\" : \"true\" ] timeoutMin = 30 recordVideo = true performanceMetrics = false devices = [ [ \"model\" : \"Pixel2\" , \"version\" : \"27\" , \"orientation\" : \"portrait\" , \"locale\" : \"en\" ] ] projectId = \"id\" flankVersion = \"20.05.2\" testShards = 5 flakyTestAttempts = 0 }","title":"Reported flade config"},{"location":"bugs/null_no_tests_found/#what-we-know-from-report-issue","text":"Firebase shows error null no tests found Same error occurs with and without test-targets: - notAnnotation org.junit.Ignore With test-targets: - notAnnotation org.junit.Ignore firebase additionally shows One or more shards failed because there are no test cases inside. Please check your sharding configuration. Occurs with custom annotation-based filter. It was noticed only with the multi-module tests. If number of shards > 2 , there is a high chance one of the shards will have no tests. test-targets-always-run is not the case. Duplicated apk names probably are not the case, but we should ensure.","title":"What we know from report issue"},{"location":"bugs/null_no_tests_found/#how-to-reproduce-it","text":"Use additional-test-app-apk option and set different apks with same names from different directories. Apks will overlap on bucket because of names collision. This should give similar result to reported.","title":"How to reproduce it"},{"location":"bugs/null_no_tests_found/#proposals","text":"~~Drop ignored tests before shard calculation and use them only for results. https://github.com/Flank/flank/pull/853~~ ~~Apks uploaded to bucket could overlap if has same names, fixing this could help. https://github.com/Flank/flank/pull/854~~ Create multi-module project which will provide many test apks and try to reproduce issue. Ensure that our knowledge about issue in What we know from report issue is correct. Based on: https://stackoverflow.com/questions/39241640/android-instrumented-test-no-tests-found try to make real multi dex app and try trun tests on it Play with @BefeoreClass and @Before annotations (exception on @BeforeClass produce null without test cases on test-lab).","title":"Proposals"},{"location":"bugs/smart_flank_upload_results_rules/","text":"Smart Flank rules of validation result types Flank trying to avoid override smart-flank-gcs-path by different JUnit report type. That's means: If user select in smart-flank-gcs-path command FulJUnitResult.xml and flag --full-junit-result is not set Flank fail with the message smart-flank-gcs-path is set with FullJUnitReport.xml but in this run --full-junit-result is disabled, please set --full-junit-result flag If user set in smart-flank-gcs-path command JUnitResult.xml and flag --full-junit-result is set Flank fails with message smart-flank-gcs-path is set with JUnitReport.xml but in this run --full-junit-result enabled, please turn off --full-junit-result flag If smart-flank-gcs-path is set to a different name than JUnitReport.xml and FullJUnitReport.xml flank not validating report type Flank not validating report type if flag --smart-flank-disable-upload set","title":"Smart Flank rules of validation result types"},{"location":"bugs/smart_flank_upload_results_rules/#smart-flank-rules-of-validation-result-types","text":"Flank trying to avoid override smart-flank-gcs-path by different JUnit report type. That's means: If user select in smart-flank-gcs-path command FulJUnitResult.xml and flag --full-junit-result is not set Flank fail with the message smart-flank-gcs-path is set with FullJUnitReport.xml but in this run --full-junit-result is disabled, please set --full-junit-result flag If user set in smart-flank-gcs-path command JUnitResult.xml and flag --full-junit-result is set Flank fails with message smart-flank-gcs-path is set with JUnitReport.xml but in this run --full-junit-result enabled, please turn off --full-junit-result flag If smart-flank-gcs-path is set to a different name than JUnitReport.xml and FullJUnitReport.xml flank not validating report type Flank not validating report type if flag --smart-flank-disable-upload set","title":"Smart Flank rules of validation result types"},{"location":"desktop/flank_desktop/","text":"Flank desktop prototype Prototype Flank options The prototype will run a simple Android test on a given apk and test the apk with options to specify some flanks flags, as well as max tests shards. Those flags are the most popular one based on mixpanel analytics Flags disable sharding disable results upload fail fast disable usage statistics auto Google login Input Max tests shards apk path test apk path Prototype design The mockup of the design is shown below: Design elements At the top left side there are some small buttons to toggle state of boolean options Below there is text field to input maxTestsShards Below there are two text fields with buttons placed at the right to open file chooser/provide a path to files At the very bottom of left column there is a button to start Flank test run. On the right side there is a big window which is responsible for displaying output Setup Please follow getting started guide to set up new module for a prototype. There is also a gradle plugin for simplifying usage of Jetbrains compose. More info on project page . Additional resources IntelliJ Idea plugin Tutorial Latest build Template to use without gradle","title":"Flank desktop prototype"},{"location":"desktop/flank_desktop/#flank-desktop-prototype","text":"","title":"Flank desktop prototype"},{"location":"desktop/flank_desktop/#prototype-flank-options","text":"The prototype will run a simple Android test on a given apk and test the apk with options to specify some flanks flags, as well as max tests shards. Those flags are the most popular one based on mixpanel analytics Flags disable sharding disable results upload fail fast disable usage statistics auto Google login Input Max tests shards apk path test apk path","title":"Prototype Flank options"},{"location":"desktop/flank_desktop/#prototype-design","text":"The mockup of the design is shown below:","title":"Prototype design"},{"location":"desktop/flank_desktop/#design-elements","text":"At the top left side there are some small buttons to toggle state of boolean options Below there is text field to input maxTestsShards Below there are two text fields with buttons placed at the right to open file chooser/provide a path to files At the very bottom of left column there is a button to start Flank test run. On the right side there is a big window which is responsible for displaying output","title":"Design elements"},{"location":"desktop/flank_desktop/#setup","text":"Please follow getting started guide to set up new module for a prototype. There is also a gradle plugin for simplifying usage of Jetbrains compose. More info on project page .","title":"Setup"},{"location":"desktop/flank_desktop/#additional-resources","text":"IntelliJ Idea plugin Tutorial Latest build Template to use without gradle","title":"Additional resources"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/","text":"Auto discover rest of tests when using test-target-for-shards option Currently, when using test-target-for-shards you need to specify all tests manually to split to shards. If you do not do that then only listed tests are executed. It will be good to provide an option to auto-discover other than specified tests to make test-target-for-shards better and easier to use. References Conversation on Slack 1532 Motivation test-target-for-shards is not so easy to use, this change will improve it when using Flank and give users more flexibility and control over creating shards. Goals New option is available on the flank configuration level User could specify which packages/class/tests run on specific shards by using a new option Other tests that are not specified are then automatically added to a different set of shards/shard Design Add new option under Flank configuration, possible ideas are: plan-sharding , tests-for-shards , smart-test-targets-for-shards . New option should at least behave the same as test-target-for-shards . The left over tests should then be sorted into other shards or a single shard based upon the configuration provided by the user. Flank should fast fail if disable-sharding is set to true or max-test-shards is lower than specified test shards by user + 1. API It is hard to plan for this change to the API because by the time this is worked upon the codebase may have changed significantly influencing the usage of this request. This option will be applied to many places in code. Please follow up Design section for the implementation plan. Results A new option will be created which will split sharding based on user input or an alternative flag (described in Alternative Considered ) which will change behavior of test-target-for-shards . Dependencies There are no dependencies that will have a impact on this task. It is recommended to do this after Flank's refactor to make it easier. Testing Add some tests(not all) as a value to a new option Verify that they are correctly split into shards and there is one more shard with rest of tests(not specified) Alternatives Considered Add a new flag to Flank configuration, which overrides test-target-for-shards and automatically add the rest of the tests to separate shard/shards(if max tests shards are greater than left count).","title":"Auto discover rest of tests when using test-target-for-shards option"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#auto-discover-rest-of-tests-when-using-test-target-for-shards-option","text":"Currently, when using test-target-for-shards you need to specify all tests manually to split to shards. If you do not do that then only listed tests are executed. It will be good to provide an option to auto-discover other than specified tests to make test-target-for-shards better and easier to use.","title":"Auto discover rest of tests when using test-target-for-shards option"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#references","text":"Conversation on Slack 1532","title":"References"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#motivation","text":"test-target-for-shards is not so easy to use, this change will improve it when using Flank and give users more flexibility and control over creating shards.","title":"Motivation"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#goals","text":"New option is available on the flank configuration level User could specify which packages/class/tests run on specific shards by using a new option Other tests that are not specified are then automatically added to a different set of shards/shard","title":"Goals"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#design","text":"Add new option under Flank configuration, possible ideas are: plan-sharding , tests-for-shards , smart-test-targets-for-shards . New option should at least behave the same as test-target-for-shards . The left over tests should then be sorted into other shards or a single shard based upon the configuration provided by the user. Flank should fast fail if disable-sharding is set to true or max-test-shards is lower than specified test shards by user + 1.","title":"Design"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#api","text":"It is hard to plan for this change to the API because by the time this is worked upon the codebase may have changed significantly influencing the usage of this request. This option will be applied to many places in code. Please follow up Design section for the implementation plan.","title":"API"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#results","text":"A new option will be created which will split sharding based on user input or an alternative flag (described in Alternative Considered ) which will change behavior of test-target-for-shards .","title":"Results"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#dependencies","text":"There are no dependencies that will have a impact on this task. It is recommended to do this after Flank's refactor to make it easier.","title":"Dependencies"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#testing","text":"Add some tests(not all) as a value to a new option Verify that they are correctly split into shards and there is one more shard with rest of tests(not specified)","title":"Testing"},{"location":"feature/1532-explore-test-target-for-shards-with-extra-features/#alternatives-considered","text":"Add a new flag to Flank configuration, which overrides test-target-for-shards and automatically add the rest of the tests to separate shard/shards(if max tests shards are greater than left count).","title":"Alternatives Considered"},{"location":"feature/1609-add-apk-name-to-result-matrix/","text":"Add apk name or module name to the result matrix Currently, the result matrix displays matrices ids only. When there are lots of additional test apks it's hard to distinguish which apk has failed. Having the apk file name associated with the results (matrices) will give a better overview and will allow a user to find a failing apk module without the need of jumping between result URLs. Current result table: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 failure \u2502 matrix-b1fizzmw53qka \u2502 NexusLowRes-29-de-portrait \u2502 82 test cases failed, 206 passed, 1 flaky \u2502 \u2502 failure \u2502 matrix-wq8upcrn5uoca \u2502 NexusLowRes-29-de-portrait \u2502 5 test cases failed, 4 passed \u2502 \u2502 failure \u2502 matrix-2n50loc2ze1h4 \u2502 NexusLowRes-29-de-portrait \u2502 1 test cases failed, 6 passed \u2502 \u2502 success \u2502 matrix-3eo91na6dzmd1 \u2502 NexusLowRes-29-de-portrait \u2502 8 test cases passed \u2502 \u2502 failure \u2502 matrix-1ztkep4prcq43 \u2502 NexusLowRes-29-de-portrait \u2502 1 test cases failed \u2502 \u2502 failure \u2502 matrix-w9wpp0jwt4yba \u2502 NexusLowRes-29-de-portrait \u2502 23 test cases failed, 6 passed \u2502 \u2502 success \u2502 matrix-30cqwb4sc1rqh \u2502 NexusLowRes-29-de-portrait \u2502 17 test cases passed \u2502 \u2502 success \u2502 matrix-32kzu6e7a1t3h \u2502 NexusLowRes-29-de-portrait \u2502 2 test cases passed \u2502 \u2502 failure \u2502 matrix-3dp8qqblpyav7 \u2502 NexusLowRes-29-de-portrait \u2502 20 test cases failed, 12 passed \u2502 \u2502 failure \u2502 matrix-zw52wsouk4zma \u2502 NexusLowRes-29-de-portrait \u2502 1 test cases failed \u2502 \u2502 INVALID \u2502 matrix-36qxkc2szybc6 \u2502 \u2502 NO_TEST_RUNNER_CLASS \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 References Issue #1609 Motivation Flank gives the possibility to run multiple apks in parallel within a single test run. This feature is extensively used by companies with big projects that include a multitude of additional modules. Unfortunately, processing results in the current form is neither easy nor straightforward. The user needs to associate the matrix with the related apk/modules which can be time-consuming and error-prone. To achieve a better result overview, flank should add the apk name to both the result table and the output JSON. Goals Add an associated apk file name to the result table and output JSON. Design Flank is not aware of any project, modules, etc so it can't find module names on its own, in a reasonable way, without breaking backward compatibility. On the other hand, finding the apk file name is simple and straightforward. To achieve it we need: 1. add new field app to the SavedMatrix data class 2. update private extension function SavedMatrix#updateProperties to extract apk file name from the newMatrix * find app used during tests under the TestMatrix -> TestSpecification * once we have TestSpecification , iterate over all specs (android/ios/instrumentation/etc.) and find non-null * since we can run only one spec within a matrix, only one (spec) is non-null * fetch appApk from spec (this is full gs:// path) TestSpecification#appApk * extract file name from gs:// path * logic should be converted to a separate function to preserve readability * Google API likes to return null - flank should handle it! * the same for empty file name (who knows what Google API will bring) 3. Update SavedMatrixTableUtil and add new column 4. Update OutputReportLoggers to log apk file name: * switch it.matrixId to it.app in: matrices . map { it . matrixId to it . testAxises // it.app }. toMap () * above is not resilient to the same name apks * instead, we can add a filed under matrix id in JSON \"matrix-1ealdqtvrtn5r\" : { \"app\" : \"my-super-app.apk\" , \"test-axises\" : [ \"results here\" ] } API No changes. Results A new table column APP will be added to the result table. Similar for output JSON. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 APP | TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 failure \u2502 matrix-b1fizzmw53qka \u2502 my-super-app.apk | NexusLowRes-29-de-portrait \u2502 82 test cases failed, 206 passed, 1 flaky \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Dependencies There are no dependencies that will have an impact on this task. Testing Unit test(s) should be added to verify fetching apk filename logic and its association with result matrix Integration tests should be updated to verify App column Alternatives Considered It would be possible to display a module name instead of an apk file name, it requires either: 1. additional option in yml for a user to provide module: * new option in IArgs * only manually 2. use Fladle to pass module information to flank * can be automatic * feature available for users who use fladle (not available for 'flank only users) * requires PR for both, fladle and flank * strong coupling between tools","title":"Add apk name or module name to the result matrix"},{"location":"feature/1609-add-apk-name-to-result-matrix/#add-apk-name-or-module-name-to-the-result-matrix","text":"Currently, the result matrix displays matrices ids only. When there are lots of additional test apks it's hard to distinguish which apk has failed. Having the apk file name associated with the results (matrices) will give a better overview and will allow a user to find a failing apk module without the need of jumping between result URLs. Current result table: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 failure \u2502 matrix-b1fizzmw53qka \u2502 NexusLowRes-29-de-portrait \u2502 82 test cases failed, 206 passed, 1 flaky \u2502 \u2502 failure \u2502 matrix-wq8upcrn5uoca \u2502 NexusLowRes-29-de-portrait \u2502 5 test cases failed, 4 passed \u2502 \u2502 failure \u2502 matrix-2n50loc2ze1h4 \u2502 NexusLowRes-29-de-portrait \u2502 1 test cases failed, 6 passed \u2502 \u2502 success \u2502 matrix-3eo91na6dzmd1 \u2502 NexusLowRes-29-de-portrait \u2502 8 test cases passed \u2502 \u2502 failure \u2502 matrix-1ztkep4prcq43 \u2502 NexusLowRes-29-de-portrait \u2502 1 test cases failed \u2502 \u2502 failure \u2502 matrix-w9wpp0jwt4yba \u2502 NexusLowRes-29-de-portrait \u2502 23 test cases failed, 6 passed \u2502 \u2502 success \u2502 matrix-30cqwb4sc1rqh \u2502 NexusLowRes-29-de-portrait \u2502 17 test cases passed \u2502 \u2502 success \u2502 matrix-32kzu6e7a1t3h \u2502 NexusLowRes-29-de-portrait \u2502 2 test cases passed \u2502 \u2502 failure \u2502 matrix-3dp8qqblpyav7 \u2502 NexusLowRes-29-de-portrait \u2502 20 test cases failed, 12 passed \u2502 \u2502 failure \u2502 matrix-zw52wsouk4zma \u2502 NexusLowRes-29-de-portrait \u2502 1 test cases failed \u2502 \u2502 INVALID \u2502 matrix-36qxkc2szybc6 \u2502 \u2502 NO_TEST_RUNNER_CLASS \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Add apk name or module name to the result matrix"},{"location":"feature/1609-add-apk-name-to-result-matrix/#references","text":"Issue #1609","title":"References"},{"location":"feature/1609-add-apk-name-to-result-matrix/#motivation","text":"Flank gives the possibility to run multiple apks in parallel within a single test run. This feature is extensively used by companies with big projects that include a multitude of additional modules. Unfortunately, processing results in the current form is neither easy nor straightforward. The user needs to associate the matrix with the related apk/modules which can be time-consuming and error-prone. To achieve a better result overview, flank should add the apk name to both the result table and the output JSON.","title":"Motivation"},{"location":"feature/1609-add-apk-name-to-result-matrix/#goals","text":"Add an associated apk file name to the result table and output JSON.","title":"Goals"},{"location":"feature/1609-add-apk-name-to-result-matrix/#design","text":"Flank is not aware of any project, modules, etc so it can't find module names on its own, in a reasonable way, without breaking backward compatibility. On the other hand, finding the apk file name is simple and straightforward. To achieve it we need: 1. add new field app to the SavedMatrix data class 2. update private extension function SavedMatrix#updateProperties to extract apk file name from the newMatrix * find app used during tests under the TestMatrix -> TestSpecification * once we have TestSpecification , iterate over all specs (android/ios/instrumentation/etc.) and find non-null * since we can run only one spec within a matrix, only one (spec) is non-null * fetch appApk from spec (this is full gs:// path) TestSpecification#appApk * extract file name from gs:// path * logic should be converted to a separate function to preserve readability * Google API likes to return null - flank should handle it! * the same for empty file name (who knows what Google API will bring) 3. Update SavedMatrixTableUtil and add new column 4. Update OutputReportLoggers to log apk file name: * switch it.matrixId to it.app in: matrices . map { it . matrixId to it . testAxises // it.app }. toMap () * above is not resilient to the same name apks * instead, we can add a filed under matrix id in JSON \"matrix-1ealdqtvrtn5r\" : { \"app\" : \"my-super-app.apk\" , \"test-axises\" : [ \"results here\" ] }","title":"Design"},{"location":"feature/1609-add-apk-name-to-result-matrix/#api","text":"No changes.","title":"API"},{"location":"feature/1609-add-apk-name-to-result-matrix/#results","text":"A new table column APP will be added to the result table. Similar for output JSON. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 APP | TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 failure \u2502 matrix-b1fizzmw53qka \u2502 my-super-app.apk | NexusLowRes-29-de-portrait \u2502 82 test cases failed, 206 passed, 1 flaky \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Results"},{"location":"feature/1609-add-apk-name-to-result-matrix/#dependencies","text":"There are no dependencies that will have an impact on this task.","title":"Dependencies"},{"location":"feature/1609-add-apk-name-to-result-matrix/#testing","text":"Unit test(s) should be added to verify fetching apk filename logic and its association with result matrix Integration tests should be updated to verify App column","title":"Testing"},{"location":"feature/1609-add-apk-name-to-result-matrix/#alternatives-considered","text":"It would be possible to display a module name instead of an apk file name, it requires either: 1. additional option in yml for a user to provide module: * new option in IArgs * only manually 2. use Fladle to pass module information to flank * can be automatic * feature available for users who use fladle (not available for 'flank only users) * requires PR for both, fladle and flank * strong coupling between tools","title":"Alternatives Considered"},{"location":"feature/1665-custom-sharding/","text":"Custom sharding With #1665 Flank received the new feature called Custom Sharding . It enables Flank to consume predefined sharding and apply it during a test run. The feature gives flexibility and enables manual optimization. It also allows users to set up different sharding per app-test apk pair (android only). Android Below you can find an example flow with all features explained 1. Acquire dump shard for current configuration Suppose you config with options: flank.yml gcloud: app: ./app-debug.apk robo-script: ./MainActivity_robo_script.json flank: max-test-shards: 2 additional-app-test-apks: - test: ./debug-1.apk - test: ../build-dir/debug-2.apk - test: gs://path/to/your/bucket/debug-3.apk flank firebase test android run -c=flank.yml will run on 4 matrices: 1x Robo run (just for example that robo-script will not collide with custom sharding on additional-app-test-apks ) 3x instrumentation tests with 2 shards max each flank firebase test android run -c=flank.yml --dump-shards produces android_shards.json with sharding: { \"matrix-0\" : { \"app\" : \"[PATH]/app-debug.apk\" , \"test\" : \"[PATH]/debug-1.apk\" , \"shards\" : { \"shard-0\" : [ \"class com.TestClassA#test1\" , \"class com.TestClassA#test2\" , \"class com.package2.TestClassB#test4\" ], \"shard-1\" : [ \"class com.TestClassA#test3\" , \"class com.package2.TestClassB#test1\" , \"class com.package2.TestClassB#test2\" , \"class com.package2.TestClassB#test3\" ] }, \"junit-ignored\" : [ \"class com.TestClassA#ignoredTest\" ] }, \"matrix-1\" : { \"app\" : \"[PATH]/app-debug.apk\" , \"test\" : \"[PATH]/debug-2.apk\" , \"shards\" : { \"shard-0\" : [ \"class com.ParameterizedTest\" , \"class com.package3.TestClass3#test5\" ], \"shard-1\" : [ \"class com.package3.TestClass3#test1\" , \"class com.package3.TestClass3#test2\" , \"class com.package3.TestClass3#test3\" , \"class com.package3.TestClass3#test4\" ] }, \"junit-ignored\" : [ \"class com.package.3.TestClassA#ignoredTest1\" , \"class com.package.3.TestClassA#ignoredTest2\" ] }, \"matrix-2\" : { \"app\" : \"[PATH]/app-debug.apk\" , \"test\" : \"[PATH]/debug-3.apk\" , \"shards\" : { \"shard-0\" : [ \"class com.package4.TestClass4#test1\" , \"class com.package4.TestClass4#test2\" , \"class com.package4.subpackage.TestClass6#test3\" ], \"shard-1\" : [ \"class com.package4.TestClass4#test3\" , \"class com.package4.subpackage.TestClass6#test1\" , \"class com.package4.subpackage.TestClass6#test2\" ] }, \"junit-ignored\" : [ ] } } 2. Prepare custom sharding JSON file You can now make changes as you wish, flank will attempt to find corresponding app-test pair names, and then apply custom sharding. for debug-1.apk let's add another shard and move TestClassB#test4 & TestClassB#test3 into it: { \"matrix-0\": { \"app\": \"[PATH]/app-debug.apk\", \"test\": \"[PATH]/debug-1.apk\", \"shards\": { \"shard-0\": [ \"class com.TestClassA#test1\", \"class com.TestClassA#test2\" ], \"shard-1\": [ \"class com.TestClassA#test3\", \"class com.package2.TestClassB#test1\", \"class com.package2.TestClassB#test2\" ], \"shard-2\": [ \"class com.package2.TestClassB#test4\", \"class com.package2.TestClassB#test3\" ] }, \"junit-ignored\": [ \"class com.TestClassA#ignoredTest\" ] }, \"matrix-1\": {...}, \"matrix-2\": {...} } for debug-2.apk we know that parameterized test takes lots of time so we want to have it in a separate shard: { \"matrix-0\": {...}, \"matrix-1\": { \"app\": \"[PATH]/app-debug.apk\", \"test\": \"[PATH]/debug-2.apk\", \"shards\": { \"shard-0\": [ \"class com.ParameterizedTest\" ], \"shard-1\": [ \"class com.package3.TestClass3#test1\", \"class com.package3.TestClass3#test2\", \"class com.package3.TestClass3#test3\", \"class com.package3.TestClass3#test4\", \"class com.package3.TestClass3#test5\" ] }, \"junit-ignored\": [ \"class com.package.3.TestClassA#ignoredTest1\", \"class com.package.3.TestClassA#ignoredTest2\" ] }, \"matrix-2\": {...} } for debug-3.apk all tests are rather quick, so we don't care about sharding, let's move them into one shard: { \"matrix-0\": {...}, \"matrix-1\": {...}, \"matrix-2\": { \"app\": \"[PATH]/app-debug.apk\", \"test\": \"gs://path/to/your/bucket/debug-3.apk\", \"shards\": { \"shard-0\": [ \"class com.package4.TestClass4#test1\", \"class com.package4.TestClass4#test2\", \"class com.package4.subpackage.TestClass6#test3\", \"class com.package4.TestClass4#test3\", \"class com.package4.subpackage.TestClass6#test1\", \"class com.package4.subpackage.TestClass6#test2\" ] }, \"junit-ignored\": [ ] } } Let's save newly created JSON as custom_sharding.json 3. Add custom sharding to your configuration Update flank.yml with custom-sharding-json option: gcloud: app: ./app-debug.apk robo-script: ./MainActivity_robo_script.json flank: max-test-shards: 2 additional-app-test-apks: - test: ./debug-1.apk - test: ../build-dir/debug-2.apk - test: gs://path/to/your/bucket/debug-3.apk custom-sharding-json: ./custom_sharding.json You can verify if shards are correctly applied by running the following command flank firebase test android run -c=flank.yml --dump-shards . This command will parse your shards configuration JSON into internal structures used for executing test and will print them back to the JSON file. The diff between the file specified in custom-sharding-json and the output file produced by --dump-shards should show that no changes were applied to custom shard configuration. 4. Start Test Run You can now start a flank test run. With the updated config there will still be 4 matrices: 1x Robo test 3x instrumentation tests: debug-1.apk with 3 shards debug-2.apk with 2 shards debug-3.apk with 1 shard iOS iOS custom sharding works exactly the same as Android (dump shards, modify, add a path to JSON file, etc) with some small exceptions: there is no additional-app-test-apks feature for iOS every shard in iOS is a separate matrix (FTL limitations) dump shard JSON is different for xctestrun with test plans, therefore there is different custom sharding JSON structure for both versions. Custom sharding JSON for xctestrun without Test Plans (example) [ { \"ExampleSwiftTests\" : [ \"ExampleSwiftTestsClass/test1\" , \"ExampleSwiftTestsClass/test2\" , \"ExampleSwiftTestsClass/test3\" , \"ExampleSwiftTestsClass/test4\" , \"ExampleSwiftTestsClass/test5\" ] }, { \"ExampleSwiftTests\" : [ \"ExampleSwiftTestsClass/test6\" , \"ExampleSwiftTestsClass/test7\" , \"ExampleSwiftTestsClass/test8\" , \"ExampleSwiftTestsClass/test9\" ] }, { \"ExampleSwiftTests\" : [ \"ExampleSwiftTestsClass/test10\" , \"ExampleSwiftTestsClass/test11\" , \"ExampleSwiftTestsClass/test12\" , \"ExampleSwiftTestsClass/test13\" ] }, { \"ExampleSwiftTests\" : [ \"ExampleSwiftTestsClass/test14\" , \"ExampleSwiftTestsClass/test15\" , \"ExampleSwiftTestsClass/test16\" , \"ExampleSwiftTestsClass/test17\" ] } ] Custom sharding JSON for xctestrun with Test Plans (example) { \"en\" : [ { \"SecondUITests\" : [ \"SecondUITestsClass/test2_PLLocale\" , \"SecondUITestsClass/test2_3\" , \"SecondUITestsClass/test2_ENLocale\" ], \"UITests\" : [ \"UITestsClass/test1_1\" , \"UITestsClass/test1_2\" , \"UITestsClass/test1_3\" ] }, { \"UITests\" : [ \"UITestsClass/test1_ENLocale\" , \"UITestsClass/test1_PLLocale\" ] }, { \"SecondUITests\" : [ \"SecondUITestsClass/test2_1\" , \"SecondUITestsClass/test2_2\" ] } ], \"pl\" : [ { \"UITests\" : [ \"UITestsClass/test1_1\" , \"UITestsClass/test1_2\" , \"UITestsClass/test1_3\" , \"UITestsClass/test1_ENLocale\" , \"UITestsClass/test1_PLLocale\" ] }, { \"SecondUITests\" : [ \"SecondUITestsClass/test2_1\" , \"SecondUITestsClass/test2_2\" , \"SecondUITestsClass/test2_PLLocale\" , \"SecondUITestsClass/test2_3\" , \"SecondUITestsClass/test2_ENLocale\" ] } ] } NOTE: flank DOES NOT validate the provided custom sharding JSON -- it's your responsibility to provide a proper configuration flank will apply sharding by searching for test pairs by app apk and test apk paths custom sharding supports gs:// paths custom sharding JSON is a source of truth -- no smart sharding is applied (or sharding related configurations) matrices ids and shard ids are not important, the only requirement is -- they should be unique you can provide custom sharding JSON created entirely from scratch custom sharding is very similar to test-targets-for-shard , which means you can use the same test targets when preparing custom sharding. Below example will create 3 shards, one for each of the packages ( bar , foo , parameterized ): { \"matrix-0\" : { \"app\" : \"./any-app.apk\" , \"test\" : \"./any-debug.apk\" , \"shards\" : { \"shard-0\" : [ \"package com.bar\" ], \"shard-1\" : [ \"package com.parametrized\" ], \"shard-2\" : [ \"package com.similar\" ] }, \"junit-ignored\" : [ ] } } Problems? Something missing? If you believe there is a problem with the custom sharding, or you would like to have some additional feature -- let us know and create an issue in flank's backlog. Any feedback is more than welcome!","title":"Custom sharding"},{"location":"feature/1665-custom-sharding/#custom-sharding","text":"With #1665 Flank received the new feature called Custom Sharding . It enables Flank to consume predefined sharding and apply it during a test run. The feature gives flexibility and enables manual optimization. It also allows users to set up different sharding per app-test apk pair (android only).","title":"Custom sharding"},{"location":"feature/1665-custom-sharding/#android","text":"Below you can find an example flow with all features explained","title":"Android"},{"location":"feature/1665-custom-sharding/#1-acquire-dump-shard-for-current-configuration","text":"Suppose you config with options: flank.yml gcloud: app: ./app-debug.apk robo-script: ./MainActivity_robo_script.json flank: max-test-shards: 2 additional-app-test-apks: - test: ./debug-1.apk - test: ../build-dir/debug-2.apk - test: gs://path/to/your/bucket/debug-3.apk flank firebase test android run -c=flank.yml will run on 4 matrices: 1x Robo run (just for example that robo-script will not collide with custom sharding on additional-app-test-apks ) 3x instrumentation tests with 2 shards max each flank firebase test android run -c=flank.yml --dump-shards produces android_shards.json with sharding: { \"matrix-0\" : { \"app\" : \"[PATH]/app-debug.apk\" , \"test\" : \"[PATH]/debug-1.apk\" , \"shards\" : { \"shard-0\" : [ \"class com.TestClassA#test1\" , \"class com.TestClassA#test2\" , \"class com.package2.TestClassB#test4\" ], \"shard-1\" : [ \"class com.TestClassA#test3\" , \"class com.package2.TestClassB#test1\" , \"class com.package2.TestClassB#test2\" , \"class com.package2.TestClassB#test3\" ] }, \"junit-ignored\" : [ \"class com.TestClassA#ignoredTest\" ] }, \"matrix-1\" : { \"app\" : \"[PATH]/app-debug.apk\" , \"test\" : \"[PATH]/debug-2.apk\" , \"shards\" : { \"shard-0\" : [ \"class com.ParameterizedTest\" , \"class com.package3.TestClass3#test5\" ], \"shard-1\" : [ \"class com.package3.TestClass3#test1\" , \"class com.package3.TestClass3#test2\" , \"class com.package3.TestClass3#test3\" , \"class com.package3.TestClass3#test4\" ] }, \"junit-ignored\" : [ \"class com.package.3.TestClassA#ignoredTest1\" , \"class com.package.3.TestClassA#ignoredTest2\" ] }, \"matrix-2\" : { \"app\" : \"[PATH]/app-debug.apk\" , \"test\" : \"[PATH]/debug-3.apk\" , \"shards\" : { \"shard-0\" : [ \"class com.package4.TestClass4#test1\" , \"class com.package4.TestClass4#test2\" , \"class com.package4.subpackage.TestClass6#test3\" ], \"shard-1\" : [ \"class com.package4.TestClass4#test3\" , \"class com.package4.subpackage.TestClass6#test1\" , \"class com.package4.subpackage.TestClass6#test2\" ] }, \"junit-ignored\" : [ ] } }","title":"1. Acquire dump shard for current configuration"},{"location":"feature/1665-custom-sharding/#2-prepare-custom-sharding-json-file","text":"You can now make changes as you wish, flank will attempt to find corresponding app-test pair names, and then apply custom sharding. for debug-1.apk let's add another shard and move TestClassB#test4 & TestClassB#test3 into it: { \"matrix-0\": { \"app\": \"[PATH]/app-debug.apk\", \"test\": \"[PATH]/debug-1.apk\", \"shards\": { \"shard-0\": [ \"class com.TestClassA#test1\", \"class com.TestClassA#test2\" ], \"shard-1\": [ \"class com.TestClassA#test3\", \"class com.package2.TestClassB#test1\", \"class com.package2.TestClassB#test2\" ], \"shard-2\": [ \"class com.package2.TestClassB#test4\", \"class com.package2.TestClassB#test3\" ] }, \"junit-ignored\": [ \"class com.TestClassA#ignoredTest\" ] }, \"matrix-1\": {...}, \"matrix-2\": {...} } for debug-2.apk we know that parameterized test takes lots of time so we want to have it in a separate shard: { \"matrix-0\": {...}, \"matrix-1\": { \"app\": \"[PATH]/app-debug.apk\", \"test\": \"[PATH]/debug-2.apk\", \"shards\": { \"shard-0\": [ \"class com.ParameterizedTest\" ], \"shard-1\": [ \"class com.package3.TestClass3#test1\", \"class com.package3.TestClass3#test2\", \"class com.package3.TestClass3#test3\", \"class com.package3.TestClass3#test4\", \"class com.package3.TestClass3#test5\" ] }, \"junit-ignored\": [ \"class com.package.3.TestClassA#ignoredTest1\", \"class com.package.3.TestClassA#ignoredTest2\" ] }, \"matrix-2\": {...} } for debug-3.apk all tests are rather quick, so we don't care about sharding, let's move them into one shard: { \"matrix-0\": {...}, \"matrix-1\": {...}, \"matrix-2\": { \"app\": \"[PATH]/app-debug.apk\", \"test\": \"gs://path/to/your/bucket/debug-3.apk\", \"shards\": { \"shard-0\": [ \"class com.package4.TestClass4#test1\", \"class com.package4.TestClass4#test2\", \"class com.package4.subpackage.TestClass6#test3\", \"class com.package4.TestClass4#test3\", \"class com.package4.subpackage.TestClass6#test1\", \"class com.package4.subpackage.TestClass6#test2\" ] }, \"junit-ignored\": [ ] } } Let's save newly created JSON as custom_sharding.json","title":"2. Prepare custom sharding JSON file"},{"location":"feature/1665-custom-sharding/#3-add-custom-sharding-to-your-configuration","text":"Update flank.yml with custom-sharding-json option: gcloud: app: ./app-debug.apk robo-script: ./MainActivity_robo_script.json flank: max-test-shards: 2 additional-app-test-apks: - test: ./debug-1.apk - test: ../build-dir/debug-2.apk - test: gs://path/to/your/bucket/debug-3.apk custom-sharding-json: ./custom_sharding.json You can verify if shards are correctly applied by running the following command flank firebase test android run -c=flank.yml --dump-shards . This command will parse your shards configuration JSON into internal structures used for executing test and will print them back to the JSON file. The diff between the file specified in custom-sharding-json and the output file produced by --dump-shards should show that no changes were applied to custom shard configuration.","title":"3. Add custom sharding to your configuration"},{"location":"feature/1665-custom-sharding/#4-start-test-run","text":"You can now start a flank test run. With the updated config there will still be 4 matrices: 1x Robo test 3x instrumentation tests: debug-1.apk with 3 shards debug-2.apk with 2 shards debug-3.apk with 1 shard","title":"4. Start Test Run"},{"location":"feature/1665-custom-sharding/#ios","text":"iOS custom sharding works exactly the same as Android (dump shards, modify, add a path to JSON file, etc) with some small exceptions: there is no additional-app-test-apks feature for iOS every shard in iOS is a separate matrix (FTL limitations) dump shard JSON is different for xctestrun with test plans, therefore there is different custom sharding JSON structure for both versions.","title":"iOS"},{"location":"feature/1665-custom-sharding/#custom-sharding-json-for-xctestrun-without-test-plans-example","text":"[ { \"ExampleSwiftTests\" : [ \"ExampleSwiftTestsClass/test1\" , \"ExampleSwiftTestsClass/test2\" , \"ExampleSwiftTestsClass/test3\" , \"ExampleSwiftTestsClass/test4\" , \"ExampleSwiftTestsClass/test5\" ] }, { \"ExampleSwiftTests\" : [ \"ExampleSwiftTestsClass/test6\" , \"ExampleSwiftTestsClass/test7\" , \"ExampleSwiftTestsClass/test8\" , \"ExampleSwiftTestsClass/test9\" ] }, { \"ExampleSwiftTests\" : [ \"ExampleSwiftTestsClass/test10\" , \"ExampleSwiftTestsClass/test11\" , \"ExampleSwiftTestsClass/test12\" , \"ExampleSwiftTestsClass/test13\" ] }, { \"ExampleSwiftTests\" : [ \"ExampleSwiftTestsClass/test14\" , \"ExampleSwiftTestsClass/test15\" , \"ExampleSwiftTestsClass/test16\" , \"ExampleSwiftTestsClass/test17\" ] } ]","title":"Custom sharding JSON for xctestrun without Test Plans (example)"},{"location":"feature/1665-custom-sharding/#custom-sharding-json-for-xctestrun-with-test-plans-example","text":"{ \"en\" : [ { \"SecondUITests\" : [ \"SecondUITestsClass/test2_PLLocale\" , \"SecondUITestsClass/test2_3\" , \"SecondUITestsClass/test2_ENLocale\" ], \"UITests\" : [ \"UITestsClass/test1_1\" , \"UITestsClass/test1_2\" , \"UITestsClass/test1_3\" ] }, { \"UITests\" : [ \"UITestsClass/test1_ENLocale\" , \"UITestsClass/test1_PLLocale\" ] }, { \"SecondUITests\" : [ \"SecondUITestsClass/test2_1\" , \"SecondUITestsClass/test2_2\" ] } ], \"pl\" : [ { \"UITests\" : [ \"UITestsClass/test1_1\" , \"UITestsClass/test1_2\" , \"UITestsClass/test1_3\" , \"UITestsClass/test1_ENLocale\" , \"UITestsClass/test1_PLLocale\" ] }, { \"SecondUITests\" : [ \"SecondUITestsClass/test2_1\" , \"SecondUITestsClass/test2_2\" , \"SecondUITestsClass/test2_PLLocale\" , \"SecondUITestsClass/test2_3\" , \"SecondUITestsClass/test2_ENLocale\" ] } ] }","title":"Custom sharding JSON for xctestrun with Test Plans (example)"},{"location":"feature/1665-custom-sharding/#note","text":"flank DOES NOT validate the provided custom sharding JSON -- it's your responsibility to provide a proper configuration flank will apply sharding by searching for test pairs by app apk and test apk paths custom sharding supports gs:// paths custom sharding JSON is a source of truth -- no smart sharding is applied (or sharding related configurations) matrices ids and shard ids are not important, the only requirement is -- they should be unique you can provide custom sharding JSON created entirely from scratch custom sharding is very similar to test-targets-for-shard , which means you can use the same test targets when preparing custom sharding. Below example will create 3 shards, one for each of the packages ( bar , foo , parameterized ): { \"matrix-0\" : { \"app\" : \"./any-app.apk\" , \"test\" : \"./any-debug.apk\" , \"shards\" : { \"shard-0\" : [ \"package com.bar\" ], \"shard-1\" : [ \"package com.parametrized\" ], \"shard-2\" : [ \"package com.similar\" ] }, \"junit-ignored\" : [ ] } }","title":"NOTE:"},{"location":"feature/1665-custom-sharding/#problems-something-missing","text":"If you believe there is a problem with the custom sharding, or you would like to have some additional feature -- let us know and create an issue in flank's backlog. Any feedback is more than welcome!","title":"Problems? Something missing?"},{"location":"feature/1692_test_case_based_retries/","text":"Test case based retries Description Test case-based retires is a feature request which automatically retries failed tests. Things worth mention [\"One thing that would be worth to keep in mind, since FTL does not support such logic it would require from flank creating new matrix (matrices) for failed tests. It's not something extremely difficult but there is another overhead. FTL will set up new device(s) and that takes some time. I think it's not a blocker but let's don't forget about it.\"(https://github.com/Flank/flank/issues/778#issuecomment-696027295) please also make sure that the total test count tally is not impacted by the retry. If we have 100 tests that execute, and 2 are flaky and get re-ran twice, we should still report 100 tests. I saw this issue in FTL where they actually incremented the total test count, which threw off our jenkins test analyzer. Solution Failed test case retries could be done using an automatic solution that starts failing tests just after the previous test run finished. Also, there should be a possibility to manually run only failed tests based on the report from a previous run. Test case based retries will not be compatible with the FTL option num-flaky-test-attempts . Automatic Run Flank config will contain a Boolean option to retry failed tests. When using this option flank will automatically create a new matrix with failed tests which will be run afterwards. Implementation A new option is required to be added to enable the auto-retry. When the option is present Flank will check if there are any failed tests after the test run and retry it by creating a new matrix. After running the matrix test report needs to be updated with retried test results. Result As a result, retried failed test run results will replace the first test run. Possible problems The output should clearly show information that failed tests were re-executed. Manual Run Flank will have a new command to run only failed tests. The report file should have this argument appended. Implementation To run manual retry failed test case-based retries a new flank command needs to be added. As an additional Flank argument, the previous report file needs to be provided Result As a result, we will get the new test run with a separate report.","title":"Test case based retries"},{"location":"feature/1692_test_case_based_retries/#test-case-based-retries","text":"","title":"Test case based retries"},{"location":"feature/1692_test_case_based_retries/#description","text":"Test case-based retires is a feature request which automatically retries failed tests.","title":"Description"},{"location":"feature/1692_test_case_based_retries/#things-worth-mention","text":"[\"One thing that would be worth to keep in mind, since FTL does not support such logic it would require from flank creating new matrix (matrices) for failed tests. It's not something extremely difficult but there is another overhead. FTL will set up new device(s) and that takes some time. I think it's not a blocker but let's don't forget about it.\"(https://github.com/Flank/flank/issues/778#issuecomment-696027295) please also make sure that the total test count tally is not impacted by the retry. If we have 100 tests that execute, and 2 are flaky and get re-ran twice, we should still report 100 tests. I saw this issue in FTL where they actually incremented the total test count, which threw off our jenkins test analyzer.","title":"Things worth mention"},{"location":"feature/1692_test_case_based_retries/#solution","text":"Failed test case retries could be done using an automatic solution that starts failing tests just after the previous test run finished. Also, there should be a possibility to manually run only failed tests based on the report from a previous run. Test case based retries will not be compatible with the FTL option num-flaky-test-attempts .","title":"Solution"},{"location":"feature/1692_test_case_based_retries/#automatic","text":"","title":"Automatic"},{"location":"feature/1692_test_case_based_retries/#run","text":"Flank config will contain a Boolean option to retry failed tests. When using this option flank will automatically create a new matrix with failed tests which will be run afterwards.","title":"Run"},{"location":"feature/1692_test_case_based_retries/#implementation","text":"A new option is required to be added to enable the auto-retry. When the option is present Flank will check if there are any failed tests after the test run and retry it by creating a new matrix. After running the matrix test report needs to be updated with retried test results.","title":"Implementation"},{"location":"feature/1692_test_case_based_retries/#result","text":"As a result, retried failed test run results will replace the first test run.","title":"Result"},{"location":"feature/1692_test_case_based_retries/#possible-problems","text":"The output should clearly show information that failed tests were re-executed.","title":"Possible problems"},{"location":"feature/1692_test_case_based_retries/#manual","text":"","title":"Manual"},{"location":"feature/1692_test_case_based_retries/#run_1","text":"Flank will have a new command to run only failed tests. The report file should have this argument appended.","title":"Run"},{"location":"feature/1692_test_case_based_retries/#implementation_1","text":"To run manual retry failed test case-based retries a new flank command needs to be added. As an additional Flank argument, the previous report file needs to be provided","title":"Implementation"},{"location":"feature/1692_test_case_based_retries/#result_1","text":"As a result, we will get the new test run with a separate report.","title":"Result"},{"location":"feature/ios_test_plans/","text":"Flow Flow starts by parsing .xctestrun file. Search for: __xctestrun_metadata__ key. <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 1 </integer> </dict> FormatVersion: 1 - old version of .xctestrun FormatVersion: 2 - the newest version with test plans If format is different than 1 or 2 throw an error. FormatVersion: 1 Any other key than metadata should have corresponding TestTarget dictionary. In example below EarlGreyExampleSwiftTests has a TestTarget dictionary. <plist version= \"1.0\" > <dict> <key> EarlGreyExampleSwiftTests </key> <dict> <!-- TestTarget --> <key> BlueprintName </key> <string> EarlGreyExampleSwiftTests </string> ... </dict> <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 1 </integer> </dict> </dict> </plist> FormatVersion: 2 In this version, XML contains two keys: TestConfigurations and TestPlan in addition to __xctestrun_metadata__ . TestPlan is just a dictionary containing basic informations about current TestPlan. We can ignore it. So it's excluded from example xml below. TestConfigurations is an array of different test configurations. Test configuration contains name property and array of TestTargets. <plist version= \"1.0\" > <dict> <key> Name </key> <string> pl </string> <!-- Name property --> <key> TestTargets </key> <array> <!-- TestConfigurations --> <dict> <key> BlueprintName </key> <string> UITests </string> <!-- Test target and Test configuration properties go here --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <!-- Test target and Test configuration properties go here --> </dict> </array> </dict> </plist> Each configuration may contain different Environment Variables, languages, regions or any other properties. Those properties are stored under TestTarget. Currently FTL doesn't support specifying TestConfiguration for test execution. If there is more than one configuration FTL will probably choose one arbitrarily. For now Flank will allow specifying which test configuration should run with only-test-configuration argument. Running test plan locally Build Xcode project To build example project run command below. xcodebuild build-for-testing \\ -allowProvisioningUpdates \\ -project \"FlankMultiTestTargetsExample.xcodeproj\" \\ -scheme \"AllTests\" \\ #Scheme should have test plans enabled -derivedDataPath \"build_testplan_device\" \\ -sdk iphoneos | xcpretty This command will generate directory: Debug-iphoneos containing binaries and .xctestrun file for each TestPlan. In this example scheme AllTests has have only one test plan: AllTests with two test configurations: pl and en . Test Plan contains two Test Targets: UITests and SecondUITests Outputted .xctestrun should looks like this: <plist version= \"1.0\" > <dict> <key> TestConfigurations </key> <array> <dict> <key> Name </key> <string> en </string> <key> TestTargets </key> <!-- Test Targets for `en` test configuration --> <array> <dict> <key> BlueprintName </key> <string> UITests </string> <key> TestLanguage </key> <string> en </string> <key> TestRegion </key> <string> GB </string> <!-- Language and region --> <!-- ... Other properties --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <key> TestLanguage </key> <string> en </string> <key> TestRegion </key> <string> GB </string> <!-- Language and region --> <!-- ... Other properties --> </dict> </array> </dict> <dict> <key> Name </key> <string> pl </string> <key> TestTargets </key> <!-- Test Targets for `pl` test configuration --> <array> <dict> <key> BlueprintName </key> <string> UITests </string> <key> TestLanguage </key> <string> pl </string> <key> TestRegion </key> <string> PL </string> <!-- Language and region --> <!-- ... Other properties --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <key> TestLanguage </key> <string> pl </string> <key> TestRegion </key> <string> PL </string> <!-- Language and region --> <!-- ... Other properties --> </dict> </array> </dict> </array> <key> TestPlan </key> <dict> <key> IsDefault </key> <true/> <key> Name </key> <string> AllTests </string> </dict> <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 2 </integer> </dict> </dict> </plist> Running tests on a local device After generating binaries and .xctestrun file we can run tests using command. xcodebuild test-without-building \\ -xctestrun \"build_testplan_device/Build/Products/testrun.xctestrun\" \\ -destination \"platform=iOS,id=00008030-000209DC1A50802E\" \\ -only-test-configuration pl | xcpretty Option: -only-test-configuration pl allows to specify which test configuration should Xcode run.","title":"Flow"},{"location":"feature/ios_test_plans/#flow","text":"Flow starts by parsing .xctestrun file. Search for: __xctestrun_metadata__ key. <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 1 </integer> </dict> FormatVersion: 1 - old version of .xctestrun FormatVersion: 2 - the newest version with test plans If format is different than 1 or 2 throw an error.","title":"Flow"},{"location":"feature/ios_test_plans/#formatversion-1","text":"Any other key than metadata should have corresponding TestTarget dictionary. In example below EarlGreyExampleSwiftTests has a TestTarget dictionary. <plist version= \"1.0\" > <dict> <key> EarlGreyExampleSwiftTests </key> <dict> <!-- TestTarget --> <key> BlueprintName </key> <string> EarlGreyExampleSwiftTests </string> ... </dict> <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 1 </integer> </dict> </dict> </plist>","title":"FormatVersion: 1"},{"location":"feature/ios_test_plans/#formatversion-2","text":"In this version, XML contains two keys: TestConfigurations and TestPlan in addition to __xctestrun_metadata__ . TestPlan is just a dictionary containing basic informations about current TestPlan. We can ignore it. So it's excluded from example xml below. TestConfigurations is an array of different test configurations. Test configuration contains name property and array of TestTargets. <plist version= \"1.0\" > <dict> <key> Name </key> <string> pl </string> <!-- Name property --> <key> TestTargets </key> <array> <!-- TestConfigurations --> <dict> <key> BlueprintName </key> <string> UITests </string> <!-- Test target and Test configuration properties go here --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <!-- Test target and Test configuration properties go here --> </dict> </array> </dict> </plist> Each configuration may contain different Environment Variables, languages, regions or any other properties. Those properties are stored under TestTarget. Currently FTL doesn't support specifying TestConfiguration for test execution. If there is more than one configuration FTL will probably choose one arbitrarily. For now Flank will allow specifying which test configuration should run with only-test-configuration argument.","title":"FormatVersion: 2"},{"location":"feature/ios_test_plans/#running-test-plan-locally","text":"","title":"Running test plan locally"},{"location":"feature/ios_test_plans/#build-xcode-project","text":"To build example project run command below. xcodebuild build-for-testing \\ -allowProvisioningUpdates \\ -project \"FlankMultiTestTargetsExample.xcodeproj\" \\ -scheme \"AllTests\" \\ #Scheme should have test plans enabled -derivedDataPath \"build_testplan_device\" \\ -sdk iphoneos | xcpretty This command will generate directory: Debug-iphoneos containing binaries and .xctestrun file for each TestPlan. In this example scheme AllTests has have only one test plan: AllTests with two test configurations: pl and en . Test Plan contains two Test Targets: UITests and SecondUITests Outputted .xctestrun should looks like this: <plist version= \"1.0\" > <dict> <key> TestConfigurations </key> <array> <dict> <key> Name </key> <string> en </string> <key> TestTargets </key> <!-- Test Targets for `en` test configuration --> <array> <dict> <key> BlueprintName </key> <string> UITests </string> <key> TestLanguage </key> <string> en </string> <key> TestRegion </key> <string> GB </string> <!-- Language and region --> <!-- ... Other properties --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <key> TestLanguage </key> <string> en </string> <key> TestRegion </key> <string> GB </string> <!-- Language and region --> <!-- ... Other properties --> </dict> </array> </dict> <dict> <key> Name </key> <string> pl </string> <key> TestTargets </key> <!-- Test Targets for `pl` test configuration --> <array> <dict> <key> BlueprintName </key> <string> UITests </string> <key> TestLanguage </key> <string> pl </string> <key> TestRegion </key> <string> PL </string> <!-- Language and region --> <!-- ... Other properties --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <key> TestLanguage </key> <string> pl </string> <key> TestRegion </key> <string> PL </string> <!-- Language and region --> <!-- ... Other properties --> </dict> </array> </dict> </array> <key> TestPlan </key> <dict> <key> IsDefault </key> <true/> <key> Name </key> <string> AllTests </string> </dict> <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 2 </integer> </dict> </dict> </plist>","title":"Build Xcode project"},{"location":"feature/ios_test_plans/#running-tests-on-a-local-device","text":"After generating binaries and .xctestrun file we can run tests using command. xcodebuild test-without-building \\ -xctestrun \"build_testplan_device/Build/Products/testrun.xctestrun\" \\ -destination \"platform=iOS,id=00008030-000209DC1A50802E\" \\ -only-test-configuration pl | xcpretty Option: -only-test-configuration pl allows to specify which test configuration should Xcode run.","title":"Running tests on a local device"},{"location":"feature/summary_output/","text":"Formatted summary output \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-35czp85w4h3a7 \u2502 greatqlte-26-en-portrait \u2502 20 test cases passed \u2502 \u2502 failure \u2502 matrix-35czp85w4h3a7 \u2502 Nexus6P-26-en-portrait \u2502 1 test cases failed, 16 passed, 3 flaky \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 User scenario As a user I want to see finely formatted summary result at the end of execution output. Motivation Gcloud prints summary output in the table. It looks nice and is readable. Why we wouldn't have same in flank? Possible outputs Numbers are representing OUTCOME column, points are representing TEST DETAILS column. 1. success | flaky * ${1} test cases passed | ${2} skipped | ${3} flakes | (Native crash) | --- 2. failure * ${1} test cases failed | ${2} errors | ${3} passed | ${4} skipped | ${4} flakes | (Native crash) * Application crashed | (Native crash) * Test timed out | (Native crash) * App failed to install | (Native crash) * Unknown failure | (Native crash) 3. inconclusive * Infrastructure failure * Test run aborted by user * Unknown reason 4. skipped * Incompatible device/OS combination * App does not support the device architecture * App does not support the OS version * Unknown reason Implementation details Outcome calculation v2 #919 Because of rate limit issue , the new implementation of test details calculation is based on gcloud's CreateMatrixOutcomeSummaryUsingEnvironments . which was already described in Outcome calculation v1 section. The activity diagram for gcloud The activity diagram for flank Both diagrams are showing slightly different aspects of flow but the main difference between gcloud and flank is that the flank is calculating billable minutes in addition. The billable minutes are able to calculate from list of steps , so while gcloud is fetching steps only when environments are corrupted, the flank always required at least steps . Drawbacks Calculating outcome details basing on steps may not return info about flaky tests. But it is possible to reuse data that is collecting for JUnitReport as alternative way for generating outcome details. It should be delivered in dedicated pull request. Flank is not displaying info about the environment in outcome table this problem is described in #983 issue. Outcome calculation v1 It should be mentioned there are some crucial differences how flank and gcloud calculates outcome value. Gcloud is using following API calls 1. self._client.projects_histories_executions_environments.List(request) 2. self._client.projects_histories_executions_steps.List(request) The first one is default, but if returns any environment without environmentResult.outcome , the second one will be used to obtain steps . Both environemnts and steps can provide outcome . The difference between them is the steps returns success event if tests are flaky . Currently, we don't know why self._client.projects_histories_executions_environments.List(request) may return empty environmentResult.outcome . In difference to gcloud flank uses 3 api call to obtain necessary data 1. TestMatrix - GcTesting.get.projects().testMatrices().get(projectId, testMatrixId) 2. Step - toolsResults.projects().histories().executions().steps().get(projectId, historyId, executionId, stepId) 3. ListTestCasesResponse - toolsResults.projects().histories().executions().steps().testCases().get(projectId, historyId, executionId, stepId) TestMatrix from first call provides ToolResultsStep through TestExecution which is used to obtain arguments for next two calls. This is part of flank legacy. Those api calls provides data for JUnitResult . As JUnitResult contains all data required to generate table output, we can reuse it. In result, we are forced to calculate flaky outcomes on flank site because of step . Probably it is place for little improvement in the future. Test details calculation When flank and gcloud implementations can be slightly different because of programming languages, the logic behind is the mainly same.","title":"Formatted summary output"},{"location":"feature/summary_output/#formatted-summary-output","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-35czp85w4h3a7 \u2502 greatqlte-26-en-portrait \u2502 20 test cases passed \u2502 \u2502 failure \u2502 matrix-35czp85w4h3a7 \u2502 Nexus6P-26-en-portrait \u2502 1 test cases failed, 16 passed, 3 flaky \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Formatted summary output"},{"location":"feature/summary_output/#user-scenario","text":"As a user I want to see finely formatted summary result at the end of execution output.","title":"User scenario"},{"location":"feature/summary_output/#motivation","text":"Gcloud prints summary output in the table. It looks nice and is readable. Why we wouldn't have same in flank?","title":"Motivation"},{"location":"feature/summary_output/#possible-outputs","text":"Numbers are representing OUTCOME column, points are representing TEST DETAILS column. 1. success | flaky * ${1} test cases passed | ${2} skipped | ${3} flakes | (Native crash) | --- 2. failure * ${1} test cases failed | ${2} errors | ${3} passed | ${4} skipped | ${4} flakes | (Native crash) * Application crashed | (Native crash) * Test timed out | (Native crash) * App failed to install | (Native crash) * Unknown failure | (Native crash) 3. inconclusive * Infrastructure failure * Test run aborted by user * Unknown reason 4. skipped * Incompatible device/OS combination * App does not support the device architecture * App does not support the OS version * Unknown reason","title":"Possible outputs"},{"location":"feature/summary_output/#implementation-details","text":"","title":"Implementation details"},{"location":"feature/summary_output/#outcome-calculation-v2-919","text":"Because of rate limit issue , the new implementation of test details calculation is based on gcloud's CreateMatrixOutcomeSummaryUsingEnvironments . which was already described in Outcome calculation v1 section. The activity diagram for gcloud The activity diagram for flank Both diagrams are showing slightly different aspects of flow but the main difference between gcloud and flank is that the flank is calculating billable minutes in addition. The billable minutes are able to calculate from list of steps , so while gcloud is fetching steps only when environments are corrupted, the flank always required at least steps .","title":"Outcome calculation v2 #919"},{"location":"feature/summary_output/#drawbacks","text":"Calculating outcome details basing on steps may not return info about flaky tests. But it is possible to reuse data that is collecting for JUnitReport as alternative way for generating outcome details. It should be delivered in dedicated pull request. Flank is not displaying info about the environment in outcome table this problem is described in #983 issue.","title":"Drawbacks"},{"location":"feature/summary_output/#outcome-calculation-v1","text":"It should be mentioned there are some crucial differences how flank and gcloud calculates outcome value. Gcloud is using following API calls 1. self._client.projects_histories_executions_environments.List(request) 2. self._client.projects_histories_executions_steps.List(request) The first one is default, but if returns any environment without environmentResult.outcome , the second one will be used to obtain steps . Both environemnts and steps can provide outcome . The difference between them is the steps returns success event if tests are flaky . Currently, we don't know why self._client.projects_histories_executions_environments.List(request) may return empty environmentResult.outcome . In difference to gcloud flank uses 3 api call to obtain necessary data 1. TestMatrix - GcTesting.get.projects().testMatrices().get(projectId, testMatrixId) 2. Step - toolsResults.projects().histories().executions().steps().get(projectId, historyId, executionId, stepId) 3. ListTestCasesResponse - toolsResults.projects().histories().executions().steps().testCases().get(projectId, historyId, executionId, stepId) TestMatrix from first call provides ToolResultsStep through TestExecution which is used to obtain arguments for next two calls. This is part of flank legacy. Those api calls provides data for JUnitResult . As JUnitResult contains all data required to generate table output, we can reuse it. In result, we are forced to calculate flaky outcomes on flank site because of step . Probably it is place for little improvement in the future.","title":"Outcome calculation v1"},{"location":"feature/summary_output/#test-details-calculation","text":"When flank and gcloud implementations can be slightly different because of programming languages, the logic behind is the mainly same.","title":"Test details calculation"},{"location":"flank-github-action/flank_github_acton_sdd/","text":"Flank Github action Add Github action which allows running Flank References Github actions documentation Creating a Docker container action Creating a JavaScript action Creating a composite run steps action Github actions marketplace Motivation Bitrise and Circle CI have steps in their CIs to run Flank. As Github is a widely used code repository and Github actions become more and more popular there should be also Github action that allows running Flank. After creating Github action, Flank could reach more users. Goals Flank actions should be as easy as possible to run Flank action should be maintainable together with Flank runner (always be up to date with options) Flank action should be available on the public repository Design There should be a possibility to specify all flank options using Github action variables as input, as well as passing the configuration itself. User must provide at least: Flank version (default latest) platform to run (iOS or Android) Flank service account file to authenticate Flank options or flank configuration file path API Flank Github action will be developed using composite run steps action which just packs other actions and runs it as a single action. Design proposal name : 'Flank' description : 'Run Flank from Github actions!' inputs : version : description : 'Version of flank to run' required : true default : <latest> platform : description : 'Platform to run iOS or Android' required : true flank_option1 : description : 'Description of option 1' required : false # validation will done when running ... flank_optionN : description : 'Description of option N' required : false # validation will done when running flank_configuration_file : description : 'Path to configuration file' required : false # validation will done when running flank_service_account : description : 'Path to service account file to authenticate' required : true outputs : output_report : description : \"Output report\" value : ${{ steps.report.outputs.random-id }} runs : using : \"composite\" steps : - id : download flank run : <download flank> shell : bash - id : validate configuration run : <check if configuation file or any options are specified> shell : bash - id : run flank run : <check if configuation file or any options are specified> shell : bash - id : report run : <show run report to user> shell : bash Usage With config file on : [ push ] jobs : hello_world_job : runs-on : ubuntu-latest name : A job to say hello steps : - uses : actions/checkout@v2 - id : flank uses : actions/flank@v1 with : version : '21.01.0' platform : 'android' flank_service_account : './service_account.json' flank_configuration_file : './flank.yml' - run : cat ${{ steps.flank.outputs.output_report }} shell : bash With options on : [ push ] jobs : hello_world_job : runs-on : ubuntu-latest name : A job to say hello steps : - uses : actions/checkout@v2 - id : flank uses : actions/flank@v1 with : version : '21.01.0' platform : 'android' flank_service_account : './service_account.json' app : \"../test_projects/android/apks/app-debug.apk\" test : \"../test_projects/android/apks/app-debug-androidTest.apk\" results-dir : test_dir legacy-junit-result : true - run : cat ${{ steps.flank.outputs.output_report }} shell : bash Results After finishing Flank GitHub actions should be published to Github actions marketplace . There should be also an announcement on Flank channel on Slack. The thing to consider is also using it for our internal verification together with integration tests Dependencies Github action needs to have action.yml file in the root of the repository, however, Flank repository has the file for posting Slack message after release. However it is only used by Flank team, so it will be best to move it as a side repository and keep the main Flank action in Flank's mono repository Testing A new testing repository will be set up to test action as described in GitHub actions documentation Alternatives Considered The investigation was done to choose the best way to develop custom GitHub action. Docker container action is not the best choice, because users are forced to run Flank action on Ubuntu workflow . The second alternative considered was using JavaScript as action language . However, this is another language to maintain in our project and it will not be the best option to choose for such an important thing","title":"Flank Github action"},{"location":"flank-github-action/flank_github_acton_sdd/#flank-github-action","text":"Add Github action which allows running Flank","title":"Flank Github action"},{"location":"flank-github-action/flank_github_acton_sdd/#references","text":"Github actions documentation Creating a Docker container action Creating a JavaScript action Creating a composite run steps action Github actions marketplace","title":"References"},{"location":"flank-github-action/flank_github_acton_sdd/#motivation","text":"Bitrise and Circle CI have steps in their CIs to run Flank. As Github is a widely used code repository and Github actions become more and more popular there should be also Github action that allows running Flank. After creating Github action, Flank could reach more users.","title":"Motivation"},{"location":"flank-github-action/flank_github_acton_sdd/#goals","text":"Flank actions should be as easy as possible to run Flank action should be maintainable together with Flank runner (always be up to date with options) Flank action should be available on the public repository","title":"Goals"},{"location":"flank-github-action/flank_github_acton_sdd/#design","text":"There should be a possibility to specify all flank options using Github action variables as input, as well as passing the configuration itself. User must provide at least: Flank version (default latest) platform to run (iOS or Android) Flank service account file to authenticate Flank options or flank configuration file path","title":"Design"},{"location":"flank-github-action/flank_github_acton_sdd/#api","text":"Flank Github action will be developed using composite run steps action which just packs other actions and runs it as a single action.","title":"API"},{"location":"flank-github-action/flank_github_acton_sdd/#design-proposal","text":"name : 'Flank' description : 'Run Flank from Github actions!' inputs : version : description : 'Version of flank to run' required : true default : <latest> platform : description : 'Platform to run iOS or Android' required : true flank_option1 : description : 'Description of option 1' required : false # validation will done when running ... flank_optionN : description : 'Description of option N' required : false # validation will done when running flank_configuration_file : description : 'Path to configuration file' required : false # validation will done when running flank_service_account : description : 'Path to service account file to authenticate' required : true outputs : output_report : description : \"Output report\" value : ${{ steps.report.outputs.random-id }} runs : using : \"composite\" steps : - id : download flank run : <download flank> shell : bash - id : validate configuration run : <check if configuation file or any options are specified> shell : bash - id : run flank run : <check if configuation file or any options are specified> shell : bash - id : report run : <show run report to user> shell : bash","title":"Design proposal"},{"location":"flank-github-action/flank_github_acton_sdd/#usage","text":"","title":"Usage"},{"location":"flank-github-action/flank_github_acton_sdd/#with-config-file","text":"on : [ push ] jobs : hello_world_job : runs-on : ubuntu-latest name : A job to say hello steps : - uses : actions/checkout@v2 - id : flank uses : actions/flank@v1 with : version : '21.01.0' platform : 'android' flank_service_account : './service_account.json' flank_configuration_file : './flank.yml' - run : cat ${{ steps.flank.outputs.output_report }} shell : bash","title":"With config file"},{"location":"flank-github-action/flank_github_acton_sdd/#with-options","text":"on : [ push ] jobs : hello_world_job : runs-on : ubuntu-latest name : A job to say hello steps : - uses : actions/checkout@v2 - id : flank uses : actions/flank@v1 with : version : '21.01.0' platform : 'android' flank_service_account : './service_account.json' app : \"../test_projects/android/apks/app-debug.apk\" test : \"../test_projects/android/apks/app-debug-androidTest.apk\" results-dir : test_dir legacy-junit-result : true - run : cat ${{ steps.flank.outputs.output_report }} shell : bash","title":"With options"},{"location":"flank-github-action/flank_github_acton_sdd/#results","text":"After finishing Flank GitHub actions should be published to Github actions marketplace . There should be also an announcement on Flank channel on Slack. The thing to consider is also using it for our internal verification together with integration tests","title":"Results"},{"location":"flank-github-action/flank_github_acton_sdd/#dependencies","text":"Github action needs to have action.yml file in the root of the repository, however, Flank repository has the file for posting Slack message after release. However it is only used by Flank team, so it will be best to move it as a side repository and keep the main Flank action in Flank's mono repository","title":"Dependencies"},{"location":"flank-github-action/flank_github_acton_sdd/#testing","text":"A new testing repository will be set up to test action as described in GitHub actions documentation","title":"Testing"},{"location":"flank-github-action/flank_github_acton_sdd/#alternatives-considered","text":"The investigation was done to choose the best way to develop custom GitHub action. Docker container action is not the best choice, because users are forced to run Flank action on Ubuntu workflow . The second alternative considered was using JavaScript as action language . However, this is another language to maintain in our project and it will not be the best option to choose for such an important thing","title":"Alternatives Considered"},{"location":"flank-github-action/store_documentation/","text":"Action This Github action allows for running Flank as a Github workflow. Documentation for Flank is at flank.github.io/flank Usage Inputs Input name Description Required Default version Flank version to run. Minimal supported version is v21.03.1 . Leaving it blank will fallback to latest version. false latest available service_account Service account to authenticate with. Could be path to file, link to file or file content itself. More information about creating a service account could be found at documentation true platform Platform to run. Could be ios or android true flank_configuration_file Flank configuration file. More information on how it should look like is in documentation true Outputs Output name Description gcloud_results_directory Link to Gcloud store where results are stored. local_results_directory Path to local results directory. All output files from this run are stored inside. You could use it as output artifacts. Adding to workflows - name : <your name> id : <id of action> uses : Flank/flank@master with : version : <Flank version to run, minimum supported is v21.03.1, default latest> service_account : <file content, file link or path to file with service account> platform : [ android|ios ] flank_configuration_file : <Path to configuration file> Example workflows Service account file content from secrets - name : flank run id : flank_run uses : Flank/flank@master with : # Flank version to run version : v21.03.1 # Service account file content from secrets service_account : ${{ secrets.SERVICE_ACCOUNT }} # Run Android tests platform : android # Path to configuration file from local repo flank_configuration_file : './testing/android/flank-simple-success.yml' - name : output run : | # Use local directory output echo \"Local directory: ${{ steps.flank_run.outputs.local_results_directory }}\" # Use Gcloud storage output echo \"Gcloud: ${{ steps.flank_run.outputs.gcloud_results_directory }}\" Service account file from repository - name : flank run id : flank_run uses : Flank/flank@master with : # Service account file from repository service_account : './service_account.json' # Run Android tests platform : android # Path to configuration file from local repo flank_configuration_file : './testing/android/flank-simple-success.yml' - name : output run : | # Use local directory output echo \"Local directory: ${{ steps.flank_run.outputs.local_results_directory }}\" # Use Gcloud storage output echo \"Gcloud: ${{ steps.flank_run.outputs.gcloud_results_directory }}\" Create service account during workflow - name : Create service account run : echo '${{ secrets.SERVICE_ACCOUNT }}' > service_account_created.json - name : flank run id : flank_run uses : Flank/flank@master with : # Service account created in previous step service_account : './service_account_created.json' # Run Android tests platform : android # Path to configuration file from local repo flank_configuration_file : './testing/android/flank-simple-success.yml' - name : output run : | # Use local directory output echo \"Local directory: ${{ steps.flank_run.outputs.local_results_directory }}\" # Use Gcloud storage output echo \"Gcloud: ${{ steps.flank_run.outputs.gcloud_results_directory }}\" Runner OS support All 3 runner operating systems are supported.","title":"Store documentation"},{"location":"flank-github-action/store_documentation/#action","text":"This Github action allows for running Flank as a Github workflow. Documentation for Flank is at flank.github.io/flank","title":"Action"},{"location":"flank-github-action/store_documentation/#usage","text":"","title":"Usage"},{"location":"flank-github-action/store_documentation/#inputs","text":"Input name Description Required Default version Flank version to run. Minimal supported version is v21.03.1 . Leaving it blank will fallback to latest version. false latest available service_account Service account to authenticate with. Could be path to file, link to file or file content itself. More information about creating a service account could be found at documentation true platform Platform to run. Could be ios or android true flank_configuration_file Flank configuration file. More information on how it should look like is in documentation true","title":"Inputs"},{"location":"flank-github-action/store_documentation/#outputs","text":"Output name Description gcloud_results_directory Link to Gcloud store where results are stored. local_results_directory Path to local results directory. All output files from this run are stored inside. You could use it as output artifacts.","title":"Outputs"},{"location":"flank-github-action/store_documentation/#adding-to-workflows","text":"- name : <your name> id : <id of action> uses : Flank/flank@master with : version : <Flank version to run, minimum supported is v21.03.1, default latest> service_account : <file content, file link or path to file with service account> platform : [ android|ios ] flank_configuration_file : <Path to configuration file>","title":"Adding to workflows"},{"location":"flank-github-action/store_documentation/#example-workflows","text":"","title":"Example workflows"},{"location":"flank-github-action/store_documentation/#service-account-file-content-from-secrets","text":"- name : flank run id : flank_run uses : Flank/flank@master with : # Flank version to run version : v21.03.1 # Service account file content from secrets service_account : ${{ secrets.SERVICE_ACCOUNT }} # Run Android tests platform : android # Path to configuration file from local repo flank_configuration_file : './testing/android/flank-simple-success.yml' - name : output run : | # Use local directory output echo \"Local directory: ${{ steps.flank_run.outputs.local_results_directory }}\" # Use Gcloud storage output echo \"Gcloud: ${{ steps.flank_run.outputs.gcloud_results_directory }}\"","title":"Service account file content from secrets"},{"location":"flank-github-action/store_documentation/#service-account-file-from-repository","text":"- name : flank run id : flank_run uses : Flank/flank@master with : # Service account file from repository service_account : './service_account.json' # Run Android tests platform : android # Path to configuration file from local repo flank_configuration_file : './testing/android/flank-simple-success.yml' - name : output run : | # Use local directory output echo \"Local directory: ${{ steps.flank_run.outputs.local_results_directory }}\" # Use Gcloud storage output echo \"Gcloud: ${{ steps.flank_run.outputs.gcloud_results_directory }}\"","title":"Service account file from repository"},{"location":"flank-github-action/store_documentation/#create-service-account-during-workflow","text":"- name : Create service account run : echo '${{ secrets.SERVICE_ACCOUNT }}' > service_account_created.json - name : flank run id : flank_run uses : Flank/flank@master with : # Service account created in previous step service_account : './service_account_created.json' # Run Android tests platform : android # Path to configuration file from local repo flank_configuration_file : './testing/android/flank-simple-success.yml' - name : output run : | # Use local directory output echo \"Local directory: ${{ steps.flank_run.outputs.local_results_directory }}\" # Use Gcloud storage output echo \"Gcloud: ${{ steps.flank_run.outputs.gcloud_results_directory }}\"","title":"Create service account during workflow"},{"location":"flank-github-action/store_documentation/#runner-os-support","text":"All 3 runner operating systems are supported.","title":"Runner OS support"},{"location":"flank-output-investigation/flank_current_output/","text":"Flank output Android Version section version: local_snapshot revision: d5e7c76ab206373c6cdf0c1a25e2575f323e5929 session id: 7a7ceda1-658e-461f-bd37-04f2b2a90a37 Args section AndroidArgs gcloud: results-bucket: test-lab-v9cn46bb990nx-kz69ymd4nm9aq results-dir: 2021 -03-01_13-01-56.722225_lYrQ record-video: false timeout: 15m async: false client-details: network-profile: null results-history-name: null # Android gcloud app: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/apk/app-single-success-debug-androidTest.apk additional-apks: auto-google-login: false use-orchestrator: true directories-to-pull: grant-permissions: all type: null other-files: scenario-numbers: scenario-labels: obb-files: obb-names: performance-metrics: false num-uniform-shards: null test-runner-class: null test-targets: robo-directives: robo-script: null device: - model: NexusLowRes version: 28 locale: en orientation: portrait num-flaky-test-attempts: 0 test-targets-for-shard: fail-fast: false flank: max-test-shards: 1 shard-time: -1 num-test-runs: 1 smart-flank-gcs-path: smart-flank-disable-upload: false default-test-time: 120 .0 use-average-test-time-for-new-tests: false files-to-download: test-targets-always-run: disable-sharding: true project: flank-open-source local-result-dir: results full-junit-result: false # Android Flank Yml keep-file-path: false additional-app-test-apks: run-timeout: -1 legacy-junit-result: false ignore-failed-tests: false output-style: single disable-results-upload: false default-class-test-time: 240 .0 disable-usage-statistics: false output-report: none Run tests section RunTests Saved 1 shards to /Users/adamfilipowicz/Repos/flank/results/2021-03-01_13-01-56.722225_lYrQ/android_shards.json Uploading [ android_shards.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ app-debug.apk ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ app-single-success-debug-androidTest.apk ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... 1 test / 1 shard Uploading [ session_id.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... 1 matrix ids created in 0m 5s Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ ] Matrices webLink section matrix-1kozhsv2imkru https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.da0c237aaa33732/matrices/8233077922466140188/executions/bs.d3f60304f671ce86 Test status 3m 11s Test executions status: FINISHED:1 3m 11s matrix-1kozhsv2imkru FINISHED Cost report section CostReport Virtual devices $0 .02 for 1m Uploading [ CostReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Results section MatrixResultsReport 1 / 1 ( 100 .00% ) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-1kozhsv2imkru \u2502 NexusLowRes-28-en-portrait \u2502 1 test cases passed, 1 skipped \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Uploading [ MatrixResultsReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ JUnitReport.xml ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ matrix_ids.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... FetchArtifacts Updating matrix file Matrices webLink matrix-1kozhsv2imkru https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.da0c237aaa33732/matrices/8233077922466140188/executions/bs.d3f60304f671ce86 Ios Version section version: local_snapshot revision: d5e7c76ab206373c6cdf0c1a25e2575f323e5929 session id: 7a7ceda1-658e-461f-bd37-04f2b2a90a37 Args section IosArgs gcloud: results-bucket: test-lab-v9cn46bb990nx-kz69ymd4nm9aq results-dir: test_dir record-video: false timeout: 15m async: false client-details: network-profile: null results-history-name: null # iOS gcloud test: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/EarlGreyExample.zip xctestrun-file: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/EarlGreyExampleSwiftTests.xctestrun xcode-version: null device: - model: iphone8 version: 13 .6 locale: en orientation: portrait num-flaky-test-attempts: 0 directories-to-pull: other-files: additional-ipas: scenario-numbers: type: xctest app: test-special-entitlements: false fail-fast: false flank: max-test-shards: 1 shard-time: -1 num-test-runs: 1 smart-flank-gcs-path: smart-flank-disable-upload: false default-test-time: 120 .0 use-average-test-time-for-new-tests: false test-targets-always-run: files-to-download: keep-file-path: false full-junit-result: false # iOS flank test-targets: disable-sharding: false project: flank-open-source local-result-dir: results run-timeout: -1 ignore-failed-tests: false output-style: single disable-results-upload: false default-class-test-time: 240 .0 disable-usage-statistics: false only-test-configuration: skip-test-configuration: output-report: none WARNING: Google cloud storage result directory should be unique, otherwise results from multiple test matrices will be overwritten or intermingled Run tests section RunTests Found xctest: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/Debug-iphoneos/EarlGreyExampleSwift.app/PlugIns/EarlGreyExampleSwiftTests.xctest isMacOS = true ( mac os x ) nm -U \"/Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/Debug-iphoneos/EarlGreyExampleSwift.app/PlugIns/EarlGreyExampleSwiftTests.xctest/EarlGreyExampleSwiftTests\" nm -gU \"/Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/Debug-iphoneos/EarlGreyExampleSwift.app/PlugIns/EarlGreyExampleSwiftTests.xctest/EarlGreyExampleSwiftTests\" | xargs -s 262144 xcrun swift-demangle Smart Flank cache hit: 0 % ( 0 / 17 ) Shard times: 2040s Saved 1 shards to /Users/adamfilipowicz/Repos/flank/results/test_dir/ios_shards.json Uploading [ ios_shards.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... 17 tests / 1 shard Uploading [ EarlGreyExample.zip ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/..... Uploading [ EarlGreyExampleSwiftTests_shard_0.xctestrun ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/shard_0/... Uploading [ session_id.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... 1 matrix ids created in 0m 29s Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir ] Matrices webLink section Matrices webLink matrix-3d331uzs97mlt https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.a3b607c9bb6d0088/matrices/5523514684002242128/executions/bs.70a259d113387c0c Test status 3m 11s Test executions status: FINISHED:1 3m 11s matrix-1kozhsv2imkru FINISHED Cost report section CostReport Physical devices $0 .08 for 1m Uploading [ CostReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... Results section MatrixResultsReport 1 / 1 ( 100 .00% ) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-3d331uzs97mlt \u2502 iphone8-13.6-en-portrait \u2502 17 test cases passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Uploading [ MatrixResultsReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... Uploading [ JUnitReport.xml ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... Uploading [ matrix_ids.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... FetchArtifacts Updating matrix file Matrices webLink matrix-3d331uzs97mlt https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.a3b607c9bb6d0088/matrices/5523514684002242128/executions/bs.70a259d113387c0c","title":"Flank output"},{"location":"flank-output-investigation/flank_current_output/#flank-output","text":"","title":"Flank output"},{"location":"flank-output-investigation/flank_current_output/#android","text":"","title":"Android"},{"location":"flank-output-investigation/flank_current_output/#version-section","text":"version: local_snapshot revision: d5e7c76ab206373c6cdf0c1a25e2575f323e5929 session id: 7a7ceda1-658e-461f-bd37-04f2b2a90a37","title":"Version section"},{"location":"flank-output-investigation/flank_current_output/#args-section","text":"AndroidArgs gcloud: results-bucket: test-lab-v9cn46bb990nx-kz69ymd4nm9aq results-dir: 2021 -03-01_13-01-56.722225_lYrQ record-video: false timeout: 15m async: false client-details: network-profile: null results-history-name: null # Android gcloud app: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/apk/app-single-success-debug-androidTest.apk additional-apks: auto-google-login: false use-orchestrator: true directories-to-pull: grant-permissions: all type: null other-files: scenario-numbers: scenario-labels: obb-files: obb-names: performance-metrics: false num-uniform-shards: null test-runner-class: null test-targets: robo-directives: robo-script: null device: - model: NexusLowRes version: 28 locale: en orientation: portrait num-flaky-test-attempts: 0 test-targets-for-shard: fail-fast: false flank: max-test-shards: 1 shard-time: -1 num-test-runs: 1 smart-flank-gcs-path: smart-flank-disable-upload: false default-test-time: 120 .0 use-average-test-time-for-new-tests: false files-to-download: test-targets-always-run: disable-sharding: true project: flank-open-source local-result-dir: results full-junit-result: false # Android Flank Yml keep-file-path: false additional-app-test-apks: run-timeout: -1 legacy-junit-result: false ignore-failed-tests: false output-style: single disable-results-upload: false default-class-test-time: 240 .0 disable-usage-statistics: false output-report: none","title":"Args section"},{"location":"flank-output-investigation/flank_current_output/#run-tests-section","text":"RunTests Saved 1 shards to /Users/adamfilipowicz/Repos/flank/results/2021-03-01_13-01-56.722225_lYrQ/android_shards.json Uploading [ android_shards.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ app-debug.apk ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ app-single-success-debug-androidTest.apk ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... 1 test / 1 shard Uploading [ session_id.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... 1 matrix ids created in 0m 5s Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ ]","title":"Run tests section"},{"location":"flank-output-investigation/flank_current_output/#matrices-weblink-section","text":"matrix-1kozhsv2imkru https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.da0c237aaa33732/matrices/8233077922466140188/executions/bs.d3f60304f671ce86","title":"Matrices webLink section"},{"location":"flank-output-investigation/flank_current_output/#test-status","text":"3m 11s Test executions status: FINISHED:1 3m 11s matrix-1kozhsv2imkru FINISHED","title":"Test status"},{"location":"flank-output-investigation/flank_current_output/#cost-report-section","text":"CostReport Virtual devices $0 .02 for 1m Uploading [ CostReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/...","title":"Cost report section"},{"location":"flank-output-investigation/flank_current_output/#results-section","text":"MatrixResultsReport 1 / 1 ( 100 .00% ) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-1kozhsv2imkru \u2502 NexusLowRes-28-en-portrait \u2502 1 test cases passed, 1 skipped \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Uploading [ MatrixResultsReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ JUnitReport.xml ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ matrix_ids.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... FetchArtifacts Updating matrix file Matrices webLink matrix-1kozhsv2imkru https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.da0c237aaa33732/matrices/8233077922466140188/executions/bs.d3f60304f671ce86","title":"Results section"},{"location":"flank-output-investigation/flank_current_output/#ios","text":"","title":"Ios"},{"location":"flank-output-investigation/flank_current_output/#version-section_1","text":"version: local_snapshot revision: d5e7c76ab206373c6cdf0c1a25e2575f323e5929 session id: 7a7ceda1-658e-461f-bd37-04f2b2a90a37","title":"Version section"},{"location":"flank-output-investigation/flank_current_output/#args-section_1","text":"IosArgs gcloud: results-bucket: test-lab-v9cn46bb990nx-kz69ymd4nm9aq results-dir: test_dir record-video: false timeout: 15m async: false client-details: network-profile: null results-history-name: null # iOS gcloud test: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/EarlGreyExample.zip xctestrun-file: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/EarlGreyExampleSwiftTests.xctestrun xcode-version: null device: - model: iphone8 version: 13 .6 locale: en orientation: portrait num-flaky-test-attempts: 0 directories-to-pull: other-files: additional-ipas: scenario-numbers: type: xctest app: test-special-entitlements: false fail-fast: false flank: max-test-shards: 1 shard-time: -1 num-test-runs: 1 smart-flank-gcs-path: smart-flank-disable-upload: false default-test-time: 120 .0 use-average-test-time-for-new-tests: false test-targets-always-run: files-to-download: keep-file-path: false full-junit-result: false # iOS flank test-targets: disable-sharding: false project: flank-open-source local-result-dir: results run-timeout: -1 ignore-failed-tests: false output-style: single disable-results-upload: false default-class-test-time: 240 .0 disable-usage-statistics: false only-test-configuration: skip-test-configuration: output-report: none WARNING: Google cloud storage result directory should be unique, otherwise results from multiple test matrices will be overwritten or intermingled","title":"Args section"},{"location":"flank-output-investigation/flank_current_output/#run-tests-section_1","text":"RunTests Found xctest: /Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/Debug-iphoneos/EarlGreyExampleSwift.app/PlugIns/EarlGreyExampleSwiftTests.xctest isMacOS = true ( mac os x ) nm -U \"/Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/Debug-iphoneos/EarlGreyExampleSwift.app/PlugIns/EarlGreyExampleSwiftTests.xctest/EarlGreyExampleSwiftTests\" nm -gU \"/Users/adamfilipowicz/Repos/flank/test_runner/src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/Debug-iphoneos/EarlGreyExampleSwift.app/PlugIns/EarlGreyExampleSwiftTests.xctest/EarlGreyExampleSwiftTests\" | xargs -s 262144 xcrun swift-demangle Smart Flank cache hit: 0 % ( 0 / 17 ) Shard times: 2040s Saved 1 shards to /Users/adamfilipowicz/Repos/flank/results/test_dir/ios_shards.json Uploading [ ios_shards.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... 17 tests / 1 shard Uploading [ EarlGreyExample.zip ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/..... Uploading [ EarlGreyExampleSwiftTests_shard_0.xctestrun ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/shard_0/... Uploading [ session_id.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... 1 matrix ids created in 0m 29s Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir ]","title":"Run tests section"},{"location":"flank-output-investigation/flank_current_output/#matrices-weblink-section_1","text":"Matrices webLink matrix-3d331uzs97mlt https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.a3b607c9bb6d0088/matrices/5523514684002242128/executions/bs.70a259d113387c0c","title":"Matrices webLink section"},{"location":"flank-output-investigation/flank_current_output/#test-status_1","text":"3m 11s Test executions status: FINISHED:1 3m 11s matrix-1kozhsv2imkru FINISHED","title":"Test status"},{"location":"flank-output-investigation/flank_current_output/#cost-report-section_1","text":"CostReport Physical devices $0 .08 for 1m Uploading [ CostReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/...","title":"Cost report section"},{"location":"flank-output-investigation/flank_current_output/#results-section_1","text":"MatrixResultsReport 1 / 1 ( 100 .00% ) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-3d331uzs97mlt \u2502 iphone8-13.6-en-portrait \u2502 17 test cases passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Uploading [ MatrixResultsReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... Uploading [ JUnitReport.xml ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... Uploading [ matrix_ids.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/test_dir/... FetchArtifacts Updating matrix file Matrices webLink matrix-3d331uzs97mlt https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.a3b607c9bb6d0088/matrices/5523514684002242128/executions/bs.70a259d113387c0c","title":"Results section"},{"location":"flank-output-investigation/flank_output_propositon/","text":"Flank output proposition Android Version section version: local_snapshot revision: d5e7c76ab206373c6cdf0c1a25e2575f323e5929 session id: 7a7ceda1-658e-461f-bd37-04f2b2a90a37 Args section <Diff of arguments> Dump shards [ Dump shards ] Saved 1 shards to /Users/adamfilipowicz/Repos/flank/results/2021-03-01_13-01-56.722225_lYrQ/android_shards.json Uploading [ android_shards.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading files [ Uploading files ] Uploading [ app-debug.apk ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ app-single-success-debug-androidTest.apk ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ session_id.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Matrix info [ Matrix info ] Found 1 test / 1 shard 1 matrix ids created in 0m 5s matrix-1kozhsv2imkru https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.da0c237aaa33732/matrices/8233077922466140188/executions/bs.d3f60304f671ce86 Storage information Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ ] Test status [ Execution statuses ] 3m 11s Test executions status: FINISHED:1 3m 11s matrix-1kozhsv2imkru FINISHED Cost report section [ CostReport ] Virtual devices $0 .02 for 1m Uploading [ CostReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Results section [ MatrixResultsReport ] 1 / 1 ( 100 .00% ) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-1kozhsv2imkru \u2502 NexusLowRes-28-en-portrait \u2502 1 test cases passed, 1 skipped \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Uploading [ MatrixResultsReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ JUnitReport.xml ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ matrix_ids.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... matrix-1kozhsv2imkru https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.da0c237aaa33732/matrices/8233077922466140188/executions/bs.d3f60304f671ce86","title":"Flank output proposition"},{"location":"flank-output-investigation/flank_output_propositon/#flank-output-proposition","text":"","title":"Flank output proposition"},{"location":"flank-output-investigation/flank_output_propositon/#android","text":"","title":"Android"},{"location":"flank-output-investigation/flank_output_propositon/#version-section","text":"version: local_snapshot revision: d5e7c76ab206373c6cdf0c1a25e2575f323e5929 session id: 7a7ceda1-658e-461f-bd37-04f2b2a90a37","title":"Version section"},{"location":"flank-output-investigation/flank_output_propositon/#args-section","text":"<Diff of arguments>","title":"Args section"},{"location":"flank-output-investigation/flank_output_propositon/#dump-shards","text":"[ Dump shards ] Saved 1 shards to /Users/adamfilipowicz/Repos/flank/results/2021-03-01_13-01-56.722225_lYrQ/android_shards.json Uploading [ android_shards.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/...","title":"Dump shards"},{"location":"flank-output-investigation/flank_output_propositon/#uploading-files","text":"[ Uploading files ] Uploading [ app-debug.apk ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ app-single-success-debug-androidTest.apk ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ session_id.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/...","title":"Uploading files"},{"location":"flank-output-investigation/flank_output_propositon/#matrix-info","text":"[ Matrix info ] Found 1 test / 1 shard 1 matrix ids created in 0m 5s matrix-1kozhsv2imkru https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.da0c237aaa33732/matrices/8233077922466140188/executions/bs.d3f60304f671ce86","title":"Matrix info"},{"location":"flank-output-investigation/flank_output_propositon/#storage-information","text":"Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ ]","title":"Storage information"},{"location":"flank-output-investigation/flank_output_propositon/#test-status","text":"[ Execution statuses ] 3m 11s Test executions status: FINISHED:1 3m 11s matrix-1kozhsv2imkru FINISHED","title":"Test status"},{"location":"flank-output-investigation/flank_output_propositon/#cost-report-section","text":"[ CostReport ] Virtual devices $0 .02 for 1m Uploading [ CostReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/...","title":"Cost report section"},{"location":"flank-output-investigation/flank_output_propositon/#results-section","text":"[ MatrixResultsReport ] 1 / 1 ( 100 .00% ) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-1kozhsv2imkru \u2502 NexusLowRes-28-en-portrait \u2502 1 test cases passed, 1 skipped \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Uploading [ MatrixResultsReport.txt ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ JUnitReport.xml ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... Uploading [ matrix_ids.json ] to https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-03-01_13-01-56.722225_lYrQ/... matrix-1kozhsv2imkru https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.da0c237aaa33732/matrices/8233077922466140188/executions/bs.d3f60304f671ce86","title":"Results section"},{"location":"flank-output-investigation/gcloud_current_output/","text":"Gcloud output Android Welcome section Have questions, feedback, or issues? Get support by visiting: https://firebase.google.com/support/ Upload info Uploading [ ../test_projects/android/apks/app-debug.apk ] to Firebase Test Lab... Uploading [ ../test_projects/android/apks/app-debug-androidTest.apk ] to Firebase Test Lab... Storage information Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-02-28_19:08:02.351035_LadZ/ ] Info before running test Test [matrix-1uohkjt5rsr28] has been created in the Google Cloud. Firebase Test Lab will execute your instrumentation test on 1 device(s). Creating individual test executions...done. Test status Test results will be streamed to [ https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.bf178d418be9d33e/matrices/4823249401837879607 ] . 19 :08:18 Test is Pending 19 :08:46 Starting attempt 1 . 19 :08:46 Started logcat recording. 19 :08:46 Test is Running 19 :08:53 Started crash monitoring. 19 :08:53 Preparing device. 19 :09:00 Logging in to Google account on device. 19 :09:06 Installing apps. 19 :09:06 Retrieving Pre-Test Package Stats information from the device. 19 :09:06 Retrieving Performance Environment information from the device. 19 :09:06 Started crash detection. 19 :09:06 Started Out of memory detection 19 :09:06 Started performance monitoring. 19 :09:06 Started video recording. 19 :09:06 Starting instrumentation test. 19 :09:06 Completed instrumentation test. 19 :09:13 Stopped performance monitoring. 19 :09:13 Retrieving Post-test Package Stats information from the device. 19 :09:13 Logging out of Google account on device. 19 :09:20 Stopped crash monitoring. 19 :09:20 Stopped logcat recording. 19 :09:40 Done. Test time = 1 ( secs ) 19 :09:40 Starting results processing. Attempt: 1 19 :09:47 Completed results processing. Time taken = 4 ( secs ) 19 :09:47 Test is Finished Completion info Instrumentation testing complete. Results More details are available at [ https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.bf178d418be9d33e/matrices/4823249401837879607 ] . \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Passed \u2502 walleye-27-en-portrait \u2502 1 test cases passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Survey at the end To take a quick anonymous survey, run: $ gcloud survey iOS Welcome section Have questions, feedback, or issues? Get support by emailing: ftl-ios-feedback@google.com Upload info Uploading [ ./src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/EarlGreyExample.zip ] to Firebase Test Lab... Uploading [ ./src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/EarlGreyExampleSwiftTests.xctestrun ] to Firebase Test Lab... Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-02-28_19:15:20.392870_nuvf/ ] Storage information Test [ matrix-2k6ldtp789lvy ] has been created in the Google Cloud. Firebase Test Lab will execute your xctest test on 1 device ( s ) . Creating individual test executions...done. Info before running test Test [matrix-2k6ldtp789lvy] has been created in the Google Cloud. Firebase Test Lab will execute your xctest test on 1 device(s). Creating individual test executions...done. Test status Test results will be streamed to [ https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.a3b607c9bb6d0088/matrices/7622498233210161623 ] . 19 :15:49 Test is Pending 19 :16:44 Starting attempt 1 . 19 :16:44 Checking Internet connection... 19 :16:44 Test is Running 19 :18:22 Internet connection stable! 19 :18:41 Started device logs task 19 :20:20 Stopped device logs task 19 :20:39 Done. Test time = 62 ( secs ) 19 :20:39 Starting results processing. Attempt: 1 19 :20:45 Completed results processing. Time taken = 5 ( secs ) 19 :20:45 Test is Finished Completion info Xctest testing complete. Results More details are available at [ https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.a3b607c9bb6d0088/matrices/7622498233210161623 ] . \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Passed \u2502 iphone8-11.2-en-portrait \u2502 17 test cases passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Gcloud output"},{"location":"flank-output-investigation/gcloud_current_output/#gcloud-output","text":"","title":"Gcloud output"},{"location":"flank-output-investigation/gcloud_current_output/#android","text":"","title":"Android"},{"location":"flank-output-investigation/gcloud_current_output/#welcome-section","text":"Have questions, feedback, or issues? Get support by visiting: https://firebase.google.com/support/","title":"Welcome section"},{"location":"flank-output-investigation/gcloud_current_output/#upload-info","text":"Uploading [ ../test_projects/android/apks/app-debug.apk ] to Firebase Test Lab... Uploading [ ../test_projects/android/apks/app-debug-androidTest.apk ] to Firebase Test Lab...","title":"Upload info"},{"location":"flank-output-investigation/gcloud_current_output/#storage-information","text":"Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-02-28_19:08:02.351035_LadZ/ ]","title":"Storage information"},{"location":"flank-output-investigation/gcloud_current_output/#info-before-running-test","text":"Test [matrix-1uohkjt5rsr28] has been created in the Google Cloud. Firebase Test Lab will execute your instrumentation test on 1 device(s). Creating individual test executions...done.","title":"Info before running test"},{"location":"flank-output-investigation/gcloud_current_output/#test-status","text":"Test results will be streamed to [ https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.bf178d418be9d33e/matrices/4823249401837879607 ] . 19 :08:18 Test is Pending 19 :08:46 Starting attempt 1 . 19 :08:46 Started logcat recording. 19 :08:46 Test is Running 19 :08:53 Started crash monitoring. 19 :08:53 Preparing device. 19 :09:00 Logging in to Google account on device. 19 :09:06 Installing apps. 19 :09:06 Retrieving Pre-Test Package Stats information from the device. 19 :09:06 Retrieving Performance Environment information from the device. 19 :09:06 Started crash detection. 19 :09:06 Started Out of memory detection 19 :09:06 Started performance monitoring. 19 :09:06 Started video recording. 19 :09:06 Starting instrumentation test. 19 :09:06 Completed instrumentation test. 19 :09:13 Stopped performance monitoring. 19 :09:13 Retrieving Post-test Package Stats information from the device. 19 :09:13 Logging out of Google account on device. 19 :09:20 Stopped crash monitoring. 19 :09:20 Stopped logcat recording. 19 :09:40 Done. Test time = 1 ( secs ) 19 :09:40 Starting results processing. Attempt: 1 19 :09:47 Completed results processing. Time taken = 4 ( secs ) 19 :09:47 Test is Finished","title":"Test status"},{"location":"flank-output-investigation/gcloud_current_output/#completion-info","text":"Instrumentation testing complete.","title":"Completion info"},{"location":"flank-output-investigation/gcloud_current_output/#results","text":"More details are available at [ https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.bf178d418be9d33e/matrices/4823249401837879607 ] . \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Passed \u2502 walleye-27-en-portrait \u2502 1 test cases passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Results"},{"location":"flank-output-investigation/gcloud_current_output/#survey-at-the-end","text":"To take a quick anonymous survey, run: $ gcloud survey","title":"Survey at the end"},{"location":"flank-output-investigation/gcloud_current_output/#ios","text":"","title":"iOS"},{"location":"flank-output-investigation/gcloud_current_output/#welcome-section_1","text":"Have questions, feedback, or issues? Get support by emailing: ftl-ios-feedback@google.com","title":"Welcome section"},{"location":"flank-output-investigation/gcloud_current_output/#upload-info_1","text":"Uploading [ ./src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/EarlGreyExample.zip ] to Firebase Test Lab... Uploading [ ./src/test/kotlin/ftl/fixtures/tmp/ios/EarlGreyExample/EarlGreyExampleSwiftTests.xctestrun ] to Firebase Test Lab... Raw results will be stored in your GCS bucket at [ https://console.developers.google.com/storage/browser/test-lab-v9cn46bb990nx-kz69ymd4nm9aq/2021-02-28_19:15:20.392870_nuvf/ ]","title":"Upload info"},{"location":"flank-output-investigation/gcloud_current_output/#storage-information_1","text":"Test [ matrix-2k6ldtp789lvy ] has been created in the Google Cloud. Firebase Test Lab will execute your xctest test on 1 device ( s ) . Creating individual test executions...done.","title":"Storage information"},{"location":"flank-output-investigation/gcloud_current_output/#info-before-running-test_1","text":"Test [matrix-2k6ldtp789lvy] has been created in the Google Cloud. Firebase Test Lab will execute your xctest test on 1 device(s). Creating individual test executions...done.","title":"Info before running test"},{"location":"flank-output-investigation/gcloud_current_output/#test-status_1","text":"Test results will be streamed to [ https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.a3b607c9bb6d0088/matrices/7622498233210161623 ] . 19 :15:49 Test is Pending 19 :16:44 Starting attempt 1 . 19 :16:44 Checking Internet connection... 19 :16:44 Test is Running 19 :18:22 Internet connection stable! 19 :18:41 Started device logs task 19 :20:20 Stopped device logs task 19 :20:39 Done. Test time = 62 ( secs ) 19 :20:39 Starting results processing. Attempt: 1 19 :20:45 Completed results processing. Time taken = 5 ( secs ) 19 :20:45 Test is Finished","title":"Test status"},{"location":"flank-output-investigation/gcloud_current_output/#completion-info_1","text":"Xctest testing complete.","title":"Completion info"},{"location":"flank-output-investigation/gcloud_current_output/#results_1","text":"More details are available at [ https://console.firebase.google.com/project/flank-open-source/testlab/histories/bh.a3b607c9bb6d0088/matrices/7622498233210161623 ] . \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 TEST_AXIS_VALUE \u2502 TEST_DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Passed \u2502 iphone8-11.2-en-portrait \u2502 17 test cases passed \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Results"},{"location":"flank-scripts/","text":"flank-scripts This repository contains helper scripts for developing flank. For now, it contains just release related scripts. Build and usage Build To build flank-scripts: - Run script buildFlankScripts.sh in flank-scripts/bash/ directory - Run command ./gradlew clean flank-scripts:assemble flank-scripts:shadowJar and manual copy file from /flank-scripts/build/libs/flank-scripts.jar to flank-scripts/bash/ - You could always run/build it from Intellij IDEA Usage Run the script with arguments flankScripts <command group> [<subgroup>] <command name> [<arguments>] If you need help with available commands or arguments you could always use the option --help Available commands and options Command List assemble - Group of commands to assemble application android - Subgroup of commands for Android test application assembly app - Assemble Android test application ios - Subgroup of commands for iOS test applications assembly earl_grey - Assemble iOS earl grey application example - Assemble iOS example application flank_example - Assemble iOS flank example application ftl - Assemble iOS ftl example application game_loop - Assemble iOS game loop application test_plans - Assemble iOS test plans application all - Assemble all iOS applications flank - Build Flank go_artifacts - Generate go artifacts dependencies - Group of commands related to dependencies tasks install_xcpretty - Install xcpretty formatter setup_ios_env - Setup iOS environment universal_framework_files - Create Universal Framework files update_binaries - Update binaries used by Flank update - Update repository 3rd party dependencies firebase - Group of commands for managing firebase integrations check_for_sdk_updates - Check for new SDK features and create update tasks for it generate_client - Generate Java Client based on api schema update_api - Update api schema github - Group of command for managing Github integration copy_issue_properties - Copy properties(assignees, story points, labels) from issue to pull request delete_old_tag - Delete old tag on GitHub delete_release - Delete old release on github make_release - Make new Github release integration_tests - Group of commands for handling integration tests (1) process_results - Process results of integration tests linter - Group of commands used for applying correct coding style apply_to_git_hooks - Apply Linter pre-commit hook apply_to_ide - Apply Linter to IDE release - Group of commands for creating Flank release delete_snapshot - Delete snapshot package from artifacts repository generate_release_notes - Generate release notes next_tag - Get tag for next release sync_with_maven_central - Sync artifact's repository with Maven central test_artifacts - Group of commands for artifacts management download - Download test artifacts zip asset to test_artifacts directory. link - Create symbolic link to under test_runner/src/test/kotlin/ftl/fixtures/tmp to test_artifacts/{branchName}. prepare - Creates a fresh copy of test artifacts for the current working branch, basing on an existing one. remove_remote - Remove remote copy of test artifacts. resolve - Automatically prepare local artifacts if needed. unzip - Unpack test artifacts zip archive. upload - Upload test artifacts zip as github release asset. zip - Create zip archive from test artifacts directory. (1) - please note that there is only one command, but it may change in the future. Arguments To show applicable arguments for command use --help or -h options: flankScripts <command group> [<subgroup>] <command name> --help or flankScripts <command group> [<subgroup>] <command name> -h Testing To test your script with different settings use the flank-debug.properties file. Uncomment and replace with desired values. Properties are skipped by git and should not be attached to a commit. Note, the test task ignores your own properties and will use the default. List of possible configs Key Description Default value zenhub.repo-id Flank's repo ID in zenhub app. Used to create new epics. 84221974 repo.flank Flank test runner repo. Essential property for github client. Flank/flank repo.gcloud_cli Flank's fork of gcloud sdk repo. Flank/gcloud_cli repo.test-artifacts Flank's source of artifacts (apks, binaries, etc) used for testing Flank/test_artifacts integration.workflow-filename GH Action integration tests workflow file. Used to fetch list of commits since it's last run. full_suite_integration_tests.yml integration.issue-poster Name of account that creates IT issue github-actions[bot] sdk-check.workflow-filename GH Action dependencies update workflow file. Used to fetch list of commits since it's last run. update_dependencies_and_client.yml sdk-check.issue-poster Name of account that creates dependencies issues/epics github-actions[bot] Directory structure . \u251c\u2500\u2500 cli \u2502 \u251c\u2500\u2500 Main.kt \u2502 \u251c\u2500\u2500 assemble \u2502 \u2502 \u251c\u2500\u2500 AssembleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 FlankCommand.kt \u2502 \u2502 \u251c\u2500\u2500 GoCommand.kt \u2502 \u2502 \u251c\u2500\u2500 android \u2502 \u2502 \u2502 \u251c\u2500\u2500 AndroidCommand.kt \u2502 \u2502 \u2502 \u2514\u2500\u2500 AppCommand.kt \u2502 \u2502 \u2514\u2500\u2500 ios \u2502 \u2502 \u251c\u2500\u2500 EarlGreyCommand.kt \u2502 \u2502 \u251c\u2500\u2500 ExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 FlankExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 FtlCommand.kt \u2502 \u2502 \u251c\u2500\u2500 GameLoopExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 IosCommand.kt \u2502 \u2502 \u251c\u2500\u2500 RunFtlLocalCommand.kt \u2502 \u2502 \u2514\u2500\u2500 TestPlansExample.kt \u2502 \u251c\u2500\u2500 dependencies \u2502 \u2502 \u251c\u2500\u2500 DependenciesCommand.kt \u2502 \u2502 \u251c\u2500\u2500 InstallXcPrettyCommand.kt \u2502 \u2502 \u251c\u2500\u2500 SetupIosEnvCommand.kt \u2502 \u2502 \u251c\u2500\u2500 UniversalFrameworkCommand.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateBinariesCommand.kt \u2502 \u2502 \u2514\u2500\u2500 UpdateCommand.kt \u2502 \u251c\u2500\u2500 firebase \u2502 \u2502 \u251c\u2500\u2500 CheckForSdkUpdatesCommand.kt \u2502 \u2502 \u251c\u2500\u2500 FirebaseCommand.kt \u2502 \u2502 \u251c\u2500\u2500 GenerateClientCommand.kt \u2502 \u2502 \u2514\u2500\u2500 UpdateApiCommand.kt \u2502 \u251c\u2500\u2500 github \u2502 \u2502 \u251c\u2500\u2500 CopyIssuePropertiesCommand.kt \u2502 \u2502 \u251c\u2500\u2500 DeleteOldTagCommand.kt \u2502 \u2502 \u251c\u2500\u2500 DeleteReleaseCommand.kt \u2502 \u2502 \u251c\u2500\u2500 DownloadFlankCommand.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubCommand.kt \u2502 \u2502 \u2514\u2500\u2500 MakeReleaseCommand.kt \u2502 \u251c\u2500\u2500 integrationtests \u2502 \u2502 \u251c\u2500\u2500 IntegrationTestsCommand.kt \u2502 \u2502 \u2514\u2500\u2500 ProcessResultCommand.kt \u2502 \u251c\u2500\u2500 linter \u2502 \u2502 \u251c\u2500\u2500 ApplyToGitHooksCommand.kt \u2502 \u2502 \u251c\u2500\u2500 ApplyToIdeCommand.kt \u2502 \u2502 \u2514\u2500\u2500 LinterCommand.kt \u2502 \u251c\u2500\u2500 release \u2502 \u2502 \u251c\u2500\u2500 GenerateReleaseNotesCommand.kt \u2502 \u2502 \u251c\u2500\u2500 NextTagCommand.kt \u2502 \u2502 \u2514\u2500\u2500 ReleaseCommand.kt \u2502 \u2514\u2500\u2500 testartifacts \u2502 \u251c\u2500\u2500 DownloadCommand.kt \u2502 \u251c\u2500\u2500 LinkCommand.kt \u2502 \u251c\u2500\u2500 PrepareCommand.kt \u2502 \u251c\u2500\u2500 RemoveRemoteCommand.kt \u2502 \u251c\u2500\u2500 ResolveCommand.kt \u2502 \u251c\u2500\u2500 TestArtifactsCommand.kt \u2502 \u251c\u2500\u2500 UnzipCommand.kt \u2502 \u251c\u2500\u2500 UploadCommand.kt \u2502 \u2514\u2500\u2500 ZipCommand.kt \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 github \u2502 \u2502 \u251c\u2500\u2500 GitHubErrorResponse.kt \u2502 \u2502 \u251c\u2500\u2500 GithubApi.kt \u2502 \u2502 \u251c\u2500\u2500 commons \u2502 \u2502 \u2502 \u2514\u2500\u2500 LastWorkflowRunDate.kt \u2502 \u2502 \u2514\u2500\u2500 objects \u2502 \u2502 \u251c\u2500\u2500 GitHubCommit.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubCreateIssue.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubCreateIssueComment.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubRelease.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubSetAssigneesRequest.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubSetLabelsRequest.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubUpdateIssue.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubWorkflowRun.kt \u2502 \u2502 \u2514\u2500\u2500 GithubPullRequest.kt \u2502 \u2514\u2500\u2500 zenhub \u2502 \u251c\u2500\u2500 ZenHubAPI.kt \u2502 \u251c\u2500\u2500 ZenHubIssue.kt \u2502 \u2514\u2500\u2500 objects \u2502 \u2514\u2500\u2500 ConvertToEpicRequest.kt \u251c\u2500\u2500 ops \u2502 \u251c\u2500\u2500 assemble \u2502 \u2502 \u251c\u2500\u2500 BuildFlank.kt \u2502 \u2502 \u251c\u2500\u2500 BuildGo.kt \u2502 \u2502 \u251c\u2500\u2500 android \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildBaseAndroidApk.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildBaseAndroidTests.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildCucumberSampleApk.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildDuplicatedNamesApks.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildMultiModulesApks.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 Common.kt \u2502 \u2502 \u2502 \u2514\u2500\u2500 RunAndroidOps.kt \u2502 \u2502 \u2514\u2500\u2500 ios \u2502 \u2502 \u251c\u2500\u2500 BuildEarlGreyExample.kt \u2502 \u2502 \u251c\u2500\u2500 BuildExample.kt \u2502 \u2502 \u251c\u2500\u2500 BuildFlankExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 BuildFtl.kt \u2502 \u2502 \u251c\u2500\u2500 BuildGameLoopExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 BuildIosIPA.kt \u2502 \u2502 \u251c\u2500\u2500 BuildIosTestArtifacts.kt \u2502 \u2502 \u251c\u2500\u2500 BuildTestPlansExample.kt \u2502 \u2502 \u251c\u2500\u2500 IosBuildCommand.kt \u2502 \u2502 \u251c\u2500\u2500 RunFtlLocal.kt \u2502 \u2502 \u2514\u2500\u2500 UniversalFramework.kt \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u251c\u2500\u2500 DownloadSoftware.kt \u2502 \u2502 \u251c\u2500\u2500 EarlGreyExampleConsts.kt \u2502 \u2502 \u251c\u2500\u2500 GenerateChangeLog.kt \u2502 \u2502 \u2514\u2500\u2500 ReleaseNotesWithType.kt \u2502 \u251c\u2500\u2500 dependencies \u2502 \u2502 \u251c\u2500\u2500 InstallXcPretty.kt \u2502 \u2502 \u251c\u2500\u2500 SetupIosEnv.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateAllDependencies.kt \u2502 \u2502 \u2514\u2500\u2500 common \u2502 \u2502 \u251c\u2500\u2500 DependenciesResultCheck.kt \u2502 \u2502 \u251c\u2500\u2500 DependencyExtensions.kt \u2502 \u2502 \u251c\u2500\u2500 DependencyUpdate.kt \u2502 \u2502 \u251c\u2500\u2500 FindOutdatedDependencies.kt \u2502 \u2502 \u251c\u2500\u2500 GradleDependency.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateDependencies.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateGradle.kt \u2502 \u2502 \u251c\u2500\u2500 UpdatePlugins.kt \u2502 \u2502 \u2514\u2500\u2500 UpdateVersionsInFile.kt \u2502 \u251c\u2500\u2500 firebase \u2502 \u2502 \u251c\u2500\u2500 CheckForSDKUpdate.kt \u2502 \u2502 \u251c\u2500\u2500 CommitList.kt \u2502 \u2502 \u251c\u2500\u2500 GenerateJavaClient.kt \u2502 \u2502 \u251c\u2500\u2500 SDKUpdateContext.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateApiJson.kt \u2502 \u2502 \u2514\u2500\u2500 common \u2502 \u2502 \u2514\u2500\u2500 Extensions.kt \u2502 \u251c\u2500\u2500 github \u2502 \u2502 \u251c\u2500\u2500 CopyGitHubProperties.kt \u2502 \u2502 \u251c\u2500\u2500 DeleteOldRelease.kt \u2502 \u2502 \u251c\u2500\u2500 DeleteOldTag.kt \u2502 \u2502 \u251c\u2500\u2500 DownloadFlank.kt \u2502 \u2502 \u2514\u2500\u2500 ReleaseFlank.kt \u2502 \u251c\u2500\u2500 integrationtests \u2502 \u2502 \u251c\u2500\u2500 ProcessIntegrationTestsResult.kt \u2502 \u2502 \u2514\u2500\u2500 common \u2502 \u2502 \u251c\u2500\u2500 ITResult.kt \u2502 \u2502 \u251c\u2500\u2500 IntegrationResultContext.kt \u2502 \u2502 \u2514\u2500\u2500 PrepareMessage.kt \u2502 \u251c\u2500\u2500 linter \u2502 \u2502 \u251c\u2500\u2500 ApplyKtlintToIdea.kt \u2502 \u2502 \u2514\u2500\u2500 LinkGitHooks.kt \u2502 \u251c\u2500\u2500 release \u2502 \u2502 \u251c\u2500\u2500 CreateReleaseNotes.kt \u2502 \u2502 \u2514\u2500\u2500 NextReleaseTag.kt \u2502 \u251c\u2500\u2500 testartifacts \u2502 \u2502 \u251c\u2500\u2500 ArtifactsArchive.kt \u2502 \u2502 \u251c\u2500\u2500 Context.kt \u2502 \u2502 \u251c\u2500\u2500 DownloadFixtures.kt \u2502 \u2502 \u251c\u2500\u2500 Helpers.kt \u2502 \u2502 \u251c\u2500\u2500 LinkArtifacts.kt \u2502 \u2502 \u251c\u2500\u2500 PrepareTestArtifacts.kt \u2502 \u2502 \u251c\u2500\u2500 RemoveRemoteCopy.kt \u2502 \u2502 \u251c\u2500\u2500 ResolveArtifacts.kt \u2502 \u2502 \u251c\u2500\u2500 UploadFixtures.kt \u2502 \u2502 \u2514\u2500\u2500 ZipArtifacts.kt \u2502 \u2514\u2500\u2500 updatebinaries \u2502 \u251c\u2500\u2500 UpdateAtomic.kt \u2502 \u251c\u2500\u2500 UpdateBinaries.kt \u2502 \u251c\u2500\u2500 UpdateLlvm.kt \u2502 \u2514\u2500\u2500 UpdateSwift.kt \u2514\u2500\u2500 utils \u251c\u2500\u2500 Env.kt \u251c\u2500\u2500 FastFailForWindows.kt \u251c\u2500\u2500 Git.kt \u251c\u2500\u2500 GradleCommand.kt \u251c\u2500\u2500 MarkdownFormatter.kt \u251c\u2500\u2500 Path.kt \u251c\u2500\u2500 Serialization.kt \u251c\u2500\u2500 ShellExecute.kt \u251c\u2500\u2500 Version.kt \u2514\u2500\u2500 exceptions \u251c\u2500\u2500 FlankScriptsExceptionMappers.kt \u2514\u2500\u2500 FlankScriptsExceptions.kt","title":"flank-scripts"},{"location":"flank-scripts/#flank-scripts","text":"This repository contains helper scripts for developing flank. For now, it contains just release related scripts.","title":"flank-scripts"},{"location":"flank-scripts/#build-and-usage","text":"","title":"Build and usage"},{"location":"flank-scripts/#build","text":"To build flank-scripts: - Run script buildFlankScripts.sh in flank-scripts/bash/ directory - Run command ./gradlew clean flank-scripts:assemble flank-scripts:shadowJar and manual copy file from /flank-scripts/build/libs/flank-scripts.jar to flank-scripts/bash/ - You could always run/build it from Intellij IDEA","title":"Build"},{"location":"flank-scripts/#usage","text":"Run the script with arguments flankScripts <command group> [<subgroup>] <command name> [<arguments>] If you need help with available commands or arguments you could always use the option --help","title":"Usage"},{"location":"flank-scripts/#available-commands-and-options","text":"","title":"Available commands and options"},{"location":"flank-scripts/#command-list","text":"assemble - Group of commands to assemble application android - Subgroup of commands for Android test application assembly app - Assemble Android test application ios - Subgroup of commands for iOS test applications assembly earl_grey - Assemble iOS earl grey application example - Assemble iOS example application flank_example - Assemble iOS flank example application ftl - Assemble iOS ftl example application game_loop - Assemble iOS game loop application test_plans - Assemble iOS test plans application all - Assemble all iOS applications flank - Build Flank go_artifacts - Generate go artifacts dependencies - Group of commands related to dependencies tasks install_xcpretty - Install xcpretty formatter setup_ios_env - Setup iOS environment universal_framework_files - Create Universal Framework files update_binaries - Update binaries used by Flank update - Update repository 3rd party dependencies firebase - Group of commands for managing firebase integrations check_for_sdk_updates - Check for new SDK features and create update tasks for it generate_client - Generate Java Client based on api schema update_api - Update api schema github - Group of command for managing Github integration copy_issue_properties - Copy properties(assignees, story points, labels) from issue to pull request delete_old_tag - Delete old tag on GitHub delete_release - Delete old release on github make_release - Make new Github release integration_tests - Group of commands for handling integration tests (1) process_results - Process results of integration tests linter - Group of commands used for applying correct coding style apply_to_git_hooks - Apply Linter pre-commit hook apply_to_ide - Apply Linter to IDE release - Group of commands for creating Flank release delete_snapshot - Delete snapshot package from artifacts repository generate_release_notes - Generate release notes next_tag - Get tag for next release sync_with_maven_central - Sync artifact's repository with Maven central test_artifacts - Group of commands for artifacts management download - Download test artifacts zip asset to test_artifacts directory. link - Create symbolic link to under test_runner/src/test/kotlin/ftl/fixtures/tmp to test_artifacts/{branchName}. prepare - Creates a fresh copy of test artifacts for the current working branch, basing on an existing one. remove_remote - Remove remote copy of test artifacts. resolve - Automatically prepare local artifacts if needed. unzip - Unpack test artifacts zip archive. upload - Upload test artifacts zip as github release asset. zip - Create zip archive from test artifacts directory. (1) - please note that there is only one command, but it may change in the future.","title":"Command List"},{"location":"flank-scripts/#arguments","text":"To show applicable arguments for command use --help or -h options: flankScripts <command group> [<subgroup>] <command name> --help or flankScripts <command group> [<subgroup>] <command name> -h","title":"Arguments"},{"location":"flank-scripts/#testing","text":"To test your script with different settings use the flank-debug.properties file. Uncomment and replace with desired values. Properties are skipped by git and should not be attached to a commit. Note, the test task ignores your own properties and will use the default.","title":"Testing"},{"location":"flank-scripts/#list-of-possible-configs","text":"Key Description Default value zenhub.repo-id Flank's repo ID in zenhub app. Used to create new epics. 84221974 repo.flank Flank test runner repo. Essential property for github client. Flank/flank repo.gcloud_cli Flank's fork of gcloud sdk repo. Flank/gcloud_cli repo.test-artifacts Flank's source of artifacts (apks, binaries, etc) used for testing Flank/test_artifacts integration.workflow-filename GH Action integration tests workflow file. Used to fetch list of commits since it's last run. full_suite_integration_tests.yml integration.issue-poster Name of account that creates IT issue github-actions[bot] sdk-check.workflow-filename GH Action dependencies update workflow file. Used to fetch list of commits since it's last run. update_dependencies_and_client.yml sdk-check.issue-poster Name of account that creates dependencies issues/epics github-actions[bot]","title":"List of possible configs"},{"location":"flank-scripts/#directory-structure","text":". \u251c\u2500\u2500 cli \u2502 \u251c\u2500\u2500 Main.kt \u2502 \u251c\u2500\u2500 assemble \u2502 \u2502 \u251c\u2500\u2500 AssembleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 FlankCommand.kt \u2502 \u2502 \u251c\u2500\u2500 GoCommand.kt \u2502 \u2502 \u251c\u2500\u2500 android \u2502 \u2502 \u2502 \u251c\u2500\u2500 AndroidCommand.kt \u2502 \u2502 \u2502 \u2514\u2500\u2500 AppCommand.kt \u2502 \u2502 \u2514\u2500\u2500 ios \u2502 \u2502 \u251c\u2500\u2500 EarlGreyCommand.kt \u2502 \u2502 \u251c\u2500\u2500 ExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 FlankExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 FtlCommand.kt \u2502 \u2502 \u251c\u2500\u2500 GameLoopExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 IosCommand.kt \u2502 \u2502 \u251c\u2500\u2500 RunFtlLocalCommand.kt \u2502 \u2502 \u2514\u2500\u2500 TestPlansExample.kt \u2502 \u251c\u2500\u2500 dependencies \u2502 \u2502 \u251c\u2500\u2500 DependenciesCommand.kt \u2502 \u2502 \u251c\u2500\u2500 InstallXcPrettyCommand.kt \u2502 \u2502 \u251c\u2500\u2500 SetupIosEnvCommand.kt \u2502 \u2502 \u251c\u2500\u2500 UniversalFrameworkCommand.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateBinariesCommand.kt \u2502 \u2502 \u2514\u2500\u2500 UpdateCommand.kt \u2502 \u251c\u2500\u2500 firebase \u2502 \u2502 \u251c\u2500\u2500 CheckForSdkUpdatesCommand.kt \u2502 \u2502 \u251c\u2500\u2500 FirebaseCommand.kt \u2502 \u2502 \u251c\u2500\u2500 GenerateClientCommand.kt \u2502 \u2502 \u2514\u2500\u2500 UpdateApiCommand.kt \u2502 \u251c\u2500\u2500 github \u2502 \u2502 \u251c\u2500\u2500 CopyIssuePropertiesCommand.kt \u2502 \u2502 \u251c\u2500\u2500 DeleteOldTagCommand.kt \u2502 \u2502 \u251c\u2500\u2500 DeleteReleaseCommand.kt \u2502 \u2502 \u251c\u2500\u2500 DownloadFlankCommand.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubCommand.kt \u2502 \u2502 \u2514\u2500\u2500 MakeReleaseCommand.kt \u2502 \u251c\u2500\u2500 integrationtests \u2502 \u2502 \u251c\u2500\u2500 IntegrationTestsCommand.kt \u2502 \u2502 \u2514\u2500\u2500 ProcessResultCommand.kt \u2502 \u251c\u2500\u2500 linter \u2502 \u2502 \u251c\u2500\u2500 ApplyToGitHooksCommand.kt \u2502 \u2502 \u251c\u2500\u2500 ApplyToIdeCommand.kt \u2502 \u2502 \u2514\u2500\u2500 LinterCommand.kt \u2502 \u251c\u2500\u2500 release \u2502 \u2502 \u251c\u2500\u2500 GenerateReleaseNotesCommand.kt \u2502 \u2502 \u251c\u2500\u2500 NextTagCommand.kt \u2502 \u2502 \u2514\u2500\u2500 ReleaseCommand.kt \u2502 \u2514\u2500\u2500 testartifacts \u2502 \u251c\u2500\u2500 DownloadCommand.kt \u2502 \u251c\u2500\u2500 LinkCommand.kt \u2502 \u251c\u2500\u2500 PrepareCommand.kt \u2502 \u251c\u2500\u2500 RemoveRemoteCommand.kt \u2502 \u251c\u2500\u2500 ResolveCommand.kt \u2502 \u251c\u2500\u2500 TestArtifactsCommand.kt \u2502 \u251c\u2500\u2500 UnzipCommand.kt \u2502 \u251c\u2500\u2500 UploadCommand.kt \u2502 \u2514\u2500\u2500 ZipCommand.kt \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 github \u2502 \u2502 \u251c\u2500\u2500 GitHubErrorResponse.kt \u2502 \u2502 \u251c\u2500\u2500 GithubApi.kt \u2502 \u2502 \u251c\u2500\u2500 commons \u2502 \u2502 \u2502 \u2514\u2500\u2500 LastWorkflowRunDate.kt \u2502 \u2502 \u2514\u2500\u2500 objects \u2502 \u2502 \u251c\u2500\u2500 GitHubCommit.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubCreateIssue.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubCreateIssueComment.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubRelease.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubSetAssigneesRequest.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubSetLabelsRequest.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubUpdateIssue.kt \u2502 \u2502 \u251c\u2500\u2500 GitHubWorkflowRun.kt \u2502 \u2502 \u2514\u2500\u2500 GithubPullRequest.kt \u2502 \u2514\u2500\u2500 zenhub \u2502 \u251c\u2500\u2500 ZenHubAPI.kt \u2502 \u251c\u2500\u2500 ZenHubIssue.kt \u2502 \u2514\u2500\u2500 objects \u2502 \u2514\u2500\u2500 ConvertToEpicRequest.kt \u251c\u2500\u2500 ops \u2502 \u251c\u2500\u2500 assemble \u2502 \u2502 \u251c\u2500\u2500 BuildFlank.kt \u2502 \u2502 \u251c\u2500\u2500 BuildGo.kt \u2502 \u2502 \u251c\u2500\u2500 android \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildBaseAndroidApk.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildBaseAndroidTests.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildCucumberSampleApk.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildDuplicatedNamesApks.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BuildMultiModulesApks.kt \u2502 \u2502 \u2502 \u251c\u2500\u2500 Common.kt \u2502 \u2502 \u2502 \u2514\u2500\u2500 RunAndroidOps.kt \u2502 \u2502 \u2514\u2500\u2500 ios \u2502 \u2502 \u251c\u2500\u2500 BuildEarlGreyExample.kt \u2502 \u2502 \u251c\u2500\u2500 BuildExample.kt \u2502 \u2502 \u251c\u2500\u2500 BuildFlankExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 BuildFtl.kt \u2502 \u2502 \u251c\u2500\u2500 BuildGameLoopExampleCommand.kt \u2502 \u2502 \u251c\u2500\u2500 BuildIosIPA.kt \u2502 \u2502 \u251c\u2500\u2500 BuildIosTestArtifacts.kt \u2502 \u2502 \u251c\u2500\u2500 BuildTestPlansExample.kt \u2502 \u2502 \u251c\u2500\u2500 IosBuildCommand.kt \u2502 \u2502 \u251c\u2500\u2500 RunFtlLocal.kt \u2502 \u2502 \u2514\u2500\u2500 UniversalFramework.kt \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u251c\u2500\u2500 DownloadSoftware.kt \u2502 \u2502 \u251c\u2500\u2500 EarlGreyExampleConsts.kt \u2502 \u2502 \u251c\u2500\u2500 GenerateChangeLog.kt \u2502 \u2502 \u2514\u2500\u2500 ReleaseNotesWithType.kt \u2502 \u251c\u2500\u2500 dependencies \u2502 \u2502 \u251c\u2500\u2500 InstallXcPretty.kt \u2502 \u2502 \u251c\u2500\u2500 SetupIosEnv.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateAllDependencies.kt \u2502 \u2502 \u2514\u2500\u2500 common \u2502 \u2502 \u251c\u2500\u2500 DependenciesResultCheck.kt \u2502 \u2502 \u251c\u2500\u2500 DependencyExtensions.kt \u2502 \u2502 \u251c\u2500\u2500 DependencyUpdate.kt \u2502 \u2502 \u251c\u2500\u2500 FindOutdatedDependencies.kt \u2502 \u2502 \u251c\u2500\u2500 GradleDependency.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateDependencies.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateGradle.kt \u2502 \u2502 \u251c\u2500\u2500 UpdatePlugins.kt \u2502 \u2502 \u2514\u2500\u2500 UpdateVersionsInFile.kt \u2502 \u251c\u2500\u2500 firebase \u2502 \u2502 \u251c\u2500\u2500 CheckForSDKUpdate.kt \u2502 \u2502 \u251c\u2500\u2500 CommitList.kt \u2502 \u2502 \u251c\u2500\u2500 GenerateJavaClient.kt \u2502 \u2502 \u251c\u2500\u2500 SDKUpdateContext.kt \u2502 \u2502 \u251c\u2500\u2500 UpdateApiJson.kt \u2502 \u2502 \u2514\u2500\u2500 common \u2502 \u2502 \u2514\u2500\u2500 Extensions.kt \u2502 \u251c\u2500\u2500 github \u2502 \u2502 \u251c\u2500\u2500 CopyGitHubProperties.kt \u2502 \u2502 \u251c\u2500\u2500 DeleteOldRelease.kt \u2502 \u2502 \u251c\u2500\u2500 DeleteOldTag.kt \u2502 \u2502 \u251c\u2500\u2500 DownloadFlank.kt \u2502 \u2502 \u2514\u2500\u2500 ReleaseFlank.kt \u2502 \u251c\u2500\u2500 integrationtests \u2502 \u2502 \u251c\u2500\u2500 ProcessIntegrationTestsResult.kt \u2502 \u2502 \u2514\u2500\u2500 common \u2502 \u2502 \u251c\u2500\u2500 ITResult.kt \u2502 \u2502 \u251c\u2500\u2500 IntegrationResultContext.kt \u2502 \u2502 \u2514\u2500\u2500 PrepareMessage.kt \u2502 \u251c\u2500\u2500 linter \u2502 \u2502 \u251c\u2500\u2500 ApplyKtlintToIdea.kt \u2502 \u2502 \u2514\u2500\u2500 LinkGitHooks.kt \u2502 \u251c\u2500\u2500 release \u2502 \u2502 \u251c\u2500\u2500 CreateReleaseNotes.kt \u2502 \u2502 \u2514\u2500\u2500 NextReleaseTag.kt \u2502 \u251c\u2500\u2500 testartifacts \u2502 \u2502 \u251c\u2500\u2500 ArtifactsArchive.kt \u2502 \u2502 \u251c\u2500\u2500 Context.kt \u2502 \u2502 \u251c\u2500\u2500 DownloadFixtures.kt \u2502 \u2502 \u251c\u2500\u2500 Helpers.kt \u2502 \u2502 \u251c\u2500\u2500 LinkArtifacts.kt \u2502 \u2502 \u251c\u2500\u2500 PrepareTestArtifacts.kt \u2502 \u2502 \u251c\u2500\u2500 RemoveRemoteCopy.kt \u2502 \u2502 \u251c\u2500\u2500 ResolveArtifacts.kt \u2502 \u2502 \u251c\u2500\u2500 UploadFixtures.kt \u2502 \u2502 \u2514\u2500\u2500 ZipArtifacts.kt \u2502 \u2514\u2500\u2500 updatebinaries \u2502 \u251c\u2500\u2500 UpdateAtomic.kt \u2502 \u251c\u2500\u2500 UpdateBinaries.kt \u2502 \u251c\u2500\u2500 UpdateLlvm.kt \u2502 \u2514\u2500\u2500 UpdateSwift.kt \u2514\u2500\u2500 utils \u251c\u2500\u2500 Env.kt \u251c\u2500\u2500 FastFailForWindows.kt \u251c\u2500\u2500 Git.kt \u251c\u2500\u2500 GradleCommand.kt \u251c\u2500\u2500 MarkdownFormatter.kt \u251c\u2500\u2500 Path.kt \u251c\u2500\u2500 Serialization.kt \u251c\u2500\u2500 ShellExecute.kt \u251c\u2500\u2500 Version.kt \u2514\u2500\u2500 exceptions \u251c\u2500\u2500 FlankScriptsExceptionMappers.kt \u2514\u2500\u2500 FlankScriptsExceptions.kt","title":"Directory structure"},{"location":"flank-scripts/command_overview/","text":"Flank-scripts command overview Command List assemble - Group of commands to assemble application android - Subgroup of commands for Android test application assembly app - Assemble Android test application ios - Subgroup of commands for iOS test applications assembly earl_grey - Assemble iOS earl grey application example - Assemble iOS example application flank_example - Assemble iOS flank example application ftl - Assemble iOS ftl example application game_loop - Assemble iOS game loop application test_plans - Assemble iOS test plans application all - Assemble all iOS applications flank - Build Flank go_artifacts - Generate go artifacts dependencies - Group of commands related to dependencies tasks install_xcpretty - Install xcpretty formatter setup_ios_env - Setup iOS environment universal_framework_files - Create Universal Framework files update_binaries - Update binaries used by Flank update - Update repository 3rd party dependencies firebase - Group of commands for managing firebase integrations check_for_sdk_updates - Check for new SDK features and create update tasks for it generate_client - Generate Java Client based on api schema update_api - Update api schema save_service_account - Save given service account to flank credentials location github - Group of command for managing Github integration copy_issue_properties - Copy properties(assignees, story points, labels) from issue to pull request delete_old_tag - Delete old tag on GitHub delete_release - Delete old release on github make_release - Make new Github release download_flank - Downloads flank.jar with selected version. integration_tests - Group of commands for handling integration tests (1) process_results - Process results of integration tests linter - Group of commands used for applying correct coding style apply_to_git_hooks - Apply Linter pre-commit hook apply_to_ide - Apply Linter to IDE release - Group of commands for creating Flank release delete_snapshot - Delete snapshot package from artifacts repository generate_release_notes - Generate release notes next_tag - Get tag for next release sync_with_maven_central - Sync artifact's repository with Maven central test_artifacts - Group of commands for artifacts management download - Download test artifacts zip asset to test_artifacts directory. link - Create symbolic link to under test_runner/src/test/kotlin/ftl/fixtures/tmp to test_artifacts/{branchName}. prepare - Creates a fresh copy of test artifacts for the current working branch, basing on an existing one. remove_remote - Remove remote copy of test artifacts. resolve - Automatically prepare local artifacts if needed. unzip - Unpack test artifacts zip archive. upload - Upload test artifacts zip as github release asset. zip - Create zip archive from test artifacts directory. (1) - please note that there is only one command, but it may change in the future. Usage flankScripts <command group> [<subgroup>] <command name> [<arguments>]","title":"Flank-scripts command overview"},{"location":"flank-scripts/command_overview/#flank-scripts-command-overview","text":"","title":"Flank-scripts command overview"},{"location":"flank-scripts/command_overview/#command-list","text":"assemble - Group of commands to assemble application android - Subgroup of commands for Android test application assembly app - Assemble Android test application ios - Subgroup of commands for iOS test applications assembly earl_grey - Assemble iOS earl grey application example - Assemble iOS example application flank_example - Assemble iOS flank example application ftl - Assemble iOS ftl example application game_loop - Assemble iOS game loop application test_plans - Assemble iOS test plans application all - Assemble all iOS applications flank - Build Flank go_artifacts - Generate go artifacts dependencies - Group of commands related to dependencies tasks install_xcpretty - Install xcpretty formatter setup_ios_env - Setup iOS environment universal_framework_files - Create Universal Framework files update_binaries - Update binaries used by Flank update - Update repository 3rd party dependencies firebase - Group of commands for managing firebase integrations check_for_sdk_updates - Check for new SDK features and create update tasks for it generate_client - Generate Java Client based on api schema update_api - Update api schema save_service_account - Save given service account to flank credentials location github - Group of command for managing Github integration copy_issue_properties - Copy properties(assignees, story points, labels) from issue to pull request delete_old_tag - Delete old tag on GitHub delete_release - Delete old release on github make_release - Make new Github release download_flank - Downloads flank.jar with selected version. integration_tests - Group of commands for handling integration tests (1) process_results - Process results of integration tests linter - Group of commands used for applying correct coding style apply_to_git_hooks - Apply Linter pre-commit hook apply_to_ide - Apply Linter to IDE release - Group of commands for creating Flank release delete_snapshot - Delete snapshot package from artifacts repository generate_release_notes - Generate release notes next_tag - Get tag for next release sync_with_maven_central - Sync artifact's repository with Maven central test_artifacts - Group of commands for artifacts management download - Download test artifacts zip asset to test_artifacts directory. link - Create symbolic link to under test_runner/src/test/kotlin/ftl/fixtures/tmp to test_artifacts/{branchName}. prepare - Creates a fresh copy of test artifacts for the current working branch, basing on an existing one. remove_remote - Remove remote copy of test artifacts. resolve - Automatically prepare local artifacts if needed. unzip - Unpack test artifacts zip archive. upload - Upload test artifacts zip as github release asset. zip - Create zip archive from test artifacts directory. (1) - please note that there is only one command, but it may change in the future.","title":"Command List"},{"location":"flank-scripts/command_overview/#usage","text":"flankScripts <command group> [<subgroup>] <command name> [<arguments>]","title":"Usage"},{"location":"flank-scripts/ops_structure/","text":"Flank-scripts ops package structure Flank-scripts ops packages mostly match cli structure, however, for widely used function there is a separate package called common . For better code organization updatebinaries has a separate package inside dependencies , as well as jfrog in release package.","title":"Ops structure"},{"location":"flank-scripts/ops_structure/#flank-scripts-ops-package-structure","text":"Flank-scripts ops packages mostly match cli structure, however, for widely used function there is a separate package called common . For better code organization updatebinaries has a separate package inside dependencies , as well as jfrog in release package.","title":"Flank-scripts ops package structure"},{"location":"gcloud/google_api_usecases/","text":"Google api use cases List of places where google api calls are used com.google.api.client.http GoogleApiLogger.kt ftl.android AndroidCatalog.kt ftl.args ArgsHelper.kt ftl.config Credentials.kt FtlConstants.kt ftl.environment ListIPBlocks.kt ListLocales.kt LocalesDescription.kt NetworkProfileDescription.kt ftl.environment.android AndroidModelDescription.kt AndroidSoftwareVersionDescription.kt ListAndroidDevices.kt ListAndroidSofwareVersions.kt ftl.environment.common ListNetworkConfiguration.kt ListOrientations.kt ListProvidedSoftware.kt ftl.environment.ios IosModelDescription.kt IosSoftwareVersionDescription.kt ListIOsDevices.kt ListIOsSofwareVersions.kt ftl.gc GcAndroidDevice.kt GcAndroidTestMatrix.kt GcIosMatrix.kt GcIosTestMatrix.kt GcStorage.kt GcTesting.kt GcTestMatrix.kt GcToolResults.kt UserAuth.kt Utils.kt ftl.gc.android CreateAndroidInstrumentationTest.kt CreateAndroidLoopTest.kt CreateAndroidRobotTest.kt SetupAndroidTest.kt SetupEnvironmentVariables.kt Utils.kt ftl.gc.ios SetupIosTest.kt ftl.http ExecuteWithRetry.kt HttpTimeoutIncrease.kt ftl.ios IosCatalog.kt ftl.json MatrixMap.kt OutcomeDetailsFormatter.kt SavedMatrix.kt ftl.log Loggers.kt ftl.mock MockServer.kt ftl.reports HtmlErrorReport.kt ftl.reports.api CreateJUnitTestCase.kt CreateJUnitTestResult.kt CreateTestExecutionData.kt CreateTestSuiteOverviewData.kt PerformanceMetrics.kt PrepareForJUnitResult.kt ProcessFromApi.kt Utils.kt ftl.reports.api.data TestExecutionData.kt TestSuiteOverviewData.kt ftl.reports.outcome BillableMinutes.kt CreateMatrixOutcomeSummary.kt CreateTestSuiteOverviewData.kt TestOutcome.kt TestOutcomeContext.kt Util.kt ftl.reports.util ReportManager.kt ftl.run RefreshLastRun.kt ftl.run.common FetchArtifacts.kt PollMatrices.kt PrettyPrint.kt ftl.run.model AndroidTestShards.kt ftl.run.platform RunAndroidTests.kt ftl.run.platform.common AfterRunTests.kt ftl.run.status ExecutionStatusListPrinter.kt TestMatrixStatusPrinter.kt ftl.util MatrixState.kt ObfuscationGson.kt TestMatrixExtension.kt Gcloud Api Calls in Flank GcToolResults.kt createToolResultsHistory getExecutionResult getStepResult getPerformanceMetric listTestCases getDefaultBucket listAllEnvironments listAllSteps TestOutcomeContext.kt fetchTestOutcomeContext GcIosTestMatrix.kt build GcAndroidTestMatrix.kt build GcStorage.kt uploadWithProgress download exist Common environment information's ListIPBlocks.kt ListLocales.kt LocalesDescription.kt NetworkConfigurationCatalog.kt NetworkProfileDescription.kt ProvidedSoftwareCatalog.kt TestEnvironmentInfo.kt AndroidModelDescription.kt AndroidSoftwareVersionDescription.kt ListAndroidDevices.kt ListAndroidSofwareVersions.kt IosModelDescription.kt IosSoftwareVersionDescription.kt ListIOsDevices.kt ListIOsSofwareVersions.kt Android Run use case AndroidRunCommand.kt L: 76 validate() ValidateAndroidArgs.kt L: 18 AndroidArgs.validate() L: 20 AndroidArgs.assertDevicesSupported() L: 97 AndroidCatalog.supportedDeviceConfig() L: 104 AndroidCatalog.androidModelIds() L: 109 AndroidCatalog.androidVersionIds() L: 112 device.getSupportedVersionId() L: 32 IArgs.checkResultsDirUnique() ValidateCommonArgs.kt L: 75 GcStorage.exist(resultsBucket, resultsDir) AndroidRunCommand.kt L: 79 newTestRun() NewTestRun.kt L: 27 runTests() L: 46 runAndroidTests() RunAndroidTests.kt L: 46 uploadOtherFiles() UploadOtherFiles.kt L: 14 GcStorage.upload() L: 47 uploadAdditionalApks() UploadApks.kt L: 50 uploadAdditionalApks() L: 56 uploadToGcloudIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload() L: 48 uploadObbFiles() UploadOtherFiles.kt L: 18 uploadObbFiles() L: 20 GcStorage.upload() L: 49 createAndroidTestContexts() CreateAndroidTestContext.kt L: 30 setupShards() L: 41 testContext.downloadApks() L: 50 app = app.downloadIfNeeded() L: 51 test = test.downloadIfNeeded() FileReference.kt L: 29 GcStorage.download() GcStorage.kt L: 179 download() L: 51 upload() UploadApks.kt L: 23 context.upload() L: 26 AndroidTestContext.upload() L: 27 is InstrumentationTestContext -> upload() L: 33 InstrumentationTestContext.upload() L: 34 app.uploadIfNeeded() L: 35 test.uploadIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload GcStorage.kt L: 60 upload() L: 28 is RoboTestContext -> upload() L: 38 RoboTestContext.upload() L: 39 app.uploadIfNeeded() L: 40 roboScript.uploadIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload GcStorage.kt L: 60 upload() L: 29 is SanityRoboTestContext -> upload() L: 47 SanityRoboTestContext.upload() L: 48 app.uploadIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload GcStorage.kt L: 60 upload() L: 30 is GameLoopContext -> upload() L: 43 GameLoopContext.upload() L: 44 app.uploadIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload GcStorage.kt L: 60 upload() L: 59 GcAndroidTestMatrix.build() GcAndroidTestMatrix.kt L: 30 build() L: 89 GcTesting.get.projects().testMatrices().create() NewTestRun.kt L: 35 ReportManager.generate() ReportManager.kt L: 97 parseTestSuite() L: 103 it.run() MatrixResultsReport.kt L: 80 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() CostReport.kt L: 43 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() L: 108 it.run() HtmlErrorReport.kt L: 35 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() L: 87 refreshMatricesAndGetExecutions() ProcessFromApi.kt L: 13 refreshMatricesAndGetExecutions() L: 18 refreshTestMatrices() L: 24 cTestMatrix.refresh() L: 121 refreshMatricesAndGetExecutions() ProcessFromApi.kt L: 13 refreshMatricesAndGetExecutions() L: 18 refreshTestMatrices() L: 24 cTestMatrix.refresh() L: 123 createAndUploadPerformanceMetricsForAndroid() L: 148 getAndUploadPerformanceMetrics() PerformanceMetrics.kt L: 16 getAndUploadPerformanceMetrics() L: 27 getPerformanceMetric() L: 45 GcToolResults.getPerformanceMetric() L: 25 performanceMetrics.upload() L: 47 GcStorage.uploadPerformanceMetrics() L: 124 GcStorage.uploadMatricesId() GcStorage.kt L: 96 uploadMatricesId() L: 60 upload() iOS Run use case IosRunCommand.kt L: 76 validate() ValidateIosArgs.kt L: 14 IosArgs.validate() L: 16 assertXcodeSupported() L: 79 IosCatalog.supportedXcode() IosCatalog.kt L: 44 xcodeVersions() L: 47 iosDeviceCatalog() L: 63: GcTesting...iosDeviceCatalog() L: 17 assertDevicesSupported(() L: 84 IosCatalog.supportedDevice() IosCatalog.kt L: 53 iosDeviceCatalog() L: 63 GcTesting...iosDeviceCatalog() L: 21 checkResultsDirUnique() ValidateCommonArgs.kt L: 75 GcStorage.exist() IosRunCommand.kt L: 78 newTestRun() NewTestRun.kt L: 27 runTests() L: 47 runIosTests() RunIosTests.kt L: 41 uploadOtherFiles() UploadOtherFiles.kt L: 14 GcStorage.upload() L: 42 uploadAdditionalIpas() UploadApks.kt L: 54 uploadToGcloudIfNeeded() L: 61 uploadIfNeeded() FileReference.kt L: 42 GcStorage.upload() L: 49 createIosTestContexts() CreateIosTestContext.kt L: 9 createXcTestContexts() CreateXcTestContext.kt L: 17 uploadIfNeeded() FileReference.kt L: 32 uploadIfNeeded() L: 42: GcStorage.upload() L: 28 GcStorage.uploadXCTestFile() GcStorage.kt L: 124 upload() L: 10 createGameloopTestContexts() CreateGameloopTestContext.kt L: 15 uploadIfNeeded() FileReference.kt L: 32 uploadIfNeeded() L: 42 GcStorage.upload() L: 50 GcIosTestMatrix.build() GcIosTestMatrix.kt L: 66 GcTesting...create() NewTestRun.kt L: 35 ReportManager.generate() ReportManager.kt L: 103 it.run() MatrixResultsReport.kt L: 80 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() CostReport.kt L: 43 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() L: 108 it.run() HtmlErrorReport.kt L: 35 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() L: 121 refreshMatricesAndGetExecutions() ProcessFromApi.kt L: 13 refreshMatricesAndGetExecutions() L: 18 refreshTestMatrices() L: 24 cTestMatrix.refresh() L: 123 createAndUploadPerformanceMetricsForAndroid() L: 148 getAndUploadPerformanceMetrics() PerformanceMetrics.kt L: 16 getAndUploadPerformanceMetrics() L: 27 getPerformanceMetric() L: 45 GcToolResults.getPerformanceMetric() L: 25 performanceMetrics.upload() L: 47 GcStorage.uploadPerformanceMetrics() L: 124 GcStorage.uploadMatricesId() GcStorage.kt L: 96 uploadMatricesId() L: 60 upload()","title":"Google api use cases"},{"location":"gcloud/google_api_usecases/#google-api-use-cases","text":"","title":"Google api use cases"},{"location":"gcloud/google_api_usecases/#list-of-places-where-google-api-calls-are-used","text":"","title":"List of places where google api calls are used"},{"location":"gcloud/google_api_usecases/#comgoogleapiclienthttp","text":"GoogleApiLogger.kt","title":"com.google.api.client.http"},{"location":"gcloud/google_api_usecases/#ftlandroid","text":"AndroidCatalog.kt","title":"ftl.android"},{"location":"gcloud/google_api_usecases/#ftlargs","text":"ArgsHelper.kt","title":"ftl.args"},{"location":"gcloud/google_api_usecases/#ftlconfig","text":"Credentials.kt FtlConstants.kt","title":"ftl.config"},{"location":"gcloud/google_api_usecases/#ftlenvironment","text":"ListIPBlocks.kt ListLocales.kt LocalesDescription.kt NetworkProfileDescription.kt","title":"ftl.environment"},{"location":"gcloud/google_api_usecases/#ftlenvironmentandroid","text":"AndroidModelDescription.kt AndroidSoftwareVersionDescription.kt ListAndroidDevices.kt ListAndroidSofwareVersions.kt","title":"ftl.environment.android"},{"location":"gcloud/google_api_usecases/#ftlenvironmentcommon","text":"ListNetworkConfiguration.kt ListOrientations.kt ListProvidedSoftware.kt","title":"ftl.environment.common"},{"location":"gcloud/google_api_usecases/#ftlenvironmentios","text":"IosModelDescription.kt IosSoftwareVersionDescription.kt ListIOsDevices.kt ListIOsSofwareVersions.kt","title":"ftl.environment.ios"},{"location":"gcloud/google_api_usecases/#ftlgc","text":"GcAndroidDevice.kt GcAndroidTestMatrix.kt GcIosMatrix.kt GcIosTestMatrix.kt GcStorage.kt GcTesting.kt GcTestMatrix.kt GcToolResults.kt UserAuth.kt Utils.kt","title":"ftl.gc"},{"location":"gcloud/google_api_usecases/#ftlgcandroid","text":"CreateAndroidInstrumentationTest.kt CreateAndroidLoopTest.kt CreateAndroidRobotTest.kt SetupAndroidTest.kt SetupEnvironmentVariables.kt Utils.kt","title":"ftl.gc.android"},{"location":"gcloud/google_api_usecases/#ftlgcios","text":"SetupIosTest.kt","title":"ftl.gc.ios"},{"location":"gcloud/google_api_usecases/#ftlhttp","text":"ExecuteWithRetry.kt HttpTimeoutIncrease.kt","title":"ftl.http"},{"location":"gcloud/google_api_usecases/#ftlios","text":"IosCatalog.kt","title":"ftl.ios"},{"location":"gcloud/google_api_usecases/#ftljson","text":"MatrixMap.kt OutcomeDetailsFormatter.kt SavedMatrix.kt","title":"ftl.json"},{"location":"gcloud/google_api_usecases/#ftllog","text":"Loggers.kt","title":"ftl.log"},{"location":"gcloud/google_api_usecases/#ftlmock","text":"MockServer.kt","title":"ftl.mock"},{"location":"gcloud/google_api_usecases/#ftlreports","text":"HtmlErrorReport.kt","title":"ftl.reports"},{"location":"gcloud/google_api_usecases/#ftlreportsapi","text":"CreateJUnitTestCase.kt CreateJUnitTestResult.kt CreateTestExecutionData.kt CreateTestSuiteOverviewData.kt PerformanceMetrics.kt PrepareForJUnitResult.kt ProcessFromApi.kt Utils.kt","title":"ftl.reports.api"},{"location":"gcloud/google_api_usecases/#ftlreportsapidata","text":"TestExecutionData.kt TestSuiteOverviewData.kt","title":"ftl.reports.api.data"},{"location":"gcloud/google_api_usecases/#ftlreportsoutcome","text":"BillableMinutes.kt CreateMatrixOutcomeSummary.kt CreateTestSuiteOverviewData.kt TestOutcome.kt TestOutcomeContext.kt Util.kt","title":"ftl.reports.outcome"},{"location":"gcloud/google_api_usecases/#ftlreportsutil","text":"ReportManager.kt","title":"ftl.reports.util"},{"location":"gcloud/google_api_usecases/#ftlrun","text":"RefreshLastRun.kt","title":"ftl.run"},{"location":"gcloud/google_api_usecases/#ftlruncommon","text":"FetchArtifacts.kt PollMatrices.kt PrettyPrint.kt","title":"ftl.run.common"},{"location":"gcloud/google_api_usecases/#ftlrunmodel","text":"AndroidTestShards.kt","title":"ftl.run.model"},{"location":"gcloud/google_api_usecases/#ftlrunplatform","text":"RunAndroidTests.kt","title":"ftl.run.platform"},{"location":"gcloud/google_api_usecases/#ftlrunplatformcommon","text":"AfterRunTests.kt","title":"ftl.run.platform.common"},{"location":"gcloud/google_api_usecases/#ftlrunstatus","text":"ExecutionStatusListPrinter.kt TestMatrixStatusPrinter.kt","title":"ftl.run.status"},{"location":"gcloud/google_api_usecases/#ftlutil","text":"MatrixState.kt ObfuscationGson.kt TestMatrixExtension.kt","title":"ftl.util"},{"location":"gcloud/google_api_usecases/#gcloud-api-calls-in-flank","text":"GcToolResults.kt createToolResultsHistory getExecutionResult getStepResult getPerformanceMetric listTestCases getDefaultBucket listAllEnvironments listAllSteps TestOutcomeContext.kt fetchTestOutcomeContext GcIosTestMatrix.kt build GcAndroidTestMatrix.kt build GcStorage.kt uploadWithProgress download exist Common environment information's ListIPBlocks.kt ListLocales.kt LocalesDescription.kt NetworkConfigurationCatalog.kt NetworkProfileDescription.kt ProvidedSoftwareCatalog.kt TestEnvironmentInfo.kt AndroidModelDescription.kt AndroidSoftwareVersionDescription.kt ListAndroidDevices.kt ListAndroidSofwareVersions.kt IosModelDescription.kt IosSoftwareVersionDescription.kt ListIOsDevices.kt ListIOsSofwareVersions.kt","title":"Gcloud Api Calls in Flank"},{"location":"gcloud/google_api_usecases/#android-run-use-case","text":"AndroidRunCommand.kt L: 76 validate() ValidateAndroidArgs.kt L: 18 AndroidArgs.validate() L: 20 AndroidArgs.assertDevicesSupported() L: 97 AndroidCatalog.supportedDeviceConfig() L: 104 AndroidCatalog.androidModelIds() L: 109 AndroidCatalog.androidVersionIds() L: 112 device.getSupportedVersionId() L: 32 IArgs.checkResultsDirUnique() ValidateCommonArgs.kt L: 75 GcStorage.exist(resultsBucket, resultsDir) AndroidRunCommand.kt L: 79 newTestRun() NewTestRun.kt L: 27 runTests() L: 46 runAndroidTests() RunAndroidTests.kt L: 46 uploadOtherFiles() UploadOtherFiles.kt L: 14 GcStorage.upload() L: 47 uploadAdditionalApks() UploadApks.kt L: 50 uploadAdditionalApks() L: 56 uploadToGcloudIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload() L: 48 uploadObbFiles() UploadOtherFiles.kt L: 18 uploadObbFiles() L: 20 GcStorage.upload() L: 49 createAndroidTestContexts() CreateAndroidTestContext.kt L: 30 setupShards() L: 41 testContext.downloadApks() L: 50 app = app.downloadIfNeeded() L: 51 test = test.downloadIfNeeded() FileReference.kt L: 29 GcStorage.download() GcStorage.kt L: 179 download() L: 51 upload() UploadApks.kt L: 23 context.upload() L: 26 AndroidTestContext.upload() L: 27 is InstrumentationTestContext -> upload() L: 33 InstrumentationTestContext.upload() L: 34 app.uploadIfNeeded() L: 35 test.uploadIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload GcStorage.kt L: 60 upload() L: 28 is RoboTestContext -> upload() L: 38 RoboTestContext.upload() L: 39 app.uploadIfNeeded() L: 40 roboScript.uploadIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload GcStorage.kt L: 60 upload() L: 29 is SanityRoboTestContext -> upload() L: 47 SanityRoboTestContext.upload() L: 48 app.uploadIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload GcStorage.kt L: 60 upload() L: 30 is GameLoopContext -> upload() L: 43 GameLoopContext.upload() L: 44 app.uploadIfNeeded() FileReference.kt L: 37 FileReference.uploadIfNeeded() L: 42 GcStorage.upload GcStorage.kt L: 60 upload() L: 59 GcAndroidTestMatrix.build() GcAndroidTestMatrix.kt L: 30 build() L: 89 GcTesting.get.projects().testMatrices().create() NewTestRun.kt L: 35 ReportManager.generate() ReportManager.kt L: 97 parseTestSuite() L: 103 it.run() MatrixResultsReport.kt L: 80 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() CostReport.kt L: 43 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() L: 108 it.run() HtmlErrorReport.kt L: 35 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() L: 87 refreshMatricesAndGetExecutions() ProcessFromApi.kt L: 13 refreshMatricesAndGetExecutions() L: 18 refreshTestMatrices() L: 24 cTestMatrix.refresh() L: 121 refreshMatricesAndGetExecutions() ProcessFromApi.kt L: 13 refreshMatricesAndGetExecutions() L: 18 refreshTestMatrices() L: 24 cTestMatrix.refresh() L: 123 createAndUploadPerformanceMetricsForAndroid() L: 148 getAndUploadPerformanceMetrics() PerformanceMetrics.kt L: 16 getAndUploadPerformanceMetrics() L: 27 getPerformanceMetric() L: 45 GcToolResults.getPerformanceMetric() L: 25 performanceMetrics.upload() L: 47 GcStorage.uploadPerformanceMetrics() L: 124 GcStorage.uploadMatricesId() GcStorage.kt L: 96 uploadMatricesId() L: 60 upload()","title":"Android Run use case"},{"location":"gcloud/google_api_usecases/#ios-run-use-case","text":"IosRunCommand.kt L: 76 validate() ValidateIosArgs.kt L: 14 IosArgs.validate() L: 16 assertXcodeSupported() L: 79 IosCatalog.supportedXcode() IosCatalog.kt L: 44 xcodeVersions() L: 47 iosDeviceCatalog() L: 63: GcTesting...iosDeviceCatalog() L: 17 assertDevicesSupported(() L: 84 IosCatalog.supportedDevice() IosCatalog.kt L: 53 iosDeviceCatalog() L: 63 GcTesting...iosDeviceCatalog() L: 21 checkResultsDirUnique() ValidateCommonArgs.kt L: 75 GcStorage.exist() IosRunCommand.kt L: 78 newTestRun() NewTestRun.kt L: 27 runTests() L: 47 runIosTests() RunIosTests.kt L: 41 uploadOtherFiles() UploadOtherFiles.kt L: 14 GcStorage.upload() L: 42 uploadAdditionalIpas() UploadApks.kt L: 54 uploadToGcloudIfNeeded() L: 61 uploadIfNeeded() FileReference.kt L: 42 GcStorage.upload() L: 49 createIosTestContexts() CreateIosTestContext.kt L: 9 createXcTestContexts() CreateXcTestContext.kt L: 17 uploadIfNeeded() FileReference.kt L: 32 uploadIfNeeded() L: 42: GcStorage.upload() L: 28 GcStorage.uploadXCTestFile() GcStorage.kt L: 124 upload() L: 10 createGameloopTestContexts() CreateGameloopTestContext.kt L: 15 uploadIfNeeded() FileReference.kt L: 32 uploadIfNeeded() L: 42 GcStorage.upload() L: 50 GcIosTestMatrix.build() GcIosTestMatrix.kt L: 66 GcTesting...create() NewTestRun.kt L: 35 ReportManager.generate() ReportManager.kt L: 103 it.run() MatrixResultsReport.kt L: 80 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() CostReport.kt L: 43 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() L: 108 it.run() HtmlErrorReport.kt L: 35 GcStorage.uploadReportResult() GcStorage.kt L: 115 upload() L: 121 refreshMatricesAndGetExecutions() ProcessFromApi.kt L: 13 refreshMatricesAndGetExecutions() L: 18 refreshTestMatrices() L: 24 cTestMatrix.refresh() L: 123 createAndUploadPerformanceMetricsForAndroid() L: 148 getAndUploadPerformanceMetrics() PerformanceMetrics.kt L: 16 getAndUploadPerformanceMetrics() L: 27 getPerformanceMetric() L: 45 GcToolResults.getPerformanceMetric() L: 25 performanceMetrics.upload() L: 47 GcStorage.uploadPerformanceMetrics() L: 124 GcStorage.uploadMatricesId() GcStorage.kt L: 96 uploadMatricesId() L: 60 upload()","title":"iOS Run use case"},{"location":"onboarding/1_environment_setup/","text":"Environment setup This document may be incomplete now or in the future, so if you faced any problems ask the team for help. Mac Install a brew, it's not mandatory but may be convenient for installing other software. Currently, the zsh is the default shell on a mac. If you prefer bash use chsh -s /bin/bash . Env config Bunch of useful exports. You can paste them to your .bashrc FLANK_REPO = \"type path to your local flank repository\" export PATH = $PATH : $HOME / $FLANK_REPO /flank/test_runner/bash export PATH = $PATH : $HOME / $FLANK_REPO /flank/test_projects/android/bash export PATH = $PATH : $HOME /Library/Android/sdk/platform-tools export PATH = $PATH : $HOME /Library/Python/2.7/bin #export PATH=$PATH:$HOME/\"path to your local gcloud repository\"/gcloud_cli/google-cloud-sdk/bin export FLANK_PROJECT_ID = flank-open-source export GOOGLE_CLOUD_PROJECT = flank-open-source export GITHUB_TOKEN = \"type your gihub token here\" Common Ask for access to internal slack channels Ask for an invitation to firebase slack Ask for access to GitHub repo Ask for access to test bucket on a google cloud platform Install zenhub extension to get access to our board. Install Oracle JDK 8 Unfortunately there is no official way to download installer without a login account. Unfortunately unofficial instruction from here sometimes isn't working. Use JetBrains Toolbox to install IDE. Install JetBrains Toolbox download from website or brew cask install jetbrains-toolbox Install IntelliJ idea (community may be enough) Install the Android studio. Setup local flank repository Clone the repo git clone --recursive https://github.com/Flank/flank.git Init submodule git submodule update --init --recursive updates the submodules Build flank running ./update_flank.sh Auth google account Run flank auth login [ ./flank ]. Click on the link and authenticate the account in a browser. Flank will save the credential to ~/.flank. Install gcloud. Be aware gcloud requires a python environment. You can clone https://github.com/Flank/gcloud_cli Or follow official instruction https://cloud.google.com/sdk/docs/quickstarts Don't forget about exports for Python and gcloud Configure pre-commit hook for ktlint code autoformatting Make sure you can execute flank-scripts from the command line if not navigate to in the command line to ./flank-scripts/bash Run flankScripts linter apply_to_git_hooks Apply Ktlint style to Idea project. Make sure you can execute flank-scripts from the command line if not navigate to in the command line to ./flank-scripts/bash Run flankScripts linter apply_to_ide","title":"Environment setup"},{"location":"onboarding/1_environment_setup/#environment-setup","text":"This document may be incomplete now or in the future, so if you faced any problems ask the team for help.","title":"Environment setup"},{"location":"onboarding/1_environment_setup/#mac","text":"Install a brew, it's not mandatory but may be convenient for installing other software. Currently, the zsh is the default shell on a mac. If you prefer bash use chsh -s /bin/bash .","title":"Mac"},{"location":"onboarding/1_environment_setup/#env-config","text":"Bunch of useful exports. You can paste them to your .bashrc FLANK_REPO = \"type path to your local flank repository\" export PATH = $PATH : $HOME / $FLANK_REPO /flank/test_runner/bash export PATH = $PATH : $HOME / $FLANK_REPO /flank/test_projects/android/bash export PATH = $PATH : $HOME /Library/Android/sdk/platform-tools export PATH = $PATH : $HOME /Library/Python/2.7/bin #export PATH=$PATH:$HOME/\"path to your local gcloud repository\"/gcloud_cli/google-cloud-sdk/bin export FLANK_PROJECT_ID = flank-open-source export GOOGLE_CLOUD_PROJECT = flank-open-source export GITHUB_TOKEN = \"type your gihub token here\"","title":"Env config"},{"location":"onboarding/1_environment_setup/#common","text":"Ask for access to internal slack channels Ask for an invitation to firebase slack Ask for access to GitHub repo Ask for access to test bucket on a google cloud platform Install zenhub extension to get access to our board. Install Oracle JDK 8 Unfortunately there is no official way to download installer without a login account. Unfortunately unofficial instruction from here sometimes isn't working. Use JetBrains Toolbox to install IDE. Install JetBrains Toolbox download from website or brew cask install jetbrains-toolbox Install IntelliJ idea (community may be enough) Install the Android studio. Setup local flank repository Clone the repo git clone --recursive https://github.com/Flank/flank.git Init submodule git submodule update --init --recursive updates the submodules Build flank running ./update_flank.sh Auth google account Run flank auth login [ ./flank ]. Click on the link and authenticate the account in a browser. Flank will save the credential to ~/.flank. Install gcloud. Be aware gcloud requires a python environment. You can clone https://github.com/Flank/gcloud_cli Or follow official instruction https://cloud.google.com/sdk/docs/quickstarts Don't forget about exports for Python and gcloud Configure pre-commit hook for ktlint code autoformatting Make sure you can execute flank-scripts from the command line if not navigate to in the command line to ./flank-scripts/bash Run flankScripts linter apply_to_git_hooks Apply Ktlint style to Idea project. Make sure you can execute flank-scripts from the command line if not navigate to in the command line to ./flank-scripts/bash Run flankScripts linter apply_to_ide","title":"Common"},{"location":"onboarding/2_contribution/","text":"Contribution process This document describes contribution details for full-time contributors. Daily community monitoring Check if there are any new critical issues in reported issues . Check if there are any new issues on the flank slack channel . This is good practice to check out those points once a day by someone from the team. Looking for a new task Perform daily monitoring if needed. Ask the team if someone needs help. We prefer teamwork over solo-work, our goal is quality so verification is important. Check flank zenhub board . Typically, the most important issues are prioritized descending in the Ranked column. Estimating tasks In the flank, we are using 3 points scale for estimates tasks complexity (snake, tiger, dragon). For more details see estimation.md Working on a new task Perform estimation if needed. If the task looks complex it's good practice asking a team for help. Typically, estimates should be already done on a weekly review, but issues reported after review may have a lack of estimation. According to task complexity choose the best strategy. Snakes Snake is easy and don't need any explanations. So if the task requires some implementation, everything that you need is a new branch, pull request with proper description and some tests. After you finished don't forget to mention someone from the team for help. Tigers If you are starting work on a tiger, always consider sharing some work with someone from the team, for more details see Buddy system section. Typically, every tiger brings some common things to do like: * Pull request with proper description which typically may contain the following information: * description * user story * definition of done * how to reproduce * Documentation. for more details see the Documentation section. * Unit tests * Dedicated YAML config with required assets for manual or integration testing Dragons Dragon requires research, so you should always start from writing some documentations draft. This draft should contain any important pieces of information about the task and should be synchronized with the status of knowledge according to any progression. The dragon task may be started by one developer but shouldn't be handle alone too long. The task could be a dragon as long as it's unknown and mysterious when there is a plan on how to deal with it, it's automatically become one or many tigers. So the main goal when deal with a dragon is to prepare documentation on how to solve the problem. Documentation Be aware, some tasks sometimes couldn't be resolved for many reasons, so it's really important to have documentation always up to date. Having documentation up to date gives the ability to drop work at any moment and back to in the future without loss of any information. Buddy system See buddy_system.md to read about team work in a flank. Ktlint styling and pre-commit hooks To ensure that the correct styling for Flank is upheld ensure that environment setup is read, and the correct flankScripts commands have been run so that Ktlint is applied to the idea project, and the pre-commit hook is working correctly.","title":"Contribution"},{"location":"onboarding/2_contribution/#contribution-process","text":"This document describes contribution details for full-time contributors.","title":"Contribution process"},{"location":"onboarding/2_contribution/#daily-community-monitoring","text":"Check if there are any new critical issues in reported issues . Check if there are any new issues on the flank slack channel . This is good practice to check out those points once a day by someone from the team.","title":"Daily community monitoring"},{"location":"onboarding/2_contribution/#looking-for-a-new-task","text":"Perform daily monitoring if needed. Ask the team if someone needs help. We prefer teamwork over solo-work, our goal is quality so verification is important. Check flank zenhub board . Typically, the most important issues are prioritized descending in the Ranked column.","title":"Looking for a new task"},{"location":"onboarding/2_contribution/#estimating-tasks","text":"In the flank, we are using 3 points scale for estimates tasks complexity (snake, tiger, dragon). For more details see estimation.md","title":"Estimating tasks"},{"location":"onboarding/2_contribution/#working-on-a-new-task","text":"Perform estimation if needed. If the task looks complex it's good practice asking a team for help. Typically, estimates should be already done on a weekly review, but issues reported after review may have a lack of estimation. According to task complexity choose the best strategy.","title":"Working on a new task"},{"location":"onboarding/2_contribution/#snakes","text":"Snake is easy and don't need any explanations. So if the task requires some implementation, everything that you need is a new branch, pull request with proper description and some tests. After you finished don't forget to mention someone from the team for help.","title":"Snakes"},{"location":"onboarding/2_contribution/#tigers","text":"If you are starting work on a tiger, always consider sharing some work with someone from the team, for more details see Buddy system section. Typically, every tiger brings some common things to do like: * Pull request with proper description which typically may contain the following information: * description * user story * definition of done * how to reproduce * Documentation. for more details see the Documentation section. * Unit tests * Dedicated YAML config with required assets for manual or integration testing","title":"Tigers"},{"location":"onboarding/2_contribution/#dragons","text":"Dragon requires research, so you should always start from writing some documentations draft. This draft should contain any important pieces of information about the task and should be synchronized with the status of knowledge according to any progression. The dragon task may be started by one developer but shouldn't be handle alone too long. The task could be a dragon as long as it's unknown and mysterious when there is a plan on how to deal with it, it's automatically become one or many tigers. So the main goal when deal with a dragon is to prepare documentation on how to solve the problem.","title":"Dragons"},{"location":"onboarding/2_contribution/#documentation","text":"Be aware, some tasks sometimes couldn't be resolved for many reasons, so it's really important to have documentation always up to date. Having documentation up to date gives the ability to drop work at any moment and back to in the future without loss of any information.","title":"Documentation"},{"location":"onboarding/2_contribution/#buddy-system","text":"See buddy_system.md to read about team work in a flank.","title":"Buddy system"},{"location":"onboarding/2_contribution/#ktlint-styling-and-pre-commit-hooks","text":"To ensure that the correct styling for Flank is upheld ensure that environment setup is read, and the correct flankScripts commands have been run so that Ktlint is applied to the idea project, and the pre-commit hook is working correctly.","title":"Ktlint styling and pre-commit hooks"},{"location":"onboarding/3_estimation/","text":"Estimating Eng Work: Snakes, Tigers, Dragons Introduction This document describes a model for estimating the amount of work an engineering task requires. It is not about estimating how long that work will take \u2014 that is a different discussion for a different time. It is not about how to arrive at these estimates \u2014 that is also a different discussion for a different time. The primary aim of estimating size is to increase the velocity of those doing the work, i.e. engineering team. These estimates might give some amount of signal to other stakeholders, e.g. product management or eng leadership, but that is not the goal. Those stakeholders must be informed by different means. Because these estimates serve the needs of the team, they need to be arrived at by the engineers who are going to be doing the work. Any observer who wishes to adjust the team-arrived estimates needs to sign up for executing those tasks. Snakes A \u201csnake\u201d is a small task that can be executed without interrupting or requiring attention from any other member of the team. Small wordsmithing updates to a design doc, or a runbook are good examples of a snake. Creating a new diff to fix a bug that one noticed while working on something unrelated \u2014 not a snake \u2014 because a diff review will require attention from another team member. Drive-by fixes inside a diff that\u2019s already going out for review can sometimes be a snake, depending on the complexity of the fix. The bottom line is, tasks that are sized as a \u201csnake\u201d do not need to be talked about, they just need to be done. As a corollary, there is no task that is 2 snakes or 7 snakes. It\u2019s either a snake or one of the creatures below. The idea of a \u201csnake\u201d comes from US Army Ranger training. They\u2019re told that if, when moving through an area, they see a snake that presents a danger, they should just kill the snake. They should not yell \u201ccheck this out, SNAKE!\u201d Jim Barksdale , who created two industries , making our jobs today possible, also used the analogy. \u201cDo not have a meeting about a snake. Do not form a committee to discuss a snake. Do not send out a survey about a snake. Just kill it.\u201d Tigers One \u201ctiger\u201d is an amount of work that is well understood by the team. It is a real creature with shared characteristics, so, once you have dispatched one, you have an idea of what it\u2019s like to dispatch another one. The team has to agree on what constitutes one tiger and comparing tigers across teams is a counterproductive exercise. E.g. 1 Tiger is: fixing an NPE (or language-equivalent), writing a unit test to ensure that the fix actually fixed something, reviewing that, landing the diff, following through to ensure it makes it to production. Tasks can be 1, 2, 3, 5, 8, or 13 tigers \u2014 nothing in between. If a task is estimated to be bigger than 13 tigers, it needs to be broken up. Dragons A dragon is a mythical creature; each one is unlike any other encountered before. Not enough is known at estimation time to meaningfully discuss the task and assign a number of tigers. When a team identifies a dragon, the next step is to create a task that generates information required to turn that dragon into tiger-sized tasks (often that de-dragonning task is best done as a time-boxed exercise, but that\u2019s a different discussion for a different time). posted on internal slack channel by @bootstraponline","title":"Estimation"},{"location":"onboarding/3_estimation/#estimating-eng-work-snakes-tigers-dragons","text":"","title":"Estimating Eng Work: Snakes, Tigers, Dragons"},{"location":"onboarding/3_estimation/#introduction","text":"This document describes a model for estimating the amount of work an engineering task requires. It is not about estimating how long that work will take \u2014 that is a different discussion for a different time. It is not about how to arrive at these estimates \u2014 that is also a different discussion for a different time. The primary aim of estimating size is to increase the velocity of those doing the work, i.e. engineering team. These estimates might give some amount of signal to other stakeholders, e.g. product management or eng leadership, but that is not the goal. Those stakeholders must be informed by different means. Because these estimates serve the needs of the team, they need to be arrived at by the engineers who are going to be doing the work. Any observer who wishes to adjust the team-arrived estimates needs to sign up for executing those tasks.","title":"Introduction"},{"location":"onboarding/3_estimation/#snakes","text":"A \u201csnake\u201d is a small task that can be executed without interrupting or requiring attention from any other member of the team. Small wordsmithing updates to a design doc, or a runbook are good examples of a snake. Creating a new diff to fix a bug that one noticed while working on something unrelated \u2014 not a snake \u2014 because a diff review will require attention from another team member. Drive-by fixes inside a diff that\u2019s already going out for review can sometimes be a snake, depending on the complexity of the fix. The bottom line is, tasks that are sized as a \u201csnake\u201d do not need to be talked about, they just need to be done. As a corollary, there is no task that is 2 snakes or 7 snakes. It\u2019s either a snake or one of the creatures below. The idea of a \u201csnake\u201d comes from US Army Ranger training. They\u2019re told that if, when moving through an area, they see a snake that presents a danger, they should just kill the snake. They should not yell \u201ccheck this out, SNAKE!\u201d Jim Barksdale , who created two industries , making our jobs today possible, also used the analogy. \u201cDo not have a meeting about a snake. Do not form a committee to discuss a snake. Do not send out a survey about a snake. Just kill it.\u201d","title":"Snakes"},{"location":"onboarding/3_estimation/#tigers","text":"One \u201ctiger\u201d is an amount of work that is well understood by the team. It is a real creature with shared characteristics, so, once you have dispatched one, you have an idea of what it\u2019s like to dispatch another one. The team has to agree on what constitutes one tiger and comparing tigers across teams is a counterproductive exercise. E.g. 1 Tiger is: fixing an NPE (or language-equivalent), writing a unit test to ensure that the fix actually fixed something, reviewing that, landing the diff, following through to ensure it makes it to production. Tasks can be 1, 2, 3, 5, 8, or 13 tigers \u2014 nothing in between. If a task is estimated to be bigger than 13 tigers, it needs to be broken up.","title":"Tigers"},{"location":"onboarding/3_estimation/#dragons","text":"A dragon is a mythical creature; each one is unlike any other encountered before. Not enough is known at estimation time to meaningfully discuss the task and assign a number of tigers. When a team identifies a dragon, the next step is to create a task that generates information required to turn that dragon into tiger-sized tasks (often that de-dragonning task is best done as a time-boxed exercise, but that\u2019s a different discussion for a different time). posted on internal slack channel by @bootstraponline","title":"Dragons"},{"location":"onboarding/4_buddy_system/","text":"Buddy System Every task should have at least two people. Two people working on the task ensures the task gets to Done quickly. There's no single point of failure. Cross training is another valuable aspect of the buddy system. Code reviews will be faster as the buddy doesn't have to context switch to review the code. Having a buddy working on a task provides emotional support and psychological safety as there's two people responsible for completing the work. Work on one task at a time. Stop starting, start finishing. WIP limits encourage us to finish work that\u2019s already in process before introducing more work into the system. The more work teams try to juggle at once, the harder it is for them to take work to the finish line. https://www.planview.com/resources/articles/wip-limits/ Buddies are empowered to determine how to best collaborate on each task. Ideas: - One writes code, the other reviews. - One writes the business logic, the other writes tests - Pair programming. One writes code, the other observes and guides. Roles can be switched anytime. - Visual Studio Code Live Share - One designs the APIs, the other implements We believe that people thrive on being trusted, on freedom, and on being able to make a difference. So we foster freedom and empowerment wherever we can. https://jobs.netflix.com/culture source https://github.com/platform-platform/monorepo/blob/master/docs/11_collaboration.md","title":"Buddy system"},{"location":"onboarding/4_buddy_system/#buddy-system","text":"Every task should have at least two people. Two people working on the task ensures the task gets to Done quickly. There's no single point of failure. Cross training is another valuable aspect of the buddy system. Code reviews will be faster as the buddy doesn't have to context switch to review the code. Having a buddy working on a task provides emotional support and psychological safety as there's two people responsible for completing the work. Work on one task at a time. Stop starting, start finishing. WIP limits encourage us to finish work that\u2019s already in process before introducing more work into the system. The more work teams try to juggle at once, the harder it is for them to take work to the finish line. https://www.planview.com/resources/articles/wip-limits/ Buddies are empowered to determine how to best collaborate on each task. Ideas: - One writes code, the other reviews. - One writes the business logic, the other writes tests - Pair programming. One writes code, the other observes and guides. Roles can be switched anytime. - Visual Studio Code Live Share - One designs the APIs, the other implements We believe that people thrive on being trusted, on freedom, and on being able to make a difference. So we foster freedom and empowerment wherever we can. https://jobs.netflix.com/culture source https://github.com/platform-platform/monorepo/blob/master/docs/11_collaboration.md","title":"Buddy System"},{"location":"onboarding/5_code_review/","text":"Code review Do your best and do not forget about some good practices, formalized and described below. Be aware Good code review requires a good understood of a problem. Code review is an important part of the development process and delivery. Do code review by steps Read the issue related to the pull request. Read the description of the pull request. Make sure the description is clear for you. Make sure the description is sufficient for you. Read the committed changes Make sure you are ok with implementation. Make sure the result of what changed is clear for you. Make sure the pull request really solves the related issue in the desired way. Make sure the testing scenario is clear for you. Do the tests step by step and collect the output. Make sure the result of tests is equal to expected Attach report about test results under pull request. Notify about fail If any step above will fail for you, try to reproduce it for sure and notify about a faced problem using pull request comment. Additionally, you may attach any of: - Github commitable suggestion if possible, and you already know it. - Description of what is unclear for you. - Description of what should be changed or what is missing. - Any additional information important for final quality. Some tips Make sure you clearly understand what you are reviewing. Don't be afraid of paying attention to details if feel they are important. It's always a good idea to open IDE, try to identify the root, and trace the implementation if the pull request is not trivial. Ask if you are not sure, suggest if you are sure.","title":"Code review"},{"location":"onboarding/5_code_review/#code-review","text":"Do your best and do not forget about some good practices, formalized and described below.","title":"Code review"},{"location":"onboarding/5_code_review/#be-aware","text":"Good code review requires a good understood of a problem. Code review is an important part of the development process and delivery.","title":"Be aware"},{"location":"onboarding/5_code_review/#do-code-review-by-steps","text":"Read the issue related to the pull request. Read the description of the pull request. Make sure the description is clear for you. Make sure the description is sufficient for you. Read the committed changes Make sure you are ok with implementation. Make sure the result of what changed is clear for you. Make sure the pull request really solves the related issue in the desired way. Make sure the testing scenario is clear for you. Do the tests step by step and collect the output. Make sure the result of tests is equal to expected Attach report about test results under pull request.","title":"Do code review by steps"},{"location":"onboarding/5_code_review/#notify-about-fail","text":"If any step above will fail for you, try to reproduce it for sure and notify about a faced problem using pull request comment. Additionally, you may attach any of: - Github commitable suggestion if possible, and you already know it. - Description of what is unclear for you. - Description of what should be changed or what is missing. - Any additional information important for final quality.","title":"Notify about fail"},{"location":"onboarding/5_code_review/#some-tips","text":"Make sure you clearly understand what you are reviewing. Don't be afraid of paying attention to details if feel they are important. It's always a good idea to open IDE, try to identify the root, and trace the implementation if the pull request is not trivial. Ask if you are not sure, suggest if you are sure.","title":"Some tips"},{"location":"refactor/diagram/","text":"Flank Activity Diagram Description The diagram shows abstract view of all possible flank activities. The #LightGreen color relates to CLI commands. The #LightBlue color relates to domain scope. The default yellow color reserved for top-level functions that are not atomic and can be converted to frame . Frame can represent a package or rather the top-level function composed of other functions. The #snow color relates to low-level functions Low-level functions should be atomic ( can import only non-domain utils and third-party libraries? - consider ). Low-level functions cannot be converted to frame The complete activity diagram will contain only low-level activities represented by #snow color. All top-level functions represented by yellow activities contains hidden complexity and should be converted to frames. Layers presentation (CLI) -> domain -> utils -> data","title":"Flank Activity Diagram"},{"location":"refactor/diagram/#flank-activity-diagram","text":"","title":"Flank Activity Diagram"},{"location":"refactor/diagram/#description","text":"The diagram shows abstract view of all possible flank activities. The #LightGreen color relates to CLI commands. The #LightBlue color relates to domain scope. The default yellow color reserved for top-level functions that are not atomic and can be converted to frame . Frame can represent a package or rather the top-level function composed of other functions. The #snow color relates to low-level functions Low-level functions should be atomic ( can import only non-domain utils and third-party libraries? - consider ). Low-level functions cannot be converted to frame The complete activity diagram will contain only low-level activities represented by #snow color. All top-level functions represented by yellow activities contains hidden complexity and should be converted to frames.","title":"Description"},{"location":"refactor/diagram/#layers","text":"presentation (CLI) -> domain -> utils -> data","title":"Layers"},{"location":"refactor/important_cli_commands/","text":"The list of Flank CLI commands that calls domain use cases auth/LoginCommand.kt firebase/CancelCommand.kt firebase/RefreshCommand.kt firebase/test/android/configuration/AndroidLocalesDescribeCommand.kt firebase/test/android/configuration/AndroidLocalesListCommand.kt firebase/test/android/models/AndroidModelDescribeCommand.kt firebase/test/android/models/AndroidModelsListCommand.kt firebase/test/android/orientations/AndroidOrientationsListCommand.kt firebase/test/android/versions/AndroidVersionsDescribeCommand.kt firebase/test/android/versions/AndroidVersionsListCommand.kt firebase/test/android/AndroidDoctorCommand.kt firebase/test/android/AndroidRunCommand.kt firebase/test/android/AndroidTestEnvironmentCommand.kt firebase/test/ios/configuration/IosLocalesDescribeCommand.kt firebase/test/ios/configuration/IosLocalesListCommand.kt firebase/test/ios/models/IosModelDescribeCommand.kt firebase/test/ios/models/IosModelsListCommand.kt firebase/test/ios/orientations/IosOrientationsListCommand.kt firebase/test/ios/versions/IosVersionsDescribeCommand.kt firebase/test/ios/versions/IosVersionsListCommand.kt firebase/test/ios/IosDoctorCommand.kt firebase/test/ios/IosRunCommand.kt firebase/test/ios/IosTestEnvironmentCommand.kt firebase/test/ipblocks/IPBlocksListCommand.kt firebase/test/networkprofiles/NetworkProfilesDescribeCommand.kt firebase/test/networkprofiles/NetworkProfilesListCommand.kt firebase/test/providedsoftware/ProvidedSoftwareListCommand.kt","title":"Important cli commands"},{"location":"refactor/investigation/","text":"Flank investigation This document contains investigation of flank architecture, layers and package structure. Motivation Currently, the flank code is not well documented, and the only source of true is the implementation. The code base has grown over the last year, now it's almost impossible to keep in mind whole project. So to make further work more doable and convenient, it's necessary to identify hidden constraints in code, expose them in documentation and do refactor. This document is and entity point for further improvements. Layers Presentation Issues ftl.Main command shouldn't be bound with main function through companion object due to single responsibility principle. Some commands that can run domain code are doing too much. Some of composing commands seem to be in wrong package. CLI commands Flank commands tree. ftl/cli/ \u251c\u2500\u2500 AuthCommand.kt / \u251c\u2500\u2500 FirebaseCommand.kt / \u251c\u2500\u2500 auth \u2502 \u2514\u2500\u2500 LoginCommand.kt ? ! \u2514\u2500\u2500 firebase \u251c\u2500\u2500 CancelCommand.kt ? ! \u251c\u2500\u2500 RefreshCommand.kt ? ! \u251c\u2500\u2500 TestCommand.kt / \u2514\u2500\u2500 test \u251c\u2500\u2500 AndroidCommand.kt / \u251c\u2500\u2500 CommandUtil.kt \u251c\u2500\u2500 CommonRunCommand.kt \u251c\u2500\u2500 IPBlocksCommand.kt / \u251c\u2500\u2500 IosCommand.kt / \u251c\u2500\u2500 NetworkProfilesCommand.kt / \u251c\u2500\u2500 ProvidedSoftwareCommand.kt / \u251c\u2500\u2500 android \u2502 \u251c\u2500\u2500 AndroidDoctorCommand.kt ? ! \u2502 \u251c\u2500\u2500 AndroidRunCommand.kt ? ! \u2502 \u251c\u2500\u2500 AndroidTestEnvironmentCommand.kt ? ! \u2502 \u251c\u2500\u2500 configuration \u2502 \u2502 \u251c\u2500\u2500 AndroidLocalesCommand.kt / ^ \u2502 \u2502 \u251c\u2500\u2500 AndroidLocalesDescribeCommand.kt ? ! \u2502 \u2502 \u2514\u2500\u2500 AndroidLocalesListCommand.kt ? ! \u2502 \u251c\u2500\u2500 models \u2502 \u2502 \u251c\u2500\u2500 AndroidModelDescribeCommand.kt ? ! \u2502 \u2502 \u251c\u2500\u2500 AndroidModelsCommand.kt / ^ \u2502 \u2502 \u2514\u2500\u2500 AndroidModelsListCommand.kt ? ! \u2502 \u251c\u2500\u2500 orientations \u2502 \u2502 \u251c\u2500\u2500 AndroidOrientationsCommand.kt / ^ \u2502 \u2502 \u2514\u2500\u2500 AndroidOrientationsListCommand.kt ? ! \u2502 \u2514\u2500\u2500 versions \u2502 \u251c\u2500\u2500 AndroidVersionsCommand.kt / ^ \u2502 \u251c\u2500\u2500 AndroidVersionsDescribeCommand.kt ? ! \u2502 \u2514\u2500\u2500 AndroidVersionsListCommand.kt ? ! \u251c\u2500\u2500 ios \u2502 \u251c\u2500\u2500 IosDoctorCommand.kt ? ! \u2502 \u251c\u2500\u2500 IosRunCommand.kt ? ! \u2502 \u251c\u2500\u2500 IosTestEnvironmentCommand.kt ? ! \u2502 \u251c\u2500\u2500 configuration \u2502 \u2502 \u251c\u2500\u2500 IosLocalesCommand.kt / ^ \u2502 \u2502 \u251c\u2500\u2500 IosLocalesDescribeCommand.kt ? ! \u2502 \u2502 \u2514\u2500\u2500 IosLocalesListCommand.kt ? ! \u2502 \u251c\u2500\u2500 models \u2502 \u2502 \u251c\u2500\u2500 IosModelDescribeCommand.kt ? ! \u2502 \u2502 \u251c\u2500\u2500 IosModelsCommand.kt / ^ \u2502 \u2502 \u2514\u2500\u2500 IosModelsListCommand.kt ? ! \u2502 \u251c\u2500\u2500 orientations \u2502 \u2502 \u251c\u2500\u2500 IosOrientationsCommand.kt / ^ \u2502 \u2502 \u2514\u2500\u2500 IosOrientationsListCommand.kt ? ! \u2502 \u2514\u2500\u2500 versions \u2502 \u251c\u2500\u2500 IosVersionsCommand.kt / ^ \u2502 \u251c\u2500\u2500 IosVersionsDescribeCommand.kt ? ! \u2502 \u2514\u2500\u2500 IosVersionsListCommand.kt ? ! \u251c\u2500\u2500 ipblocks \u2502 \u2514\u2500\u2500 IPBlocksListCommand.kt ? \u251c\u2500\u2500 networkprofiles \u2502 \u251c\u2500\u2500 NetworkProfilesDescribeCommand.kt ? \u2502 \u2514\u2500\u2500 NetworkProfilesListCommand.kt ? \u2514\u2500\u2500 providedsoftware \u2514\u2500\u2500 ProvidedSoftwareListCommand.kt ? command that: ? is running domain code / routes to subcommands ! is doing too much in run function ^ should be moved up in package hierarchy Domain Issues The core domain api is not easy to identify. The domain layer of flank is not clearly separated of CLI and external APIs. Domain logic is huge and complicated but there is lack of diagram for visualize it. Data & Adapters Issues External APIs are not hidden behind interfaces External API wrappers / adapters are not clearly separated from other layers. External API libs com.google.testing:firebase_apis:test_api com.google.api-client:google-api-client com.google.auth:google-auth-library-oauth2-http com.google.cloud:google-cloud-nio com.google.cloud:google-cloud-storage com.google.apis:google-api-services-toolresults io.sentry:sentry com.mixpanel:mixpanel-java List of external API usages in files Based on google_api_usecases com.google.api.client.http GoogleApiLogger.kt ftl.android AndroidCatalog.kt ftl.args ArgsHelper.kt ftl.config Credentials.kt FtlConstants.kt ftl.environment ListIPBlocks.kt $ ListLocales.kt $ LocalesDescription.kt $ NetworkProfileDescription.kt $ ftl.environment.android AndroidModelDescription.kt $ AndroidSoftwareVersionDescription.kt $ ListAndroidDevices.kt $ ListAndroidSofwareVersions.kt $ ftl.environment.common ListNetworkConfiguration.kt $ ListOrientations.kt $ ListProvidedSoftware.kt $ ftl.environment.ios IosModelDescription.kt $ IosSoftwareVersionDescription.kt $ ListIOsDevices.kt $ ListIOsSofwareVersions.kt $ ftl.gc GcAndroidDevice.kt $ GcAndroidTestMatrix.kt GcIosMatrix.kt $ GcIosTestMatrix.kt GcStorage.kt GcTesting.kt $ GcTestMatrix.kt GcToolResults.kt UserAuth.kt ftl/gc/Utils.kt $ ftl.gc.android CreateAndroidInstrumentationTest.kt $ CreateAndroidLoopTest.kt $ CreateAndroidRobotTest.kt $ SetupAndroidTest.kt $ SetupEnvironmentVariables.kt $ ftl/gc/android/Utils.kt $ ftl.gc.ios SetupIosTest.kt $ ftl.http ExecuteWithRetry.kt HttpTimeoutIncrease.kt ftl.ios IosCatalog.kt $ ftl.json MatrixMap.kt $ OutcomeDetailsFormatter.kt $ SavedMatrix.kt $ ftl.log Loggers.kt ftl.mock MockServer.kt ftl.reports HtmlErrorReport.kt ftl.reports.api CreateJUnitTestCase.kt $ CreateJUnitTestResult.kt $ CreateTestExecutionData.kt CreateTestSuiteOverviewData.kt $ PerformanceMetrics.kt PrepareForJUnitResult.kt ProcessFromApi.kt $ ftl/reports/api/Utils.kt ftl.reports.api.data TestExecutionData.kt $ TestSuiteOverviewData.kt $ ftl.reports.outcome BillableMinutes.kt $ CreateMatrixOutcomeSummary.kt $ CreateTestSuiteOverviewData.kt $ TestOutcome.kt $ TestOutcomeContext.kt $ ftl/reports/outcome/Util.kt $ ftl.reports.util ReportManager.kt $ ftl.run RefreshLastRun.kt $ ftl.run.common FetchArtifacts.kt PollMatrices.kt ftl.run.platform RunAndroidTests.kt $ ftl.run.platform.common AfterRunTests.kt $ ftl.run.status ExecutionStatusListPrinter.kt $ TestMatrixStatusPrinter.kt $ ftl.util MatrixState.kt $ TestMatrixExtension.kt $ where $ - only operates on API structures, not call methods directly Google API use cases From the CLI command point of view. The most nested points refer API calls. auth/LoginCommand.kt Authorizing google account using OAuth2 for getting credentials. firebase/CancelCommand.kt Sending cancel request using projectId and testMatrixId firebase/RefreshCommand.kt ? Getting current test matrices' status for updating matrix file if needed Polling the matrices' status for live logging until all matrices are not finished. Downloading test artifacts from bucket firebase/test/providedsoftware/ProvidedSoftwareListCommand.kt Getting softwareCatalog from testEnvironmentCatalog firebase/test/ipblocks/IPBlocksListCommand.kt Getting orchestratorVersion from testEnvironmentCatalog firebase/test/networkprofiles/NetworkProfilesDescribeCommand.kt Getting configurations from testEnvironmentCatalog through networkConfigurationCatalog firebase/test/networkprofiles/NetworkProfilesListCommand.kt Getting configurations from testEnvironmentCatalog through networkConfigurationCatalog firebase/test/android/configuration/AndroidLocalesDescribeCommand.kt Getting android device catalog for obtain locales firebase/test/android/configuration/AndroidLocalesListCommand.kt Getting android device catalog for obtain locales firebase/test/android/models/AndroidModelDescribeCommand.kt Getting android device catalog for obtain models firebase/test/android/models/AndroidModelsListCommand.kt Getting android device catalog for obtain models firebase/test/android/orientations/AndroidOrientationsListCommand.kt Getting android device catalog for obtain orientations firebase/test/android/versions/AndroidVersionsDescribeCommand.kt Getting android device catalog for obtain versions firebase/test/android/versions/AndroidVersionsListCommand.kt Getting android device catalog for obtain versions firebase/test/android/AndroidTestEnvironmentCommand.kt AndroidModelsListCommand AndroidVersionsListCommand AndroidLocalesListCommand ProvidedSoftwareListCommand NetworkProfilesListCommand AndroidOrientationsListCommand IPBlocksListCommand firebase/test/ios/configuration/IosLocalesDescribeCommand.kt Getting ios device catalog for obtain locales firebase/test/ios/configuration/IosLocalesListCommand.kt Getting ios device catalog for obtain locales firebase/test/ios/models/IosModelDescribeCommand.kt Getting ios device catalog for obtain models firebase/test/ios/models/IosModelsListCommand.kt Getting ios device catalog for obtain models firebase/test/ios/orientations/IosOrientationsListCommand.kt Getting ios device catalog for obtain orientations firebase/test/ios/versions/IosVersionsDescribeCommand.kt Getting ios device catalog for obtain versions firebase/test/ios/versions/IosVersionsListCommand.kt Getting ios device catalog for obtain versions firebase/test/ios/IosTestEnvironmentCommand.kt IosModelsListCommand IosVersionsListCommand IosLocalesListCommand ProvidedSoftwareListCommand NetworkProfilesListCommand IosOrientationsListCommand IPBlocksListCommand firebase/test/android/AndroidRunCommand.kt Validation Checking if bucket exist (common args validation) Getting android device catalog for check supported devices Running android tests Preparing data Uploading files Creating android test contexts downloading apks if needed Dumping shards Uploading dumped shards if needed Uploading Uploading apks if needed & depending on test context Building and running android test matrix After run test Uploading session ID Printing matrices web links getOrUpdateWebLink Getting test matrices Pooling matrices Getting test matrices Generating report Parsing test suite Getting test matrices Creating JUnit test result Uploading each report Cost report Matrix results report Html error report JUnit report Getting test matrices for test executions Processing junit results processing full junit result Creating JUnit test result Create and upload performance metrics Upload matrices ids Fetching artifacts Printing matrices web links get or update web for each matrix Getting test matrices firebase/test/ios/IosRunCommand.kt validation Checking if bucket exist (common args validation) Getting ios device catalog for check supported devices Running ios tests Preparing data Uploading files Dump shards if needed Uploading dumped shards if needed Creating ios test contexts Creating xctest context Uploading xctest files Creating gameloop test context Uploading app file Building and running android test matrix After run test (from this point the path is same as for android) where ? - investigate if implementation is correct or is performing some not necessary operations. Google API usage function call tree LoginCommand -> UserAuth/request CancelCommand getLastArgs IosArgs/validateRefresh -> IosArgs/assertDevicesSupported AndroidArgs/validate AndroidArgs/assertDevicesSupported AndroidCatalog/supportedDeviceConfig -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidModelIds -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidVersionIds -> AndroidCatalog/deviceCatalog(projectId).versions AndroidCatalog/getSupportedVersionId -> AndroidCatalog/deviceCatalog(projectId).models IArgs/checkResultsDirUnique -> GcStorage/exist cancelLastRun -> cancelMatrices -> GcTestMatrix/cancel RefreshCommand -> refreshLastRun getLastArgs IosArgs/validateRefresh -> IosArgs/assertDevicesSupported AndroidArgs/validate AndroidArgs/assertDevicesSupported AndroidCatalog/supportedDeviceConfig -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidModelIds -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidVersionIds -> AndroidCatalog/deviceCatalog(projectId).versions AndroidCatalog/getSupportedVersionId -> AndroidCatalog/deviceCatalog(projectId).models IArgs/checkResultsDirUnique -> GcStorage/exist refreshMatrices GcTestMatrix/refresh SavedMatrix/updateWithMatrix -> SavedMatrix/updatedSavedMatrix -> TestMatrix/fetchTestOutcomeContext TestMatrix/getToolResultsIds GcToolResults/listAllEnvironments GcToolResults/listAllSteps ProvidedSoftwareListCommand -> providedSoftwareAsTable -> getProvidedSoftware IPBlocksListCommand -> ipBlocksListAsTable -> deviceIPBlocks NetworkProfilesDescribeCommand -> networkProfileDescription -> getNetworkConfiguration NetworkProfilesListCommand -> networkConfigurationAsTable -> getNetworkConfiguration AndroidLocalesDescribeCommand -> AndroidCatalog/getLocaleDescription -> AndroidCatalog/getLocales AndroidLocalesListCommand -> AndroidCatalog/localesAsTable -> AndroidCatalog/getLocales AndroidModelDescribeCommand -> AndroidCatalog/describeModel -> AndroidCatalog/getModels AndroidModelsListCommand -> AndroidCatalog/devicesCatalogAsTable -> AndroidCatalog/getModels AndroidOrientationsListCommand -> AndroidCatalog/supportedOrientationsAsTable -> AndroidCatalog/deviceCatalog AndroidVersionsDescribeCommand -> AndroidCatalog/describeSoftwareVersion -> AndroidCatalog/getVersionsList AndroidVersionsListCommand -> AndroidCatalog/supportedVersionsAsTable -> AndroidCatalog/getVersionsList AndroidTestEnvironmentCommand AndroidModelsListCommand AndroidVersionsListCommand AndroidLocalesListCommand ProvidedSoftwareListCommand NetworkProfilesListCommand AndroidOrientationsListCommand IPBlocksListCommand IosLocalesDescribeCommand -> IosCatalog/getLocaleDescription -> IosCatalog/getLocales IosLocalesListCommand -> IosCatalog/localesAsTable -> IosCatalog/iosDeviceCatalog IosModelDescribeCommand -> IosCatalog/describeModel -> IosCatalog/getModels IosModelsListCommand -> IosCatalog/devicesCatalogAsTable -> IosCatalog/getModels IosOrientationsListCommand -> IosCatalog/supportedOrientationsAsTable -> IosCatalog/iosDeviceCatalog IosVersionsDescribeCommand -> IosCatalog/describeSoftwareVersion -> IosCatalog/getVersionsList IosVersionsListCommand -> IosCatalog/softwareVersionsAsTable -> IosCatalog/getVersionsList IosTestEnvironmentCommand IosModelsListCommand IosVersionsListCommand IosLocalesListCommand ProvidedSoftwareListCommand NetworkProfilesListCommand IosOrientationsListCommand IPBlocksListCommand AndroidRunCommand AndroidArgs/validate AndroidArgs/assertDevicesSupported AndroidCatalog/supportedDeviceConfig AndroidCatalog/androidModelIds AndroidCatalog/androidVersionIds AndroidCatalog/getSupportedVersionId IArgs/checkResultsDirUnique -> GcStorage/exist AndroidArgs/runAndroidTests GcAndroidDevice/build GcToolResults/createToolResultsHistory IArgs/uploadOtherFiles -> GcStorage/upload AndroidArgs/uploadAdditionalApks -> List<String>/uploadToGcloudIfNeeded -> FileReference/uploadIfNeeded -> GcStorage/upload AndroidArgs/uploadObbFiles -> GcStorage/upload AndroidArgs/createAndroidTestContexts -> List<AndroidTestContext>/setupShards -> InstrumentationTestContext/downloadApks -> FileReference/downloadIfNeeded -> GcStorage/download List<AndroidTestContext>/dumpShards -> GcStorage/upload List<AndroidTestContext>/upload -> AndroidTestContext/upload InstrumentationTestContext/upload -> FileReference/uploadIfNeeded -> GcStorage/upload RoboTestContext/upload -> FileReference/uploadIfNeeded -> GcStorage/upload SanityRoboTestContext/upload -> FileReference/uploadIfNeeded -> GcStorage/upload GameLoopContext/upload -> FileReference/uploadIfNeeded -> GcStorage/upload GcAndroidTestMatrix/build AbstractGoogleJsonClientRequest<T>/executeWithRetry IArgs/afterRunTests IArgs/uploadSessionId -> GcStorage/upload MatrixMap/printMatricesWebLinks -> getOrUpdateWebLink -> GcTestMatrix/refresh pollMatrices -> matrixChangesFlow -> GcTestMatrix/refresh Iterable<TestMatrix>/updateMatrixMap -> SavedMatrix/updateWithMatrix -> TestMatrix/fetchTestOutcomeContext TestMatrix/getToolResultsIds GcToolResults/listAllEnvironments GcToolResults/listAllSteps ReportManager/generate ReportManager/parseTestSuite refreshMatricesAndGetExecutions -> refreshTestMatrices -> GcTestMatrix/refresh List<TestExecution>/createJUnitTestResult -> List<TestExecution>/createTestExecutionDataListAsync -> TestExecution/createTestExecutionData -> getAsync GcToolResults/listTestCases GcToolResults/getStepResult CostReport.run -> GcStorage/uploadReportResult MatrixResultsReport.run -> GcStorage/uploadReportResult HtmlErrorReport.run -> GcStorage/uploadReportResult JUnitReport.run -> GcStorage/uploadReportResult refreshMatricesAndGetExecutions -> refreshTestMatrices -> GcTestMatrix/refresh ReportManager/processJunitResults ReportManager/processFullJunitResult -> List<TestExecution>/createJUnitTestResult -> List<TestExecution>/createTestExecutionDataListAsync -> TestExecution/createTestExecutionData -> getAsync GcToolResults/listTestCases GcToolResults/getStepResult FullJUnitReport.run -> GcStorage.uploadReportResult ReportManager/createAndUploadPerformanceMetricsForAndroid -> List<Pair<TestExecution, String>>.getAndUploadPerformanceMetrics -> TestExecution.getPerformanceMetric -> GcToolResults.getPerformanceMetric PerfMetricsSummary.upload -> GcStorage.uploadPerformanceMetrics GcStorage/uploadMatricesId fetchArtifacts Storage.BlobListOption.fields Storage.BlobListOption.prefix GcStorage.storage.list Blob.downloadTo MatrixMap/printMatricesWebLinks -> getOrUpdateWebLink -> GcTestMatrix/refresh IosRunCommand IosArgs/validate IosArgs/assertDevicesSupported IosCatalog/supportedDevice IosCatalog/Device/getSupportedVersionId IArgs/checkResultsDirUnique -> GcStorage/exist IosArgs/runIosTests GcIosMatrix/build GcToolResults/createToolResultsHistory IArgs/uploadOtherFiles -> GcStorage/upload IosArgs.uploadAdditionalIpas IosArgs.dumpShardsIfXcTest -> GcStorage/upload IosArgs/createIosTestContexts IosArgs.createXcTestContexts IArgs.uploadIfNeeded -> FileReference.uploadIfNeeded -> GcStorage.upload GcStorage.uploadXCTestFile IosArgs.createGameloopTestContexts IArgs.uploadIfNeeded -> FileReference.uploadIfNeeded -> GcStorage.upload GcIosTestMatrix/build AbstractGoogleJsonClientRequest<T>/executeWithRetry IArgs/afterRunTests - the rest of steps are same as for android","title":"Flank investigation"},{"location":"refactor/investigation/#flank-investigation","text":"This document contains investigation of flank architecture, layers and package structure.","title":"Flank investigation"},{"location":"refactor/investigation/#motivation","text":"Currently, the flank code is not well documented, and the only source of true is the implementation. The code base has grown over the last year, now it's almost impossible to keep in mind whole project. So to make further work more doable and convenient, it's necessary to identify hidden constraints in code, expose them in documentation and do refactor. This document is and entity point for further improvements.","title":"Motivation"},{"location":"refactor/investigation/#layers","text":"","title":"Layers"},{"location":"refactor/investigation/#presentation","text":"","title":"Presentation"},{"location":"refactor/investigation/#issues","text":"ftl.Main command shouldn't be bound with main function through companion object due to single responsibility principle. Some commands that can run domain code are doing too much. Some of composing commands seem to be in wrong package.","title":"Issues"},{"location":"refactor/investigation/#cli-commands","text":"Flank commands tree. ftl/cli/ \u251c\u2500\u2500 AuthCommand.kt / \u251c\u2500\u2500 FirebaseCommand.kt / \u251c\u2500\u2500 auth \u2502 \u2514\u2500\u2500 LoginCommand.kt ? ! \u2514\u2500\u2500 firebase \u251c\u2500\u2500 CancelCommand.kt ? ! \u251c\u2500\u2500 RefreshCommand.kt ? ! \u251c\u2500\u2500 TestCommand.kt / \u2514\u2500\u2500 test \u251c\u2500\u2500 AndroidCommand.kt / \u251c\u2500\u2500 CommandUtil.kt \u251c\u2500\u2500 CommonRunCommand.kt \u251c\u2500\u2500 IPBlocksCommand.kt / \u251c\u2500\u2500 IosCommand.kt / \u251c\u2500\u2500 NetworkProfilesCommand.kt / \u251c\u2500\u2500 ProvidedSoftwareCommand.kt / \u251c\u2500\u2500 android \u2502 \u251c\u2500\u2500 AndroidDoctorCommand.kt ? ! \u2502 \u251c\u2500\u2500 AndroidRunCommand.kt ? ! \u2502 \u251c\u2500\u2500 AndroidTestEnvironmentCommand.kt ? ! \u2502 \u251c\u2500\u2500 configuration \u2502 \u2502 \u251c\u2500\u2500 AndroidLocalesCommand.kt / ^ \u2502 \u2502 \u251c\u2500\u2500 AndroidLocalesDescribeCommand.kt ? ! \u2502 \u2502 \u2514\u2500\u2500 AndroidLocalesListCommand.kt ? ! \u2502 \u251c\u2500\u2500 models \u2502 \u2502 \u251c\u2500\u2500 AndroidModelDescribeCommand.kt ? ! \u2502 \u2502 \u251c\u2500\u2500 AndroidModelsCommand.kt / ^ \u2502 \u2502 \u2514\u2500\u2500 AndroidModelsListCommand.kt ? ! \u2502 \u251c\u2500\u2500 orientations \u2502 \u2502 \u251c\u2500\u2500 AndroidOrientationsCommand.kt / ^ \u2502 \u2502 \u2514\u2500\u2500 AndroidOrientationsListCommand.kt ? ! \u2502 \u2514\u2500\u2500 versions \u2502 \u251c\u2500\u2500 AndroidVersionsCommand.kt / ^ \u2502 \u251c\u2500\u2500 AndroidVersionsDescribeCommand.kt ? ! \u2502 \u2514\u2500\u2500 AndroidVersionsListCommand.kt ? ! \u251c\u2500\u2500 ios \u2502 \u251c\u2500\u2500 IosDoctorCommand.kt ? ! \u2502 \u251c\u2500\u2500 IosRunCommand.kt ? ! \u2502 \u251c\u2500\u2500 IosTestEnvironmentCommand.kt ? ! \u2502 \u251c\u2500\u2500 configuration \u2502 \u2502 \u251c\u2500\u2500 IosLocalesCommand.kt / ^ \u2502 \u2502 \u251c\u2500\u2500 IosLocalesDescribeCommand.kt ? ! \u2502 \u2502 \u2514\u2500\u2500 IosLocalesListCommand.kt ? ! \u2502 \u251c\u2500\u2500 models \u2502 \u2502 \u251c\u2500\u2500 IosModelDescribeCommand.kt ? ! \u2502 \u2502 \u251c\u2500\u2500 IosModelsCommand.kt / ^ \u2502 \u2502 \u2514\u2500\u2500 IosModelsListCommand.kt ? ! \u2502 \u251c\u2500\u2500 orientations \u2502 \u2502 \u251c\u2500\u2500 IosOrientationsCommand.kt / ^ \u2502 \u2502 \u2514\u2500\u2500 IosOrientationsListCommand.kt ? ! \u2502 \u2514\u2500\u2500 versions \u2502 \u251c\u2500\u2500 IosVersionsCommand.kt / ^ \u2502 \u251c\u2500\u2500 IosVersionsDescribeCommand.kt ? ! \u2502 \u2514\u2500\u2500 IosVersionsListCommand.kt ? ! \u251c\u2500\u2500 ipblocks \u2502 \u2514\u2500\u2500 IPBlocksListCommand.kt ? \u251c\u2500\u2500 networkprofiles \u2502 \u251c\u2500\u2500 NetworkProfilesDescribeCommand.kt ? \u2502 \u2514\u2500\u2500 NetworkProfilesListCommand.kt ? \u2514\u2500\u2500 providedsoftware \u2514\u2500\u2500 ProvidedSoftwareListCommand.kt ? command that: ? is running domain code / routes to subcommands ! is doing too much in run function ^ should be moved up in package hierarchy","title":"CLI commands"},{"location":"refactor/investigation/#domain","text":"","title":"Domain"},{"location":"refactor/investigation/#issues_1","text":"The core domain api is not easy to identify. The domain layer of flank is not clearly separated of CLI and external APIs. Domain logic is huge and complicated but there is lack of diagram for visualize it.","title":"Issues"},{"location":"refactor/investigation/#data-adapters","text":"","title":"Data &amp; Adapters"},{"location":"refactor/investigation/#issues_2","text":"External APIs are not hidden behind interfaces External API wrappers / adapters are not clearly separated from other layers.","title":"Issues"},{"location":"refactor/investigation/#external-api-libs","text":"com.google.testing:firebase_apis:test_api com.google.api-client:google-api-client com.google.auth:google-auth-library-oauth2-http com.google.cloud:google-cloud-nio com.google.cloud:google-cloud-storage com.google.apis:google-api-services-toolresults io.sentry:sentry com.mixpanel:mixpanel-java","title":"External API libs"},{"location":"refactor/investigation/#list-of-external-api-usages-in-files","text":"Based on google_api_usecases com.google.api.client.http GoogleApiLogger.kt ftl.android AndroidCatalog.kt ftl.args ArgsHelper.kt ftl.config Credentials.kt FtlConstants.kt ftl.environment ListIPBlocks.kt $ ListLocales.kt $ LocalesDescription.kt $ NetworkProfileDescription.kt $ ftl.environment.android AndroidModelDescription.kt $ AndroidSoftwareVersionDescription.kt $ ListAndroidDevices.kt $ ListAndroidSofwareVersions.kt $ ftl.environment.common ListNetworkConfiguration.kt $ ListOrientations.kt $ ListProvidedSoftware.kt $ ftl.environment.ios IosModelDescription.kt $ IosSoftwareVersionDescription.kt $ ListIOsDevices.kt $ ListIOsSofwareVersions.kt $ ftl.gc GcAndroidDevice.kt $ GcAndroidTestMatrix.kt GcIosMatrix.kt $ GcIosTestMatrix.kt GcStorage.kt GcTesting.kt $ GcTestMatrix.kt GcToolResults.kt UserAuth.kt ftl/gc/Utils.kt $ ftl.gc.android CreateAndroidInstrumentationTest.kt $ CreateAndroidLoopTest.kt $ CreateAndroidRobotTest.kt $ SetupAndroidTest.kt $ SetupEnvironmentVariables.kt $ ftl/gc/android/Utils.kt $ ftl.gc.ios SetupIosTest.kt $ ftl.http ExecuteWithRetry.kt HttpTimeoutIncrease.kt ftl.ios IosCatalog.kt $ ftl.json MatrixMap.kt $ OutcomeDetailsFormatter.kt $ SavedMatrix.kt $ ftl.log Loggers.kt ftl.mock MockServer.kt ftl.reports HtmlErrorReport.kt ftl.reports.api CreateJUnitTestCase.kt $ CreateJUnitTestResult.kt $ CreateTestExecutionData.kt CreateTestSuiteOverviewData.kt $ PerformanceMetrics.kt PrepareForJUnitResult.kt ProcessFromApi.kt $ ftl/reports/api/Utils.kt ftl.reports.api.data TestExecutionData.kt $ TestSuiteOverviewData.kt $ ftl.reports.outcome BillableMinutes.kt $ CreateMatrixOutcomeSummary.kt $ CreateTestSuiteOverviewData.kt $ TestOutcome.kt $ TestOutcomeContext.kt $ ftl/reports/outcome/Util.kt $ ftl.reports.util ReportManager.kt $ ftl.run RefreshLastRun.kt $ ftl.run.common FetchArtifacts.kt PollMatrices.kt ftl.run.platform RunAndroidTests.kt $ ftl.run.platform.common AfterRunTests.kt $ ftl.run.status ExecutionStatusListPrinter.kt $ TestMatrixStatusPrinter.kt $ ftl.util MatrixState.kt $ TestMatrixExtension.kt $ where $ - only operates on API structures, not call methods directly","title":"List of external API usages in files"},{"location":"refactor/investigation/#google-api-use-cases","text":"From the CLI command point of view. The most nested points refer API calls. auth/LoginCommand.kt Authorizing google account using OAuth2 for getting credentials. firebase/CancelCommand.kt Sending cancel request using projectId and testMatrixId firebase/RefreshCommand.kt ? Getting current test matrices' status for updating matrix file if needed Polling the matrices' status for live logging until all matrices are not finished. Downloading test artifacts from bucket firebase/test/providedsoftware/ProvidedSoftwareListCommand.kt Getting softwareCatalog from testEnvironmentCatalog firebase/test/ipblocks/IPBlocksListCommand.kt Getting orchestratorVersion from testEnvironmentCatalog firebase/test/networkprofiles/NetworkProfilesDescribeCommand.kt Getting configurations from testEnvironmentCatalog through networkConfigurationCatalog firebase/test/networkprofiles/NetworkProfilesListCommand.kt Getting configurations from testEnvironmentCatalog through networkConfigurationCatalog firebase/test/android/configuration/AndroidLocalesDescribeCommand.kt Getting android device catalog for obtain locales firebase/test/android/configuration/AndroidLocalesListCommand.kt Getting android device catalog for obtain locales firebase/test/android/models/AndroidModelDescribeCommand.kt Getting android device catalog for obtain models firebase/test/android/models/AndroidModelsListCommand.kt Getting android device catalog for obtain models firebase/test/android/orientations/AndroidOrientationsListCommand.kt Getting android device catalog for obtain orientations firebase/test/android/versions/AndroidVersionsDescribeCommand.kt Getting android device catalog for obtain versions firebase/test/android/versions/AndroidVersionsListCommand.kt Getting android device catalog for obtain versions firebase/test/android/AndroidTestEnvironmentCommand.kt AndroidModelsListCommand AndroidVersionsListCommand AndroidLocalesListCommand ProvidedSoftwareListCommand NetworkProfilesListCommand AndroidOrientationsListCommand IPBlocksListCommand firebase/test/ios/configuration/IosLocalesDescribeCommand.kt Getting ios device catalog for obtain locales firebase/test/ios/configuration/IosLocalesListCommand.kt Getting ios device catalog for obtain locales firebase/test/ios/models/IosModelDescribeCommand.kt Getting ios device catalog for obtain models firebase/test/ios/models/IosModelsListCommand.kt Getting ios device catalog for obtain models firebase/test/ios/orientations/IosOrientationsListCommand.kt Getting ios device catalog for obtain orientations firebase/test/ios/versions/IosVersionsDescribeCommand.kt Getting ios device catalog for obtain versions firebase/test/ios/versions/IosVersionsListCommand.kt Getting ios device catalog for obtain versions firebase/test/ios/IosTestEnvironmentCommand.kt IosModelsListCommand IosVersionsListCommand IosLocalesListCommand ProvidedSoftwareListCommand NetworkProfilesListCommand IosOrientationsListCommand IPBlocksListCommand firebase/test/android/AndroidRunCommand.kt Validation Checking if bucket exist (common args validation) Getting android device catalog for check supported devices Running android tests Preparing data Uploading files Creating android test contexts downloading apks if needed Dumping shards Uploading dumped shards if needed Uploading Uploading apks if needed & depending on test context Building and running android test matrix After run test Uploading session ID Printing matrices web links getOrUpdateWebLink Getting test matrices Pooling matrices Getting test matrices Generating report Parsing test suite Getting test matrices Creating JUnit test result Uploading each report Cost report Matrix results report Html error report JUnit report Getting test matrices for test executions Processing junit results processing full junit result Creating JUnit test result Create and upload performance metrics Upload matrices ids Fetching artifacts Printing matrices web links get or update web for each matrix Getting test matrices firebase/test/ios/IosRunCommand.kt validation Checking if bucket exist (common args validation) Getting ios device catalog for check supported devices Running ios tests Preparing data Uploading files Dump shards if needed Uploading dumped shards if needed Creating ios test contexts Creating xctest context Uploading xctest files Creating gameloop test context Uploading app file Building and running android test matrix After run test (from this point the path is same as for android) where ? - investigate if implementation is correct or is performing some not necessary operations.","title":"Google API use cases"},{"location":"refactor/investigation/#google-api-usage-function-call-tree","text":"LoginCommand -> UserAuth/request CancelCommand getLastArgs IosArgs/validateRefresh -> IosArgs/assertDevicesSupported AndroidArgs/validate AndroidArgs/assertDevicesSupported AndroidCatalog/supportedDeviceConfig -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidModelIds -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidVersionIds -> AndroidCatalog/deviceCatalog(projectId).versions AndroidCatalog/getSupportedVersionId -> AndroidCatalog/deviceCatalog(projectId).models IArgs/checkResultsDirUnique -> GcStorage/exist cancelLastRun -> cancelMatrices -> GcTestMatrix/cancel RefreshCommand -> refreshLastRun getLastArgs IosArgs/validateRefresh -> IosArgs/assertDevicesSupported AndroidArgs/validate AndroidArgs/assertDevicesSupported AndroidCatalog/supportedDeviceConfig -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidModelIds -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidVersionIds -> AndroidCatalog/deviceCatalog(projectId).versions AndroidCatalog/getSupportedVersionId -> AndroidCatalog/deviceCatalog(projectId).models IArgs/checkResultsDirUnique -> GcStorage/exist refreshMatrices GcTestMatrix/refresh SavedMatrix/updateWithMatrix -> SavedMatrix/updatedSavedMatrix -> TestMatrix/fetchTestOutcomeContext TestMatrix/getToolResultsIds GcToolResults/listAllEnvironments GcToolResults/listAllSteps ProvidedSoftwareListCommand -> providedSoftwareAsTable -> getProvidedSoftware IPBlocksListCommand -> ipBlocksListAsTable -> deviceIPBlocks NetworkProfilesDescribeCommand -> networkProfileDescription -> getNetworkConfiguration NetworkProfilesListCommand -> networkConfigurationAsTable -> getNetworkConfiguration AndroidLocalesDescribeCommand -> AndroidCatalog/getLocaleDescription -> AndroidCatalog/getLocales AndroidLocalesListCommand -> AndroidCatalog/localesAsTable -> AndroidCatalog/getLocales AndroidModelDescribeCommand -> AndroidCatalog/describeModel -> AndroidCatalog/getModels AndroidModelsListCommand -> AndroidCatalog/devicesCatalogAsTable -> AndroidCatalog/getModels AndroidOrientationsListCommand -> AndroidCatalog/supportedOrientationsAsTable -> AndroidCatalog/deviceCatalog AndroidVersionsDescribeCommand -> AndroidCatalog/describeSoftwareVersion -> AndroidCatalog/getVersionsList AndroidVersionsListCommand -> AndroidCatalog/supportedVersionsAsTable -> AndroidCatalog/getVersionsList AndroidTestEnvironmentCommand AndroidModelsListCommand AndroidVersionsListCommand AndroidLocalesListCommand ProvidedSoftwareListCommand NetworkProfilesListCommand AndroidOrientationsListCommand IPBlocksListCommand IosLocalesDescribeCommand -> IosCatalog/getLocaleDescription -> IosCatalog/getLocales IosLocalesListCommand -> IosCatalog/localesAsTable -> IosCatalog/iosDeviceCatalog IosModelDescribeCommand -> IosCatalog/describeModel -> IosCatalog/getModels IosModelsListCommand -> IosCatalog/devicesCatalogAsTable -> IosCatalog/getModels IosOrientationsListCommand -> IosCatalog/supportedOrientationsAsTable -> IosCatalog/iosDeviceCatalog IosVersionsDescribeCommand -> IosCatalog/describeSoftwareVersion -> IosCatalog/getVersionsList IosVersionsListCommand -> IosCatalog/softwareVersionsAsTable -> IosCatalog/getVersionsList IosTestEnvironmentCommand IosModelsListCommand IosVersionsListCommand IosLocalesListCommand ProvidedSoftwareListCommand NetworkProfilesListCommand IosOrientationsListCommand IPBlocksListCommand AndroidRunCommand AndroidArgs/validate AndroidArgs/assertDevicesSupported AndroidCatalog/supportedDeviceConfig AndroidCatalog/androidModelIds AndroidCatalog/androidVersionIds AndroidCatalog/getSupportedVersionId IArgs/checkResultsDirUnique -> GcStorage/exist AndroidArgs/runAndroidTests GcAndroidDevice/build GcToolResults/createToolResultsHistory IArgs/uploadOtherFiles -> GcStorage/upload AndroidArgs/uploadAdditionalApks -> List<String>/uploadToGcloudIfNeeded -> FileReference/uploadIfNeeded -> GcStorage/upload AndroidArgs/uploadObbFiles -> GcStorage/upload AndroidArgs/createAndroidTestContexts -> List<AndroidTestContext>/setupShards -> InstrumentationTestContext/downloadApks -> FileReference/downloadIfNeeded -> GcStorage/download List<AndroidTestContext>/dumpShards -> GcStorage/upload List<AndroidTestContext>/upload -> AndroidTestContext/upload InstrumentationTestContext/upload -> FileReference/uploadIfNeeded -> GcStorage/upload RoboTestContext/upload -> FileReference/uploadIfNeeded -> GcStorage/upload SanityRoboTestContext/upload -> FileReference/uploadIfNeeded -> GcStorage/upload GameLoopContext/upload -> FileReference/uploadIfNeeded -> GcStorage/upload GcAndroidTestMatrix/build AbstractGoogleJsonClientRequest<T>/executeWithRetry IArgs/afterRunTests IArgs/uploadSessionId -> GcStorage/upload MatrixMap/printMatricesWebLinks -> getOrUpdateWebLink -> GcTestMatrix/refresh pollMatrices -> matrixChangesFlow -> GcTestMatrix/refresh Iterable<TestMatrix>/updateMatrixMap -> SavedMatrix/updateWithMatrix -> TestMatrix/fetchTestOutcomeContext TestMatrix/getToolResultsIds GcToolResults/listAllEnvironments GcToolResults/listAllSteps ReportManager/generate ReportManager/parseTestSuite refreshMatricesAndGetExecutions -> refreshTestMatrices -> GcTestMatrix/refresh List<TestExecution>/createJUnitTestResult -> List<TestExecution>/createTestExecutionDataListAsync -> TestExecution/createTestExecutionData -> getAsync GcToolResults/listTestCases GcToolResults/getStepResult CostReport.run -> GcStorage/uploadReportResult MatrixResultsReport.run -> GcStorage/uploadReportResult HtmlErrorReport.run -> GcStorage/uploadReportResult JUnitReport.run -> GcStorage/uploadReportResult refreshMatricesAndGetExecutions -> refreshTestMatrices -> GcTestMatrix/refresh ReportManager/processJunitResults ReportManager/processFullJunitResult -> List<TestExecution>/createJUnitTestResult -> List<TestExecution>/createTestExecutionDataListAsync -> TestExecution/createTestExecutionData -> getAsync GcToolResults/listTestCases GcToolResults/getStepResult FullJUnitReport.run -> GcStorage.uploadReportResult ReportManager/createAndUploadPerformanceMetricsForAndroid -> List<Pair<TestExecution, String>>.getAndUploadPerformanceMetrics -> TestExecution.getPerformanceMetric -> GcToolResults.getPerformanceMetric PerfMetricsSummary.upload -> GcStorage.uploadPerformanceMetrics GcStorage/uploadMatricesId fetchArtifacts Storage.BlobListOption.fields Storage.BlobListOption.prefix GcStorage.storage.list Blob.downloadTo MatrixMap/printMatricesWebLinks -> getOrUpdateWebLink -> GcTestMatrix/refresh IosRunCommand IosArgs/validate IosArgs/assertDevicesSupported IosCatalog/supportedDevice IosCatalog/Device/getSupportedVersionId IArgs/checkResultsDirUnique -> GcStorage/exist IosArgs/runIosTests GcIosMatrix/build GcToolResults/createToolResultsHistory IArgs/uploadOtherFiles -> GcStorage/upload IosArgs.uploadAdditionalIpas IosArgs.dumpShardsIfXcTest -> GcStorage/upload IosArgs/createIosTestContexts IosArgs.createXcTestContexts IArgs.uploadIfNeeded -> FileReference.uploadIfNeeded -> GcStorage.upload GcStorage.uploadXCTestFile IosArgs.createGameloopTestContexts IArgs.uploadIfNeeded -> FileReference.uploadIfNeeded -> GcStorage.upload GcIosTestMatrix/build AbstractGoogleJsonClientRequest<T>/executeWithRetry IArgs/afterRunTests - the rest of steps are same as for android","title":"Google API usage function call tree"},{"location":"refactor/proposal/","text":"Flank refactor proposal Motivation The new opportunities like desktop UI or Corellium integration, requires more flexibility from the Flank. The flexibility in this case means replaceable presentation and external APIs, but also well organized and documented code base, easy to scale and navigate through. References investigation Architecture Layers associations presentation -> domain presentation -> utils domain -> utils domain -> data adapters -> data adapters -> external API Consider data & interfaces as boundaries Data & interfaces in this case means communication bridge between flank domain code and external APIs. So those are boundaries from the domain perspective, but from technical perspective those will be interfaces, type aliases, and structures. Package structure ftl cli domain data adapters utils CLI Wouldn't change a lot in comparison to current state. Should be thin as possible and aware only about domain layer API (top-level public functions and structures) and some simple utils. Requirements Aware about CLI library domain API - top-level public functions data adapters - for mapping results from the domain to console logs. Not aware about low-level domain functions helpers / libs Consider awareness about common utils (or have dedicated one). Domain Requirements New package for grouping domain code. Domain package directly can contain only: top-level public functions that represents use-cases(?). packages that contains low-level domain logic functions. Top-level domain functions are aware about own private functions low-level domain functions from nested domain packages. flank utils flank internal helpers are not aware about other top-level domain function CLI external API libs can produce flow of results are suspendable Top-level function Responsible to run domain logic directly or compose it using low-level domain functions, utils or API interfaces. For simplification, we can consider a simple common interface for all top-level functions. typealias UseCase < A , R > = suspend ( A ) -> Flow < R > Low-level The domain-related functions, private or internal, useful for organizing complicated domain code into the smaller chunks, and necessary where it comes to reuse something. The composition of low-level function should be flat, because more nesting in depth, can make a code much harder to understand. Data Will contain only data structures and interfaces, which will be a bridge between domain and external APIs. Requirements New package for grouping wrappers and adapters for external APIs. Consider separated package in ftl only for interfaces and structures, shared between api adapters and domain. External API structures and function cannot leak to other layers.","title":"Flank refactor proposal"},{"location":"refactor/proposal/#flank-refactor-proposal","text":"","title":"Flank refactor proposal"},{"location":"refactor/proposal/#motivation","text":"The new opportunities like desktop UI or Corellium integration, requires more flexibility from the Flank. The flexibility in this case means replaceable presentation and external APIs, but also well organized and documented code base, easy to scale and navigate through.","title":"Motivation"},{"location":"refactor/proposal/#references","text":"investigation","title":"References"},{"location":"refactor/proposal/#architecture","text":"","title":"Architecture"},{"location":"refactor/proposal/#layers-associations","text":"presentation -> domain presentation -> utils domain -> utils domain -> data adapters -> data adapters -> external API","title":"Layers associations"},{"location":"refactor/proposal/#consider-data-interfaces-as-boundaries","text":"Data & interfaces in this case means communication bridge between flank domain code and external APIs. So those are boundaries from the domain perspective, but from technical perspective those will be interfaces, type aliases, and structures.","title":"Consider data &amp; interfaces as boundaries"},{"location":"refactor/proposal/#package-structure","text":"ftl cli domain data adapters utils","title":"Package structure"},{"location":"refactor/proposal/#cli","text":"Wouldn't change a lot in comparison to current state. Should be thin as possible and aware only about domain layer API (top-level public functions and structures) and some simple utils.","title":"CLI"},{"location":"refactor/proposal/#requirements","text":"Aware about CLI library domain API - top-level public functions data adapters - for mapping results from the domain to console logs. Not aware about low-level domain functions helpers / libs Consider awareness about common utils (or have dedicated one).","title":"Requirements"},{"location":"refactor/proposal/#domain","text":"","title":"Domain"},{"location":"refactor/proposal/#requirements_1","text":"New package for grouping domain code. Domain package directly can contain only: top-level public functions that represents use-cases(?). packages that contains low-level domain logic functions. Top-level domain functions are aware about own private functions low-level domain functions from nested domain packages. flank utils flank internal helpers are not aware about other top-level domain function CLI external API libs can produce flow of results are suspendable","title":"Requirements"},{"location":"refactor/proposal/#top-level-function","text":"Responsible to run domain logic directly or compose it using low-level domain functions, utils or API interfaces. For simplification, we can consider a simple common interface for all top-level functions. typealias UseCase < A , R > = suspend ( A ) -> Flow < R >","title":"Top-level function"},{"location":"refactor/proposal/#low-level","text":"The domain-related functions, private or internal, useful for organizing complicated domain code into the smaller chunks, and necessary where it comes to reuse something. The composition of low-level function should be flat, because more nesting in depth, can make a code much harder to understand.","title":"Low-level"},{"location":"refactor/proposal/#data","text":"Will contain only data structures and interfaces, which will be a bridge between domain and external APIs.","title":"Data"},{"location":"refactor/proposal/#requirements_2","text":"New package for grouping wrappers and adapters for external APIs. Consider separated package in ftl only for interfaces and structures, shared between api adapters and domain. External API structures and function cannot leak to other layers.","title":"Requirements"},{"location":"refactor/steps/","text":"Refactor steps Steps below will be converted to a bunch of issues in epic scope. Move commands to correct package move to specified package the following commands ftl.cli.firebase.test.android AndroidLocalesCommand.kt AndroidModelsCommand.kt AndroidOrientationsCommand.kt AndroidVersionsCommand.kt ftl.cli.firebase.test.ios IosLocalesCommand.kt IosModelsCommand.kt IosOrientationsCommand.kt IosVersionsCommand.kt Extract logic from CLI Create package ftl.domain for domain layer For each CLI command that is marked ? ! Create associated domain function in domain package. Move the logic from command run method to domain function. Commands to domain mappings: LoginCommand -------------------> loginGoogleAccount AndroidLocalesDescribeCommand --> describeAndroidLocales AndroidLocalesListCommand ------> listAndroidLocales AndroidModelDescribeCommand ----> describeAndroidModels AndroidModelsListCommand -------> listAndroidModels AndroidOrientationsListCommand -> listAndroidOrientations AndroidVersionsDescribeCommand -> describeAndroidVersions AndroidVersionsListCommand -----> listAndroidVersions AndroidDoctorCommand -----------> runAndroidDoctor AndroidRunCommand --------------> runAndroidTest AndroidTestEnvironmentCommand --> describeAndroidTestEnvironment IosLocalesDescribeCommand ------> describeIosLocales IosLocalesListCommand ----------> listIosLocales IosModelDescribeCommand --------> describeIosModels IosModelsListCommand -----------> listIosModels IosOrientationsListCommand -----> listIosOrientations IosVersionsDescribeCommand -----> describeIosVersions IosVersionsListCommand ---------> listIosVersions IosDoctorCommand ---------------> runIosDoctor IosRunCommand ------------------> runIosTest IosTestEnvironmentCommand ------> describeIosTestEnvironment IPBlocksListCommand ------------> listIPBlocks NetworkProfilesDescribeCommand -> describeNetworkProfiles NetworkProfilesListCommand -----> listNetworkProfiles ProvidedSoftwareListCommand ----> listProvidedSoftware CancelCommand ------------------> cancelLastRun RefreshCommand -----------------> refreshLastRun Files in domain package ftl.domain \u251c\u2500\u2500 LoginGoogleAccount.kt \u251c\u2500\u2500 DescribeAndroidLocales.kt \u251c\u2500\u2500 ListAndroidLocales.kt \u251c\u2500\u2500 DescribeAndroidModels.kt \u251c\u2500\u2500 ListAndroidModels.kt \u251c\u2500\u2500 ListAndroidOrientations.kt \u251c\u2500\u2500 DescribeAndroidVersions.kt \u251c\u2500\u2500 ListAndroidVersions.kt \u251c\u2500\u2500 RunAndroidDoctor.kt \u251c\u2500\u2500 RunAndroidTest.kt \u251c\u2500\u2500 DescribeAndroidTestEnvironment.kt \u251c\u2500\u2500 DescribeIosLocales.kt \u251c\u2500\u2500 ListIosLocales.kt \u251c\u2500\u2500 DescribeIosModels.kt \u251c\u2500\u2500 ListIosModels.kt \u251c\u2500\u2500 ListIosOrientations.kt \u251c\u2500\u2500 DescribeIosVersions.kt \u251c\u2500\u2500 ListIosVersions.kt \u251c\u2500\u2500 RunIosDoctor.kt \u251c\u2500\u2500 RunIosTest.kt \u251c\u2500\u2500 DescribeIosTestEnvironment.kt \u251c\u2500\u2500 ListIPBlocks.kt \u251c\u2500\u2500 DescribeNetworkProfiles.kt \u251c\u2500\u2500 ListNetworkProfiles.kt \u251c\u2500\u2500 ListProvidedSoftware.kt \u251c\u2500\u2500 CancelLastRun.kt \u2514\u2500\u2500 RefreshLastRun.kt Use FileReference abstraction where possible For convenience, all files that needs to be synchronized with the bucket, should treat as FileReference instead of String . CommonArgs otherFiles AndroidArgs appApk testApk additionalApks roboScript obbFiles additionalAppTestApks Add packages for data layer Add ftl.interface Add ftl.adapter Create abstraction layer for external API Important note! Printing output to console is a part of presentation layer. Authorization Target ftl/gc/UserAuth.kt Interface ftl/data/AuthorizeUser.kt package ftl.data object UserAuthorization { interface Request : () -> UserAuthorization } Provided software Target ftl/environment/ProvidedSoftwareCatalog.kt Interface ftl/data/SoftwareCatalog.kt package ftl.data data class SoftwareCatalog ( val orchestratorVersion : String ) { interface Fetch : () -> SoftwareCatalog } Presentation val softwareCatalogTable : suspend SoftwareCatalog .() -> String = TODO () IP Blocks List Target ftl/environment/ListIPBlocks.kt Interface ftl/data/IpBlocks.kt package ftl.data data class IpBlock ( val block : String , val form : String , val addedDate : String ) { interface Fetch : () -> List < IpBlock > } Presentation val ipBlocksTable : suspend List < IpBlock > .() -> String = TODO () Network profiles Target ftl/environment/NetworkProfileDescription.kt Interface ftl/data/NetworkProfile.kt package ftl.data data class NetworkProfile ( val id : String , val downRule : Rule , val upRule : Rule ) { data class Rule ( val bandwidth : String , val delay : String , val packetLossRatio : Float , val packetDuplicationRatio : Float , val burst : Float ) interface Fetch : () -> List < NetworkProfile > } Presentation val networkProfileDescription : suspend NetworkProfile .() -> String = TODO () val networkProfileList : suspend List < NetworkProfile > .() -> String = TODO () Locales Target AndroidLocalesDescribeCommand -> AndroidCatalog/getLocaleDescription -> AndroidCatalog/getLocales AndroidLocalesListCommand -> AndroidCatalog/localesAsTable -> AndroidCatalog/getLocales IosLocalesDescribeCommand -> IosCatalog/getLocaleDescription -> IosCatalog/getLocales IosLocalesListCommand -> IosCatalog/localesAsTable -> IosCatalog/iosDeviceCatalog Interface ftl/data/Locales.kt package ftl.data data class Locale ( val id : String , val name : String , val region : String , val tags : List < String > , ) { data class Identity ( val projectId : String , val platform : String , ) interface Fetch : ( Identity ) -> List < Locale > } Presentation val localeDescription : suspend Locale .() -> String = TODO () val localeTable : suspend List < Locale > .() -> String = TODO () Device models Target AndroidModelDescribeCommand -> AndroidCatalog/describeModel -> AndroidCatalog/getModels AndroidModelsListCommand -> AndroidCatalog/devicesCatalogAsTable -> AndroidCatalog/getModels IosModelDescribeCommand -> IosCatalog/describeModel -> IosCatalog/getModels IosModelsListCommand -> IosCatalog/devicesCatalogAsTable -> IosCatalog/getModels AndroidArgs/validate -> AndroidArgs/assertDevicesSupported AndroidCatalog/supportedDeviceConfig -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidModelIds -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/getSupportedVersionId -> AndroidCatalog/deviceCatalog(projectId).models IosArgs/validateRefresh -> IosArgs/assertDevicesSupported -> IosCatalog/iosDeviceCatalog(projectId).models Interface ftl/data/DeviceModels.kt package ftl.data object DeviceModel { data class Android ( val id : String , val name : String , val tags : List < String > , val screenX : Int , val screenY : Int , val formFactor : String , val screenDensity : Int , val supportedVersionIds : List < String > , val form : String , val brand : String , val codename : String , val manufacturer : String , val thumbnailUrl : String , val supportedAbis : List < String > , val lowFpsVideoRecording : Boolean , ) { interface Fetch : ( projectId : String ) -> List < Android > } data class Ios ( val id : String , val name : String , val tags : List < String > , val screenX : Int , val screenY : Int , val formFactor : String , val screenDensity : Int , val supportedVersionIds : List < String > , val deviceCapabilities : List < String > , ) { interface Fetch : ( projectId : String ) -> List < Ios > } } Presentation val androidModelDescription : suspend DeviceModel . Android .() -> String = TODO () val androidModelsTable : suspend List < DeviceModel . Android > .() -> String = TODO () val iosModelDescription : suspend DeviceModel . Ios .() -> String = TODO () val iosModelsTable : suspend List < DeviceModel . Ios > .() -> String = TODO () Orientation Target AndroidOrientationsListCommand -> AndroidCatalog/supportedOrientationsAsTable -> AndroidCatalog/deviceCatalog IosOrientationsListCommand -> IosCatalog/supportedOrientationsAsTable -> IosCatalog/iosDeviceCatalog Interface ftl/data/Orientation.kt package ftl.data data class Orientation ( val id : String , val name : String , val tags : String , ) { interface Fetch : ( projectId : String , platform : String ) -> List < Orientation > } Presentation val orientationsTable : suspend List < Orientation > .() -> String = TODO () OS Version Target AndroidVersionsListCommand -> AndroidCatalog/supportedVersionsAsTable -> AndroidCatalog/getVersionsList IosVersionsListCommand -> IosCatalog/softwareVersionsAsTable -> IosCatalog/getVersionsList AndroidArgs/validate -> AndroidArgs/assertDevicesSupported -> AndroidCatalog/androidVersionIds -> AndroidCatalog/deviceCatalog(projectId).versions Interface ftl/data/OsVersion.kt package ftl.data object OsVersion { data class Android ( val apiLevel : Int , val codeName : String , val distribution : Distribution , val id : String , val releaseDate : Date , val tags : List < String > , val versionString : String , ) { interface Fetch : ( projectId : String ) -> List < Android > } data class Ios ( val id : String , val majorVersion : Int , val minorVersion : Int , val supportedXcodeVersionIds : List < String > , val tags : List < String > , ) { interface Fetch : ( projectId : String ) -> List < Ios > } } Presentation val androidOsVersionDescription : suspend OsVersion . Android .() -> String = TODO () val androidOsVersionsTable : suspend List < OsVersion . Android > .() -> String = TODO () val iosOsVersionDescription : suspend OsVersion . Ios .() -> String = TODO () val iosOsVersionsTable : suspend List < OsVersion . Ios > .() -> String = TODO () File reference Target FileReference Interface ftl/data/FileReference.kt data class FileReference ( val local : String = \"\" , val remote : String = \"\" ) { interface Exist : ( FileReference ) -> Boolean interface Upload : ( FileReference ) -> FileReference interface Download : ( FileReference ) -> FileReference } Remote storage aka GcStorage Target IArgs/checkResultsDirUnique Interface ftl/data/RemoteStorage.kt package ftl.data object RemoteStorage { data class Dir ( val bucket : String , val path : String ) class Data ( val path : String , val bytes : ByteArray? = null // Use, when file under the given path doesn't exist. ) interface Exist : ( Dir ) -> Boolean interface Upload : ( Dir , Data ) -> Unit } Test matrix results, cancel & refresh Target SavedMatrix TestOutcome TestSuiteOverviewData MatrixMap CancelCommand -> cancelLastRun -> cancelMatrices -> GcTestMatrix/cancel RefreshCommand -> refreshLastRun refreshMatrices GcTestMatrix/refresh SavedMatrix/updateWithMatrix -> SavedMatrix/updatedSavedMatrix -> TestMatrix/fetchTestOutcomeContext newTestRun pollMatrices -> matrixChangesFlow -> GcTestMatrix/refresh Iterable<TestMatrix>/updateMatrixMap -> SavedMatrix/updateWithMatrix -> TestMatrix/fetchTestOutcomeContext ReportManager/generate ReportManager/parseTestSuite refreshMatricesAndGetExecutions -> refreshTestMatrices -> GcTestMatrix/refresh Interface ftl/data/TestMatrix.kt object TestMatrix { data class Result ( val runPath : String , val map : Map < String , Data > , ) data class Data ( val matrixId : String = \"\" , val state : String = \"\" , val gcsPath : String = \"\" , val webLink : String = \"\" , val downloaded : Boolean = false , val billableMinutes : BillableMinutes = BillableMinutes (), val clientDetails : Map < String , String >? = null , val gcsPathWithoutRootBucket : String = \"\" , val gcsRootBucket : String = \"\" , val axes : List < Outcome > = emptyList () ) data class Outcome ( val device : String = \"\" , val outcome : String = \"\" , val details : String = \"\" , val suiteOverview : SuiteOverview = SuiteOverview () ) data class SuiteOverview ( val total : Int = 0 , val errors : Int = 0 , val failures : Int = 0 , val flakes : Int = 0 , val skipped : Int = 0 , val elapsedTime : Double = 0.0 , val overheadTime : Double = 0.0 ) data class BillableMinutes ( val virtual : Long = 0 , val physical : Long = 0 ) data class Summary ( val billableMinutes : BillableMinutes , val axes : List < Outcome > , ) { data class Identity ( val projectId : String , val historyId : String , val executionId : String , ) interface Fetch : ( Identity ) -> Summary } data class Identity ( val matrixId : String , val projectId : String , ) interface Cancel : ( Identity ) -> Unit interface Refresh : ( Identity ) -> Data } Presentation TODO () Execute Android tests matrix Target AndroidRunCommand -> AndroidArgs/runAndroidTests -> GcAndroidTestMatrix/build AndroidTestConfig Interface ftl/data/AndroidTestMatrix.kt package ftl.data object AndroidTestMatrix { data class Config ( // args val clientDetails : Map < String , String >? , val resultsBucket : String , val autoGoogleLogin : Boolean , val networkProfile : String? , val directoriesToPull : List < String > , val obbNames : List < String > , val environmentVariables : Map < String , String > , val autograntPermissions : Boolean , val testTimeout : String , val performanceMetrics : Boolean , val recordVideo : Boolean , val flakyTestAttempts : Int , val failFast : Boolean , val project : String , val resultsHistoryName : String? , // build val otherFiles : Map < String , String > , val runGcsPath : String , val devices : List < Device > , val additionalApkGcsPaths : List < String > , val obbFiles : Map < String , String > , ) sealed class Type { data class Instrumentation ( val appApkGcsPath : String , val testApkGcsPath : String , val testRunnerClass : String? , val orchestratorOption : String? , // sharding val disableSharding : Boolean , val testShards : ShardChunks , val numUniformShards : Int? , val keepTestTargetsEmpty : Boolean , val environmentVariables : Map < String , String > = emptyMap (), val testTargetsForShard : ShardChunks ) : Type () data class Robo ( val appApkGcsPath : String , val flankRoboDirectives : List < FlankRoboDirective >? , val roboScriptGcsPath : String? ) : Type () data class GameLoop ( val appApkGcsPath : String , val testRunnerClass : String? , val scenarioNumbers : List < String > , val scenarioLabels : List < String > ) : Type () } interface Execute : ( Config , Type ) -> TestMatrix . Result } Presentation TODO () Execute Ios tests matrix Target IosRunCommand -> IosArgs/runIosTests -> GcIosTestMatrix/build IosTestContext Interface ftl/data/IosTestMatrix.kt package ftl.data object IosTestMatrix { data class Config ( // args val clientDetails : Map < String , String >? , val networkProfile : String? , val directoriesToPull : List < String > , val testTimeout : String , val recordVideo : Boolean , val flakyTestAttempts : Int , val failFast : Boolean , val project : String , val resultsHistoryName : String? , // build val devices : List < Device > , val otherFiles : Map < String , String > , val additionalIpasGcsPaths : List < String > , ) sealed class Type { data class XcTest ( val xcTestGcsPath : String , val xcTestRunFileGcsPath : String , val xcodeVersion : String , val testSpecialEntitlements : Boolean , val matrixGcsPath : String , ) data class GameLoop ( val appGcsPath : String , val scenarios : List < Int > , val matrixGcsPath : String , ) } interface Execute : ( Config , Type ) -> TestMatrix . Result } Presentation TODO () JUnit results Target ReportManager/generate -> ReportManager/parseTestSuite ReportManager/processXmlFromFile refreshMatricesAndGetExecutions -> List<TestExecution>/createJUnitTestResult Interface ftl/data/JUnitTestResult.kt package ftl.data object JUnitTest { @JacksonXmlRootElement ( localName = \"testsuites\" ) data class Result ( @JsonInclude ( JsonInclude . Include . NON_EMPTY ) @JacksonXmlProperty ( localName = \"testsuite\" ) var testsuites : MutableList < Suite >? = null ) { data class ApiIdentity ( val projectId : String , val matrixIds : List < String > ) interface GenerateFromApi : ( ApiIdentity ) -> Result interface ParseFromFiles : ( File ) -> Result } data class Suite ( @JacksonXmlProperty ( isAttribute = true ) var name : String , @JacksonXmlProperty ( isAttribute = true ) var tests : String , // Int @JacksonXmlProperty ( isAttribute = true ) var failures : String , // Int @JacksonXmlProperty ( isAttribute = true ) var flakes : Int? = null , @JacksonXmlProperty ( isAttribute = true ) var errors : String , // Int @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( isAttribute = true ) var skipped : String? , // Int. Android only @JacksonXmlProperty ( isAttribute = true ) var time : String , // Double @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( isAttribute = true ) val timestamp : String? , // String. Android only @JacksonXmlProperty ( isAttribute = true ) val hostname : String? = \"localhost\" , // String. @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( isAttribute = true ) val testLabExecutionId : String? = null , // String. @JacksonXmlProperty ( localName = \"testcase\" ) var testcases : MutableCollection < Case >? , // not used @JsonInclude ( JsonInclude . Include . NON_NULL ) val properties : Any? = null , // <properties /> @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( localName = \"system-out\" ) val systemOut : Any? = null , // <system-out /> @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( localName = \"system-err\" ) val systemErr : Any? = null // <system-err /> ) data class Case ( // name, classname, and time are always present except for empty test cases <testcase/> @JacksonXmlProperty ( isAttribute = true ) val name : String? , @JacksonXmlProperty ( isAttribute = true ) val classname : String? , @JacksonXmlProperty ( isAttribute = true ) val time : String? , // iOS contains multiple failures for a single test. // JUnit XML allows arbitrary amounts of failure/error tags @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( localName = \"failure\" ) val failures : List < String >? = null , @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( localName = \"error\" ) val errors : List < String >? = null , @JsonInclude ( JsonInclude . Include . CUSTOM , valueFilter = FilterNotNull :: class ) val skipped : String? = \"absent\" // used by FilterNotNull to filter out absent `skipped` values ) { // Consider to move all properties to constructor if will doesn't conflict with parser @JsonInclude ( JsonInclude . Include . NON_NULL ) var webLink : String? = null @JacksonXmlProperty ( isAttribute = true ) var flaky : Boolean? = null // use null instead of false } @Suppress ( \"UnusedPrivateClass\" ) private class FilterNotNull { override fun equals ( other : Any? ): Boolean { // other is null = present // other is not null = absent (default value) return other != null } override fun hashCode (): Int { return javaClass . hashCode () } } } Performance Metrics Target GcToolResults/getPerformanceMetric Interface ftl/data/PerfMetrics.kt package ftl.data object PerfMetrics { // based on com.google.api.services.toolresults.model.PerfMetricsSummary data class Summary ( val appStartTime : AppStartTime? = null , val graphicsStats : GraphicsStats? = null , val perfEnvironment : PerfEnvironment? = null , val perfMetrics : List < String >? = null , val executionId : String? = null , val historyId : String? = null , val projectId : String? = null , val stepId : String? = null , ) data class GraphicsStats ( val buckets : List < Bucket >? = null , val highInputLatencyCount : Long? = null , val jankyFrames : Long? = null , val missedVsyncCount : Long? = null , val p50Millis : Long? = null , val p90Millis : Long? = null , val p95Millis : Long? = null , val p99Millis : Long? = null , val slowBitmapUploadCount : Long? = null , val slowDrawCount : Long? = null , val slowUiThreadCount : Long? = null , val totalFrames : Long? = null , ) { data class Bucket ( val frameCount : Long? , val renderMillis : Long? , ) } data class AppStartTime ( val fullyDrawnTime : Duration? = null , val initialDisplayTime : Duration? = null , ) data class PerfEnvironment ( val cpuInfo : CPUInfo? = null , val memoryInfo : MemoryInfo? = null , ) data class CPUInfo ( val cpuProcessor : String? = null , val cpuSpeedInGhz : Float? = null , val numberOfCores : Int? = null , ) data class MemoryInfo ( val memoryCapInKibibyte : Long? = null , val memoryTotalInKibibyte : Long? = null , ) data class Identity ( val executionId : String , val historyId : String , val projectId : String , val stepId : String , ) interface Fetch : ( Identity ) -> Summary } Fetching Artifacts Target FetchArtifacts.kt Interface ftl/data/PerfMetrics.kt package ftl.data object Artifacts { data class Identity ( val gcsPathWithoutRootBucket : String , val gcsRootBucket : String , val regex : List < Regex > , val blobPath : String , val downloadPath : DownloadPath , ) data class DownloadPath ( val localResultDir : String , val useLocalResultDir : Boolean , val keepFilePath : Boolean , ) interface Fetch : ( Identity ) -> List < String > }","title":"Refactor steps"},{"location":"refactor/steps/#refactor-steps","text":"Steps below will be converted to a bunch of issues in epic scope.","title":"Refactor steps"},{"location":"refactor/steps/#move-commands-to-correct-package","text":"move to specified package the following commands ftl.cli.firebase.test.android AndroidLocalesCommand.kt AndroidModelsCommand.kt AndroidOrientationsCommand.kt AndroidVersionsCommand.kt ftl.cli.firebase.test.ios IosLocalesCommand.kt IosModelsCommand.kt IosOrientationsCommand.kt IosVersionsCommand.kt","title":"Move commands to correct package"},{"location":"refactor/steps/#extract-logic-from-cli","text":"Create package ftl.domain for domain layer For each CLI command that is marked ? ! Create associated domain function in domain package. Move the logic from command run method to domain function.","title":"Extract logic from CLI"},{"location":"refactor/steps/#commands-to-domain-mappings","text":"LoginCommand -------------------> loginGoogleAccount AndroidLocalesDescribeCommand --> describeAndroidLocales AndroidLocalesListCommand ------> listAndroidLocales AndroidModelDescribeCommand ----> describeAndroidModels AndroidModelsListCommand -------> listAndroidModels AndroidOrientationsListCommand -> listAndroidOrientations AndroidVersionsDescribeCommand -> describeAndroidVersions AndroidVersionsListCommand -----> listAndroidVersions AndroidDoctorCommand -----------> runAndroidDoctor AndroidRunCommand --------------> runAndroidTest AndroidTestEnvironmentCommand --> describeAndroidTestEnvironment IosLocalesDescribeCommand ------> describeIosLocales IosLocalesListCommand ----------> listIosLocales IosModelDescribeCommand --------> describeIosModels IosModelsListCommand -----------> listIosModels IosOrientationsListCommand -----> listIosOrientations IosVersionsDescribeCommand -----> describeIosVersions IosVersionsListCommand ---------> listIosVersions IosDoctorCommand ---------------> runIosDoctor IosRunCommand ------------------> runIosTest IosTestEnvironmentCommand ------> describeIosTestEnvironment IPBlocksListCommand ------------> listIPBlocks NetworkProfilesDescribeCommand -> describeNetworkProfiles NetworkProfilesListCommand -----> listNetworkProfiles ProvidedSoftwareListCommand ----> listProvidedSoftware CancelCommand ------------------> cancelLastRun RefreshCommand -----------------> refreshLastRun","title":"Commands to domain mappings:"},{"location":"refactor/steps/#files-in-domain-package","text":"ftl.domain \u251c\u2500\u2500 LoginGoogleAccount.kt \u251c\u2500\u2500 DescribeAndroidLocales.kt \u251c\u2500\u2500 ListAndroidLocales.kt \u251c\u2500\u2500 DescribeAndroidModels.kt \u251c\u2500\u2500 ListAndroidModels.kt \u251c\u2500\u2500 ListAndroidOrientations.kt \u251c\u2500\u2500 DescribeAndroidVersions.kt \u251c\u2500\u2500 ListAndroidVersions.kt \u251c\u2500\u2500 RunAndroidDoctor.kt \u251c\u2500\u2500 RunAndroidTest.kt \u251c\u2500\u2500 DescribeAndroidTestEnvironment.kt \u251c\u2500\u2500 DescribeIosLocales.kt \u251c\u2500\u2500 ListIosLocales.kt \u251c\u2500\u2500 DescribeIosModels.kt \u251c\u2500\u2500 ListIosModels.kt \u251c\u2500\u2500 ListIosOrientations.kt \u251c\u2500\u2500 DescribeIosVersions.kt \u251c\u2500\u2500 ListIosVersions.kt \u251c\u2500\u2500 RunIosDoctor.kt \u251c\u2500\u2500 RunIosTest.kt \u251c\u2500\u2500 DescribeIosTestEnvironment.kt \u251c\u2500\u2500 ListIPBlocks.kt \u251c\u2500\u2500 DescribeNetworkProfiles.kt \u251c\u2500\u2500 ListNetworkProfiles.kt \u251c\u2500\u2500 ListProvidedSoftware.kt \u251c\u2500\u2500 CancelLastRun.kt \u2514\u2500\u2500 RefreshLastRun.kt","title":"Files in domain package"},{"location":"refactor/steps/#use-filereference-abstraction-where-possible","text":"For convenience, all files that needs to be synchronized with the bucket, should treat as FileReference instead of String .","title":"Use FileReference abstraction where possible"},{"location":"refactor/steps/#commonargs","text":"otherFiles","title":"CommonArgs"},{"location":"refactor/steps/#androidargs","text":"appApk testApk additionalApks roboScript obbFiles additionalAppTestApks","title":"AndroidArgs"},{"location":"refactor/steps/#add-packages-for-data-layer","text":"Add ftl.interface Add ftl.adapter","title":"Add packages for data layer"},{"location":"refactor/steps/#create-abstraction-layer-for-external-api","text":"","title":"Create abstraction layer for external API"},{"location":"refactor/steps/#important-note","text":"Printing output to console is a part of presentation layer.","title":"Important note!"},{"location":"refactor/steps/#authorization","text":"","title":"Authorization"},{"location":"refactor/steps/#target","text":"ftl/gc/UserAuth.kt","title":"Target"},{"location":"refactor/steps/#interface","text":"ftl/data/AuthorizeUser.kt package ftl.data object UserAuthorization { interface Request : () -> UserAuthorization }","title":"Interface"},{"location":"refactor/steps/#provided-software","text":"","title":"Provided software"},{"location":"refactor/steps/#target_1","text":"ftl/environment/ProvidedSoftwareCatalog.kt","title":"Target"},{"location":"refactor/steps/#interface_1","text":"ftl/data/SoftwareCatalog.kt package ftl.data data class SoftwareCatalog ( val orchestratorVersion : String ) { interface Fetch : () -> SoftwareCatalog }","title":"Interface"},{"location":"refactor/steps/#presentation","text":"val softwareCatalogTable : suspend SoftwareCatalog .() -> String = TODO ()","title":"Presentation"},{"location":"refactor/steps/#ip-blocks-list","text":"","title":"IP Blocks List"},{"location":"refactor/steps/#target_2","text":"ftl/environment/ListIPBlocks.kt","title":"Target"},{"location":"refactor/steps/#interface_2","text":"ftl/data/IpBlocks.kt package ftl.data data class IpBlock ( val block : String , val form : String , val addedDate : String ) { interface Fetch : () -> List < IpBlock > }","title":"Interface"},{"location":"refactor/steps/#presentation_1","text":"val ipBlocksTable : suspend List < IpBlock > .() -> String = TODO ()","title":"Presentation"},{"location":"refactor/steps/#network-profiles","text":"","title":"Network profiles"},{"location":"refactor/steps/#target_3","text":"ftl/environment/NetworkProfileDescription.kt","title":"Target"},{"location":"refactor/steps/#interface_3","text":"ftl/data/NetworkProfile.kt package ftl.data data class NetworkProfile ( val id : String , val downRule : Rule , val upRule : Rule ) { data class Rule ( val bandwidth : String , val delay : String , val packetLossRatio : Float , val packetDuplicationRatio : Float , val burst : Float ) interface Fetch : () -> List < NetworkProfile > }","title":"Interface"},{"location":"refactor/steps/#presentation_2","text":"val networkProfileDescription : suspend NetworkProfile .() -> String = TODO () val networkProfileList : suspend List < NetworkProfile > .() -> String = TODO ()","title":"Presentation"},{"location":"refactor/steps/#locales","text":"","title":"Locales"},{"location":"refactor/steps/#target_4","text":"AndroidLocalesDescribeCommand -> AndroidCatalog/getLocaleDescription -> AndroidCatalog/getLocales AndroidLocalesListCommand -> AndroidCatalog/localesAsTable -> AndroidCatalog/getLocales IosLocalesDescribeCommand -> IosCatalog/getLocaleDescription -> IosCatalog/getLocales IosLocalesListCommand -> IosCatalog/localesAsTable -> IosCatalog/iosDeviceCatalog","title":"Target"},{"location":"refactor/steps/#interface_4","text":"ftl/data/Locales.kt package ftl.data data class Locale ( val id : String , val name : String , val region : String , val tags : List < String > , ) { data class Identity ( val projectId : String , val platform : String , ) interface Fetch : ( Identity ) -> List < Locale > }","title":"Interface"},{"location":"refactor/steps/#presentation_3","text":"val localeDescription : suspend Locale .() -> String = TODO () val localeTable : suspend List < Locale > .() -> String = TODO ()","title":"Presentation"},{"location":"refactor/steps/#device-models","text":"","title":"Device models"},{"location":"refactor/steps/#target_5","text":"AndroidModelDescribeCommand -> AndroidCatalog/describeModel -> AndroidCatalog/getModels AndroidModelsListCommand -> AndroidCatalog/devicesCatalogAsTable -> AndroidCatalog/getModels IosModelDescribeCommand -> IosCatalog/describeModel -> IosCatalog/getModels IosModelsListCommand -> IosCatalog/devicesCatalogAsTable -> IosCatalog/getModels AndroidArgs/validate -> AndroidArgs/assertDevicesSupported AndroidCatalog/supportedDeviceConfig -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/androidModelIds -> AndroidCatalog/deviceCatalog(projectId).models AndroidCatalog/getSupportedVersionId -> AndroidCatalog/deviceCatalog(projectId).models IosArgs/validateRefresh -> IosArgs/assertDevicesSupported -> IosCatalog/iosDeviceCatalog(projectId).models","title":"Target"},{"location":"refactor/steps/#interface_5","text":"ftl/data/DeviceModels.kt package ftl.data object DeviceModel { data class Android ( val id : String , val name : String , val tags : List < String > , val screenX : Int , val screenY : Int , val formFactor : String , val screenDensity : Int , val supportedVersionIds : List < String > , val form : String , val brand : String , val codename : String , val manufacturer : String , val thumbnailUrl : String , val supportedAbis : List < String > , val lowFpsVideoRecording : Boolean , ) { interface Fetch : ( projectId : String ) -> List < Android > } data class Ios ( val id : String , val name : String , val tags : List < String > , val screenX : Int , val screenY : Int , val formFactor : String , val screenDensity : Int , val supportedVersionIds : List < String > , val deviceCapabilities : List < String > , ) { interface Fetch : ( projectId : String ) -> List < Ios > } }","title":"Interface"},{"location":"refactor/steps/#presentation_4","text":"val androidModelDescription : suspend DeviceModel . Android .() -> String = TODO () val androidModelsTable : suspend List < DeviceModel . Android > .() -> String = TODO () val iosModelDescription : suspend DeviceModel . Ios .() -> String = TODO () val iosModelsTable : suspend List < DeviceModel . Ios > .() -> String = TODO ()","title":"Presentation"},{"location":"refactor/steps/#orientation","text":"","title":"Orientation"},{"location":"refactor/steps/#target_6","text":"AndroidOrientationsListCommand -> AndroidCatalog/supportedOrientationsAsTable -> AndroidCatalog/deviceCatalog IosOrientationsListCommand -> IosCatalog/supportedOrientationsAsTable -> IosCatalog/iosDeviceCatalog","title":"Target"},{"location":"refactor/steps/#interface_6","text":"ftl/data/Orientation.kt package ftl.data data class Orientation ( val id : String , val name : String , val tags : String , ) { interface Fetch : ( projectId : String , platform : String ) -> List < Orientation > }","title":"Interface"},{"location":"refactor/steps/#presentation_5","text":"val orientationsTable : suspend List < Orientation > .() -> String = TODO ()","title":"Presentation"},{"location":"refactor/steps/#os-version","text":"","title":"OS Version"},{"location":"refactor/steps/#target_7","text":"AndroidVersionsListCommand -> AndroidCatalog/supportedVersionsAsTable -> AndroidCatalog/getVersionsList IosVersionsListCommand -> IosCatalog/softwareVersionsAsTable -> IosCatalog/getVersionsList AndroidArgs/validate -> AndroidArgs/assertDevicesSupported -> AndroidCatalog/androidVersionIds -> AndroidCatalog/deviceCatalog(projectId).versions","title":"Target"},{"location":"refactor/steps/#interface_7","text":"ftl/data/OsVersion.kt package ftl.data object OsVersion { data class Android ( val apiLevel : Int , val codeName : String , val distribution : Distribution , val id : String , val releaseDate : Date , val tags : List < String > , val versionString : String , ) { interface Fetch : ( projectId : String ) -> List < Android > } data class Ios ( val id : String , val majorVersion : Int , val minorVersion : Int , val supportedXcodeVersionIds : List < String > , val tags : List < String > , ) { interface Fetch : ( projectId : String ) -> List < Ios > } }","title":"Interface"},{"location":"refactor/steps/#presentation_6","text":"val androidOsVersionDescription : suspend OsVersion . Android .() -> String = TODO () val androidOsVersionsTable : suspend List < OsVersion . Android > .() -> String = TODO () val iosOsVersionDescription : suspend OsVersion . Ios .() -> String = TODO () val iosOsVersionsTable : suspend List < OsVersion . Ios > .() -> String = TODO ()","title":"Presentation"},{"location":"refactor/steps/#file-reference","text":"","title":"File reference"},{"location":"refactor/steps/#target_8","text":"FileReference","title":"Target"},{"location":"refactor/steps/#interface_8","text":"ftl/data/FileReference.kt data class FileReference ( val local : String = \"\" , val remote : String = \"\" ) { interface Exist : ( FileReference ) -> Boolean interface Upload : ( FileReference ) -> FileReference interface Download : ( FileReference ) -> FileReference }","title":"Interface"},{"location":"refactor/steps/#remote-storage-aka-gcstorage","text":"","title":"Remote storage aka GcStorage"},{"location":"refactor/steps/#target_9","text":"IArgs/checkResultsDirUnique","title":"Target"},{"location":"refactor/steps/#interface_9","text":"ftl/data/RemoteStorage.kt package ftl.data object RemoteStorage { data class Dir ( val bucket : String , val path : String ) class Data ( val path : String , val bytes : ByteArray? = null // Use, when file under the given path doesn't exist. ) interface Exist : ( Dir ) -> Boolean interface Upload : ( Dir , Data ) -> Unit }","title":"Interface"},{"location":"refactor/steps/#test-matrix-results-cancel-refresh","text":"","title":"Test matrix results, cancel &amp; refresh"},{"location":"refactor/steps/#target_10","text":"SavedMatrix TestOutcome TestSuiteOverviewData MatrixMap CancelCommand -> cancelLastRun -> cancelMatrices -> GcTestMatrix/cancel RefreshCommand -> refreshLastRun refreshMatrices GcTestMatrix/refresh SavedMatrix/updateWithMatrix -> SavedMatrix/updatedSavedMatrix -> TestMatrix/fetchTestOutcomeContext newTestRun pollMatrices -> matrixChangesFlow -> GcTestMatrix/refresh Iterable<TestMatrix>/updateMatrixMap -> SavedMatrix/updateWithMatrix -> TestMatrix/fetchTestOutcomeContext ReportManager/generate ReportManager/parseTestSuite refreshMatricesAndGetExecutions -> refreshTestMatrices -> GcTestMatrix/refresh","title":"Target"},{"location":"refactor/steps/#interface_10","text":"ftl/data/TestMatrix.kt object TestMatrix { data class Result ( val runPath : String , val map : Map < String , Data > , ) data class Data ( val matrixId : String = \"\" , val state : String = \"\" , val gcsPath : String = \"\" , val webLink : String = \"\" , val downloaded : Boolean = false , val billableMinutes : BillableMinutes = BillableMinutes (), val clientDetails : Map < String , String >? = null , val gcsPathWithoutRootBucket : String = \"\" , val gcsRootBucket : String = \"\" , val axes : List < Outcome > = emptyList () ) data class Outcome ( val device : String = \"\" , val outcome : String = \"\" , val details : String = \"\" , val suiteOverview : SuiteOverview = SuiteOverview () ) data class SuiteOverview ( val total : Int = 0 , val errors : Int = 0 , val failures : Int = 0 , val flakes : Int = 0 , val skipped : Int = 0 , val elapsedTime : Double = 0.0 , val overheadTime : Double = 0.0 ) data class BillableMinutes ( val virtual : Long = 0 , val physical : Long = 0 ) data class Summary ( val billableMinutes : BillableMinutes , val axes : List < Outcome > , ) { data class Identity ( val projectId : String , val historyId : String , val executionId : String , ) interface Fetch : ( Identity ) -> Summary } data class Identity ( val matrixId : String , val projectId : String , ) interface Cancel : ( Identity ) -> Unit interface Refresh : ( Identity ) -> Data }","title":"Interface"},{"location":"refactor/steps/#presentation_7","text":"TODO ()","title":"Presentation"},{"location":"refactor/steps/#execute-android-tests-matrix","text":"","title":"Execute Android tests matrix"},{"location":"refactor/steps/#target_11","text":"AndroidRunCommand -> AndroidArgs/runAndroidTests -> GcAndroidTestMatrix/build AndroidTestConfig","title":"Target"},{"location":"refactor/steps/#interface_11","text":"ftl/data/AndroidTestMatrix.kt package ftl.data object AndroidTestMatrix { data class Config ( // args val clientDetails : Map < String , String >? , val resultsBucket : String , val autoGoogleLogin : Boolean , val networkProfile : String? , val directoriesToPull : List < String > , val obbNames : List < String > , val environmentVariables : Map < String , String > , val autograntPermissions : Boolean , val testTimeout : String , val performanceMetrics : Boolean , val recordVideo : Boolean , val flakyTestAttempts : Int , val failFast : Boolean , val project : String , val resultsHistoryName : String? , // build val otherFiles : Map < String , String > , val runGcsPath : String , val devices : List < Device > , val additionalApkGcsPaths : List < String > , val obbFiles : Map < String , String > , ) sealed class Type { data class Instrumentation ( val appApkGcsPath : String , val testApkGcsPath : String , val testRunnerClass : String? , val orchestratorOption : String? , // sharding val disableSharding : Boolean , val testShards : ShardChunks , val numUniformShards : Int? , val keepTestTargetsEmpty : Boolean , val environmentVariables : Map < String , String > = emptyMap (), val testTargetsForShard : ShardChunks ) : Type () data class Robo ( val appApkGcsPath : String , val flankRoboDirectives : List < FlankRoboDirective >? , val roboScriptGcsPath : String? ) : Type () data class GameLoop ( val appApkGcsPath : String , val testRunnerClass : String? , val scenarioNumbers : List < String > , val scenarioLabels : List < String > ) : Type () } interface Execute : ( Config , Type ) -> TestMatrix . Result }","title":"Interface"},{"location":"refactor/steps/#presentation_8","text":"TODO ()","title":"Presentation"},{"location":"refactor/steps/#execute-ios-tests-matrix","text":"","title":"Execute Ios tests matrix"},{"location":"refactor/steps/#target_12","text":"IosRunCommand -> IosArgs/runIosTests -> GcIosTestMatrix/build IosTestContext","title":"Target"},{"location":"refactor/steps/#interface_12","text":"ftl/data/IosTestMatrix.kt package ftl.data object IosTestMatrix { data class Config ( // args val clientDetails : Map < String , String >? , val networkProfile : String? , val directoriesToPull : List < String > , val testTimeout : String , val recordVideo : Boolean , val flakyTestAttempts : Int , val failFast : Boolean , val project : String , val resultsHistoryName : String? , // build val devices : List < Device > , val otherFiles : Map < String , String > , val additionalIpasGcsPaths : List < String > , ) sealed class Type { data class XcTest ( val xcTestGcsPath : String , val xcTestRunFileGcsPath : String , val xcodeVersion : String , val testSpecialEntitlements : Boolean , val matrixGcsPath : String , ) data class GameLoop ( val appGcsPath : String , val scenarios : List < Int > , val matrixGcsPath : String , ) } interface Execute : ( Config , Type ) -> TestMatrix . Result }","title":"Interface"},{"location":"refactor/steps/#presentation_9","text":"TODO ()","title":"Presentation"},{"location":"refactor/steps/#junit-results","text":"","title":"JUnit results"},{"location":"refactor/steps/#target_13","text":"ReportManager/generate -> ReportManager/parseTestSuite ReportManager/processXmlFromFile refreshMatricesAndGetExecutions -> List<TestExecution>/createJUnitTestResult","title":"Target"},{"location":"refactor/steps/#interface_13","text":"ftl/data/JUnitTestResult.kt package ftl.data object JUnitTest { @JacksonXmlRootElement ( localName = \"testsuites\" ) data class Result ( @JsonInclude ( JsonInclude . Include . NON_EMPTY ) @JacksonXmlProperty ( localName = \"testsuite\" ) var testsuites : MutableList < Suite >? = null ) { data class ApiIdentity ( val projectId : String , val matrixIds : List < String > ) interface GenerateFromApi : ( ApiIdentity ) -> Result interface ParseFromFiles : ( File ) -> Result } data class Suite ( @JacksonXmlProperty ( isAttribute = true ) var name : String , @JacksonXmlProperty ( isAttribute = true ) var tests : String , // Int @JacksonXmlProperty ( isAttribute = true ) var failures : String , // Int @JacksonXmlProperty ( isAttribute = true ) var flakes : Int? = null , @JacksonXmlProperty ( isAttribute = true ) var errors : String , // Int @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( isAttribute = true ) var skipped : String? , // Int. Android only @JacksonXmlProperty ( isAttribute = true ) var time : String , // Double @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( isAttribute = true ) val timestamp : String? , // String. Android only @JacksonXmlProperty ( isAttribute = true ) val hostname : String? = \"localhost\" , // String. @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( isAttribute = true ) val testLabExecutionId : String? = null , // String. @JacksonXmlProperty ( localName = \"testcase\" ) var testcases : MutableCollection < Case >? , // not used @JsonInclude ( JsonInclude . Include . NON_NULL ) val properties : Any? = null , // <properties /> @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( localName = \"system-out\" ) val systemOut : Any? = null , // <system-out /> @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( localName = \"system-err\" ) val systemErr : Any? = null // <system-err /> ) data class Case ( // name, classname, and time are always present except for empty test cases <testcase/> @JacksonXmlProperty ( isAttribute = true ) val name : String? , @JacksonXmlProperty ( isAttribute = true ) val classname : String? , @JacksonXmlProperty ( isAttribute = true ) val time : String? , // iOS contains multiple failures for a single test. // JUnit XML allows arbitrary amounts of failure/error tags @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( localName = \"failure\" ) val failures : List < String >? = null , @JsonInclude ( JsonInclude . Include . NON_NULL ) @JacksonXmlProperty ( localName = \"error\" ) val errors : List < String >? = null , @JsonInclude ( JsonInclude . Include . CUSTOM , valueFilter = FilterNotNull :: class ) val skipped : String? = \"absent\" // used by FilterNotNull to filter out absent `skipped` values ) { // Consider to move all properties to constructor if will doesn't conflict with parser @JsonInclude ( JsonInclude . Include . NON_NULL ) var webLink : String? = null @JacksonXmlProperty ( isAttribute = true ) var flaky : Boolean? = null // use null instead of false } @Suppress ( \"UnusedPrivateClass\" ) private class FilterNotNull { override fun equals ( other : Any? ): Boolean { // other is null = present // other is not null = absent (default value) return other != null } override fun hashCode (): Int { return javaClass . hashCode () } } }","title":"Interface"},{"location":"refactor/steps/#performance-metrics","text":"","title":"Performance Metrics"},{"location":"refactor/steps/#target_14","text":"GcToolResults/getPerformanceMetric","title":"Target"},{"location":"refactor/steps/#interface_14","text":"ftl/data/PerfMetrics.kt package ftl.data object PerfMetrics { // based on com.google.api.services.toolresults.model.PerfMetricsSummary data class Summary ( val appStartTime : AppStartTime? = null , val graphicsStats : GraphicsStats? = null , val perfEnvironment : PerfEnvironment? = null , val perfMetrics : List < String >? = null , val executionId : String? = null , val historyId : String? = null , val projectId : String? = null , val stepId : String? = null , ) data class GraphicsStats ( val buckets : List < Bucket >? = null , val highInputLatencyCount : Long? = null , val jankyFrames : Long? = null , val missedVsyncCount : Long? = null , val p50Millis : Long? = null , val p90Millis : Long? = null , val p95Millis : Long? = null , val p99Millis : Long? = null , val slowBitmapUploadCount : Long? = null , val slowDrawCount : Long? = null , val slowUiThreadCount : Long? = null , val totalFrames : Long? = null , ) { data class Bucket ( val frameCount : Long? , val renderMillis : Long? , ) } data class AppStartTime ( val fullyDrawnTime : Duration? = null , val initialDisplayTime : Duration? = null , ) data class PerfEnvironment ( val cpuInfo : CPUInfo? = null , val memoryInfo : MemoryInfo? = null , ) data class CPUInfo ( val cpuProcessor : String? = null , val cpuSpeedInGhz : Float? = null , val numberOfCores : Int? = null , ) data class MemoryInfo ( val memoryCapInKibibyte : Long? = null , val memoryTotalInKibibyte : Long? = null , ) data class Identity ( val executionId : String , val historyId : String , val projectId : String , val stepId : String , ) interface Fetch : ( Identity ) -> Summary }","title":"Interface"},{"location":"refactor/steps/#fetching-artifacts","text":"","title":"Fetching Artifacts"},{"location":"refactor/steps/#target_15","text":"FetchArtifacts.kt","title":"Target"},{"location":"refactor/steps/#interface_15","text":"ftl/data/PerfMetrics.kt package ftl.data object Artifacts { data class Identity ( val gcsPathWithoutRootBucket : String , val gcsRootBucket : String , val regex : List < Regex > , val blobPath : String , val downloadPath : DownloadPath , ) data class DownloadPath ( val localResultDir : String , val useLocalResultDir : Boolean , val keepFilePath : Boolean , ) interface Fetch : ( Identity ) -> List < String > }","title":"Interface"},{"location":"refactor/flank_run/flank_run/","text":"Flank run refactor References https://github.com/Flank/flank/issues/1317 Insights Fix import dependencies Only the cli commands can be aware of run package. So any code inside the run package which is imported somewhere else then cli, must be reorganized and moved outside the run package.","title":"Flank run refactor"},{"location":"refactor/flank_run/flank_run/#flank-run-refactor","text":"","title":"Flank run refactor"},{"location":"refactor/flank_run/flank_run/#references","text":"https://github.com/Flank/flank/issues/1317","title":"References"},{"location":"refactor/flank_run/flank_run/#insights","text":"","title":"Insights"},{"location":"refactor/flank_run/flank_run/#fix-import-dependencies","text":"Only the cli commands can be aware of run package. So any code inside the run package which is imported somewhere else then cli, must be reorganized and moved outside the run package.","title":"Fix import dependencies"},{"location":"refactor/flank_run/flank_run_refactor_sdd/","text":"Flank run refactor [Not complete] References https://github.com/Flank/flank/issues/1317 Motivation Flank run is getting bigger over the time. Currently, the amount of code related to test run is large, so it could be hard to understand and keep in mind the whole process. To make the work more convenient, faster and scalable, we should re-think and reorganize the code between packages and files. Goals All logic related to flank run grouped in one root package Package structure flat as possible. Flank run process simplified to sequence of synchronous atomic steps. Packages and code are easy to identify as steps. Non-Goals Refactor code related to other commands than flank run Design Explain and diagram the technical design Identify risks and edge cases API What will the proposed API look like? Results What was the outcome of the project? Dependencies What is the project blocked on? What will be impacted by the project? Files ftl/run ftl/run/ \u251c\u2500\u2500 CancelLastRun.kt \u251c\u2500\u2500 DumpShards.kt \u251c\u2500\u2500 NewTestRun.kt \u251c\u2500\u2500 RefreshLastRun.kt \u251c\u2500\u2500 common \u2502 \u251c\u2500\u2500 FetchArtifacts.kt \u2502 \u251c\u2500\u2500 GetLastArgs.kt \u2502 \u251c\u2500\u2500 GetLastGcsPath.kt \u2502 \u251c\u2500\u2500 GetLastMatrices.kt \u2502 \u251c\u2500\u2500 PollMatrices.kt \u2502 \u251c\u2500\u2500 PrettyPrint.kt \u2502 \u251c\u2500\u2500 SaveSessionId.kt \u2502 \u2514\u2500\u2500 UpdateMatrixFile.kt \u251c\u2500\u2500 exception \u2502 \u251c\u2500\u2500 ExceptionHandler.kt \u2502 \u251c\u2500\u2500 FlankException.kt \u2502 \u2514\u2500\u2500 FlankExitCodes.kt \u251c\u2500\u2500 model \u2502 \u251c\u2500\u2500 AndroidMatrixTestShards.kt \u2502 \u251c\u2500\u2500 AndroidTestContext.kt \u2502 \u251c\u2500\u2500 AndroidTestShards.kt \u2502 \u251c\u2500\u2500 IosTestContext.kt \u2502 \u2514\u2500\u2500 TestResult.kt \u251c\u2500\u2500 platform \u2502 \u251c\u2500\u2500 RunAndroidTests.kt \u2502 \u251c\u2500\u2500 RunIosTests.kt \u2502 \u251c\u2500\u2500 android \u2502 \u2502 \u251c\u2500\u2500 AndroidTestConfig.kt \u2502 \u2502 \u251c\u2500\u2500 CreateAndroidLoopConfig.kt \u2502 \u2502 \u251c\u2500\u2500 CreateAndroidTestConfig.kt \u2502 \u2502 \u251c\u2500\u2500 CreateAndroidTestContext.kt \u2502 \u2502 \u251c\u2500\u2500 CreateInstrumentationConfig.kt \u2502 \u2502 \u251c\u2500\u2500 CreateRoboConfig.kt \u2502 \u2502 \u251c\u2500\u2500 GetAndroidMatrixShards.kt \u2502 \u2502 \u251c\u2500\u2500 ResolveApks.kt \u2502 \u2502 \u251c\u2500\u2500 UploadApks.kt \u2502 \u2502 \u2514\u2500\u2500 UploadOtherFiles.kt \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u251c\u2500\u2500 AfterRunTests.kt \u2502 \u2502 \u251c\u2500\u2500 BeforeRunMessage.kt \u2502 \u2502 \u2514\u2500\u2500 BeforeRunTests.kt \u2502 \u2514\u2500\u2500 ios \u2502 \u251c\u2500\u2500 CreateGameloopTestContext.kt \u2502 \u251c\u2500\u2500 CreateIosTestContext.kt \u2502 \u2514\u2500\u2500 CreateXcTestContext.kt \u2514\u2500\u2500 status \u251c\u2500\u2500 ExecutionStatus.kt \u251c\u2500\u2500 ExecutionStatusListPrinter.kt \u251c\u2500\u2500 ExecutionStatusPrinter.kt \u251c\u2500\u2500 OutputStyle.kt \u2514\u2500\u2500 TestMatrixStatusPrinter.kt Dependencies ftl/args/AndroidArgs.kt ftl/args/AndroidArgsCompanion.kt ftl/args/ArgsHelper.kt ftl/args/CalculateShardsResult.kt ftl/args/FlankRoboDirective.kt ftl/args/IArgs.kt ftl/args/IgnoredTestCases.kt ftl/args/IosArgs.kt ftl/args/IosArgsCompanion.kt ftl/args/ShardChunks.kt ftl/args/ValidateAndroidArgs.kt ftl/args/ValidateIosArgs.kt ftl/args/yml/AppTestPair.kt ftl/config/FtlConstants.kt ftl/filter/TestFilters.kt ftl/gc/GcAndroidDevice.kt ftl/gc/GcAndroidTestMatrix.kt ftl/gc/GcIosMatrix.kt ftl/gc/GcIosTestMatrix.kt ftl/gc/GcStorage.kt ftl/gc/GcTesting.kt ftl/gc/GcTestMatrix.kt ftl/gc/GcToolResults.kt ftl/http/ExecuteWithRetry.kt ftl/ios/xctest/XcTestData.kt ftl/ios/xctest/XcTestRunFlow.kt ftl/ios/xctest/common/Util.kt ftl/json/MatrixMap.kt ftl/json/SavedMatrix.kt ftl/reports/output/OutputReport.kt ftl/reports/output/OutputReportLoggers.kt ftl/reports/util/ReportManager.kt ftl/shard/Chunk.kt ftl/shard/Shard.kt ftl/shard/TestCasesCreator.kt ftl/util/Artifacts.kt ftl/util/CrashReporter.kt ftl/util/FileReference.kt ftl/util/FlankTestMethod.kt ftl/util/FlowExt.kt ftl/util/MatrixState.kt ftl/util/ObfuscationGson.kt ftl/util/ShardCounter.kt ftl/util/StopWatch.kt ftl/util/TestMatrixExtension.kt Testing How will the project be tested? Alternatives Considered [optional] Summarize alternative designs (pros & cons) Timeline [optional for regular tigers] Document milestones and deadlines. DONE: - NEXT: -","title":"Flank run refactor [Not complete]"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#flank-run-refactor-not-complete","text":"","title":"Flank run refactor [Not complete]"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#references","text":"https://github.com/Flank/flank/issues/1317","title":"References"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#motivation","text":"Flank run is getting bigger over the time. Currently, the amount of code related to test run is large, so it could be hard to understand and keep in mind the whole process. To make the work more convenient, faster and scalable, we should re-think and reorganize the code between packages and files.","title":"Motivation"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#goals","text":"All logic related to flank run grouped in one root package Package structure flat as possible. Flank run process simplified to sequence of synchronous atomic steps. Packages and code are easy to identify as steps.","title":"Goals"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#non-goals","text":"Refactor code related to other commands than flank run","title":"Non-Goals"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#design","text":"Explain and diagram the technical design Identify risks and edge cases","title":"Design"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#api","text":"What will the proposed API look like?","title":"API"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#results","text":"What was the outcome of the project?","title":"Results"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#dependencies","text":"What is the project blocked on? What will be impacted by the project?","title":"Dependencies"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#files","text":"","title":"Files"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#ftlrun","text":"ftl/run/ \u251c\u2500\u2500 CancelLastRun.kt \u251c\u2500\u2500 DumpShards.kt \u251c\u2500\u2500 NewTestRun.kt \u251c\u2500\u2500 RefreshLastRun.kt \u251c\u2500\u2500 common \u2502 \u251c\u2500\u2500 FetchArtifacts.kt \u2502 \u251c\u2500\u2500 GetLastArgs.kt \u2502 \u251c\u2500\u2500 GetLastGcsPath.kt \u2502 \u251c\u2500\u2500 GetLastMatrices.kt \u2502 \u251c\u2500\u2500 PollMatrices.kt \u2502 \u251c\u2500\u2500 PrettyPrint.kt \u2502 \u251c\u2500\u2500 SaveSessionId.kt \u2502 \u2514\u2500\u2500 UpdateMatrixFile.kt \u251c\u2500\u2500 exception \u2502 \u251c\u2500\u2500 ExceptionHandler.kt \u2502 \u251c\u2500\u2500 FlankException.kt \u2502 \u2514\u2500\u2500 FlankExitCodes.kt \u251c\u2500\u2500 model \u2502 \u251c\u2500\u2500 AndroidMatrixTestShards.kt \u2502 \u251c\u2500\u2500 AndroidTestContext.kt \u2502 \u251c\u2500\u2500 AndroidTestShards.kt \u2502 \u251c\u2500\u2500 IosTestContext.kt \u2502 \u2514\u2500\u2500 TestResult.kt \u251c\u2500\u2500 platform \u2502 \u251c\u2500\u2500 RunAndroidTests.kt \u2502 \u251c\u2500\u2500 RunIosTests.kt \u2502 \u251c\u2500\u2500 android \u2502 \u2502 \u251c\u2500\u2500 AndroidTestConfig.kt \u2502 \u2502 \u251c\u2500\u2500 CreateAndroidLoopConfig.kt \u2502 \u2502 \u251c\u2500\u2500 CreateAndroidTestConfig.kt \u2502 \u2502 \u251c\u2500\u2500 CreateAndroidTestContext.kt \u2502 \u2502 \u251c\u2500\u2500 CreateInstrumentationConfig.kt \u2502 \u2502 \u251c\u2500\u2500 CreateRoboConfig.kt \u2502 \u2502 \u251c\u2500\u2500 GetAndroidMatrixShards.kt \u2502 \u2502 \u251c\u2500\u2500 ResolveApks.kt \u2502 \u2502 \u251c\u2500\u2500 UploadApks.kt \u2502 \u2502 \u2514\u2500\u2500 UploadOtherFiles.kt \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u251c\u2500\u2500 AfterRunTests.kt \u2502 \u2502 \u251c\u2500\u2500 BeforeRunMessage.kt \u2502 \u2502 \u2514\u2500\u2500 BeforeRunTests.kt \u2502 \u2514\u2500\u2500 ios \u2502 \u251c\u2500\u2500 CreateGameloopTestContext.kt \u2502 \u251c\u2500\u2500 CreateIosTestContext.kt \u2502 \u2514\u2500\u2500 CreateXcTestContext.kt \u2514\u2500\u2500 status \u251c\u2500\u2500 ExecutionStatus.kt \u251c\u2500\u2500 ExecutionStatusListPrinter.kt \u251c\u2500\u2500 ExecutionStatusPrinter.kt \u251c\u2500\u2500 OutputStyle.kt \u2514\u2500\u2500 TestMatrixStatusPrinter.kt","title":"ftl/run"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#dependencies_1","text":"ftl/args/AndroidArgs.kt ftl/args/AndroidArgsCompanion.kt ftl/args/ArgsHelper.kt ftl/args/CalculateShardsResult.kt ftl/args/FlankRoboDirective.kt ftl/args/IArgs.kt ftl/args/IgnoredTestCases.kt ftl/args/IosArgs.kt ftl/args/IosArgsCompanion.kt ftl/args/ShardChunks.kt ftl/args/ValidateAndroidArgs.kt ftl/args/ValidateIosArgs.kt ftl/args/yml/AppTestPair.kt ftl/config/FtlConstants.kt ftl/filter/TestFilters.kt ftl/gc/GcAndroidDevice.kt ftl/gc/GcAndroidTestMatrix.kt ftl/gc/GcIosMatrix.kt ftl/gc/GcIosTestMatrix.kt ftl/gc/GcStorage.kt ftl/gc/GcTesting.kt ftl/gc/GcTestMatrix.kt ftl/gc/GcToolResults.kt ftl/http/ExecuteWithRetry.kt ftl/ios/xctest/XcTestData.kt ftl/ios/xctest/XcTestRunFlow.kt ftl/ios/xctest/common/Util.kt ftl/json/MatrixMap.kt ftl/json/SavedMatrix.kt ftl/reports/output/OutputReport.kt ftl/reports/output/OutputReportLoggers.kt ftl/reports/util/ReportManager.kt ftl/shard/Chunk.kt ftl/shard/Shard.kt ftl/shard/TestCasesCreator.kt ftl/util/Artifacts.kt ftl/util/CrashReporter.kt ftl/util/FileReference.kt ftl/util/FlankTestMethod.kt ftl/util/FlowExt.kt ftl/util/MatrixState.kt ftl/util/ObfuscationGson.kt ftl/util/ShardCounter.kt ftl/util/StopWatch.kt ftl/util/TestMatrixExtension.kt","title":"Dependencies"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#testing","text":"How will the project be tested?","title":"Testing"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#alternatives-considered-optional","text":"Summarize alternative designs (pros & cons)","title":"Alternatives Considered [optional]"},{"location":"refactor/flank_run/flank_run_refactor_sdd/#timeline-optional-for-regular-tigers","text":"Document milestones and deadlines. DONE: - NEXT: -","title":"Timeline [optional for regular tigers]"}]}